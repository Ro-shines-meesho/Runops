{
  "metadata": {
    "last_fetched": "2025-08-05T13:24:35.825326",
    "total_runbooks": 40,
    "source": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2678227022/DevOps+RunBooks"
  },
  "runbooks": [
    {
      "id": "3016163386",
      "title": "Contour/Envoy RunBook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3016163386/Contour+Envoy+RunBook",
      "created": "2024-09-23T10:27:43.711Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h3>How to Ensure that Contour/Envoy is Healthy</h3><p>There is a comprehensive dashboard for monitoring that Contour and Envoy are healthy.</p><p><a href=\"https://grafana-prd.meeshogcp.in/d/sipO9ZA4k/contour-global-health-metrics?orgId=1\">Contour Global Health Metrics</a></p><p>Few things to quickly ensure everything is alright:</p><ul><li><p>All envoy pods should be available</p></li><li><p>Average CPU usage should be less than 65%</p></li><li><p>RPS distribution across Envoy pods should be fairly distributed</p></li><li><p>CPU usage per envoy pod should be approximately similar</p></li><li><p>Circuit breaker should not have <code>Pending RQ</code> type of break. If found, immediately increase CB for the particular service</p></li></ul><h3>Telegraf is Showing 5XX/Latency Increase But Envoy Dashboard is Not Showing</h3><p>This happens when the Client closes the connection without waiting for a response (there might be a timeout configured in the upstream while waiting for a response). Now when the client closes the connection, envoy also does not wait for an answer from the service. So even if the service later responds with a 5XX there is no one waiting for the answer and hence Envoy will not record any response data for such calls.</p><p>But Envoy will show &ldquo;Client Aborts&rdquo; for such call where a connection with active request for abruptly dropped. This info is available in <a href=\"https://grafana.meesho.com/d/000000003/envoy-proxy?orgId=1&amp;viewPanel=28&amp;var-cluster=prd-ringmaster-frontend_prd-ringmaster-frontend_80&amp;var-hosts=All\">here</a> in the Envoy Dashboard. <a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2787049826/Contour+Migration+and+Slow+Start#Client-Aborts\">Read more about Client Aborts</a></p><h3>RPS is Not Evenly Distributed on Service Pods</h3><ul><li><p>If Zone Aware Routing is enabled (can be verified from Envoy Proxy dashboard), a skewness of &lt;20% can be considered normal.</p></li><li><p>Check the per pod latency using the Telegraf dashboard and see if few pods are latent than others. Latent pods naturally receive less traffic compared to the pods which are running normal.</p></li><li><p>Check distribution of the <a href=\"https://grafana.meesho.com/d/000000003/envoy-proxy?orgId=1&amp;viewPanel=4\">RPS per Envoy pod</a> per service, a 10% skewness is normal, but if it goes beyond that please involve SMEs.</p></li></ul><h3>There are 503s</h3><p>503s are tricky, and there can be several reasons for the same:</p><ul><li><p>Most common reason for 503s is when Envoy fails to establish a connection with the service pods. This can happen if the service pods are overwhelmed (high CPU usage, more RPS than the pods can handle, network issues etc.). In this case, scaling the service pods should help unless the issue is arising from dependencies (redis, RDS, hbase etc.).</p></li><li><p>If all the service pods become unavailable, then also Envoy throws 503.</p></li><li><p>If the Circuit Breaker for Envoy opens up, then also 503s are thrown. <a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2787049826/Contour+Migration+and+Slow+Start#Debugging-503s\">Read more about this</a></p></li></ul><h3>504s and Response Timeout</h3><p>A 504 is thrown in two cases:</p><ul><li><p>Either the service responds with 504</p></li><li><p>Envoy does not get a response from the service within the configured response timeout. The default response timeout is 15 seconds, after which envoy will throw a 504. This can be changed per service, <a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2787049826/Contour+Migration+and+Slow+Start#Response-Timeout\">read more about it</a>.</p></li></ul><h3>What is the Contour Circuit Breaker and When to Worry About it?</h3><p>Detailed information around this is available in the Contour Doc here: <a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2787049826/Contour+Migration+and+Slow+Start#Contour-Circuit-Breaker%2C-Client-Abort-and-Other-Configurations%3A\" data-card-appearance=\"inline\">https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2787049826/Contour+Migration+and+Slow+Start#Contour-Circuit-Breaker%2C-Client-Abort-and-Other-Configurations%3A</a> </p><p />",
      "word_count": 453,
      "is_runbook": true
    },
    {
      "id": "3848306705",
      "title": "PR Validation failures run-book",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3848306705/PR+Validation+failures+run-book",
      "created": "2025-04-03T13:26:32.792Z",
      "author": "Arnav Andrew Jose",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p>This run-book contains various scenarios that might stop your PR from getting merged. Kindly go through them to try and find the matching issue, which will help it to be resolved quickly without having to wait for DevOps intervention.</p><p>Note that @devops-oncall on <a href=\"https://meesho.slack.com/archives/C021QNS6JLV\">#devops-tech</a> channel in slack are not to be pinged for non-prod issues.</p><h3>Table of contents</h3><ul><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3848306705/PR+Validation+failures+run-book#Preliminary%3A-retriggering-checks\">Preliminary: re-triggering checks</a> </p></li><li><p><a href=\"#\">Preliminary: checking ringmaster status</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3848306705/PR+Validation+failures+run-book#Ringmaster-stg-check-failing\">Ringmaster-stg check failing</a></p><ul><li><p><a href=\"#\">Not deployed with latest commit</a></p></li><li><p><a href=\"#\">Deployables are missing in staging environment</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3848306705/PR+Validation+failures+run-book#Queued-for-very-long\">Staging deployables unhealthy | Checks queued for very long</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3848306705/PR+Validation+failures+run-book#Viewing-the-temporal-UI\">Viewing the temporal UI</a> </p></li></ul></li><li><p><a href=\"#\">Service_type missing</a></p></li><li><p><a href=\"#\">Jenkins says app sync failed</a></p></li><li><p><a href=\"#\">Both ringmaster checks saying &ldquo;expected&rdquo; for very long</a></p></li></ul><h1>Preliminary: retriggering checks</h1><p>In many of the problems here, after implementing the fix you will be asked to retrigger the checks. The simplest way to do this is by converting the PR to draft and then marking it as ready to review.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"618\" ac:original-width=\"586\" ac:custom-width=\"true\" ac:width=\"586\"><ri:attachment ri:filename=\"Screenshot 2025-01-28 at 5.08.33 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p>click this link on github on the PR page to convert PR to a draft</p></ac:caption></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"648\" ac:original-width=\"1005\" ac:custom-width=\"true\" ac:width=\"586\"><ri:attachment ri:filename=\"Screenshot 2025-01-28 at 5.10.59 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p>click this button near the bottom of the page to mark this PR as ready for review. This will run failing checks again.</p></ac:caption></ac:image><h1>Preliminary: Navigating to ringmaster deployment status</h1><p>If the stg check has failed, you can open ringmaster, click on the drop-down on the right side of the page, and select <em>gcp_stg</em>.<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"156\" ac:original-width=\"403\" ac:custom-width=\"true\" ac:width=\"403\"><ri:attachment ri:filename=\"Screenshot 2025-02-03 at 4.39.06 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>then, click on CICD from the top left, and in the search bar enter repo name.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"263\" ac:original-width=\"846\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-02-03 at 4.41.37 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>you can click on the deployments tab here and see the status of the latest deployments of all your services.</p><p>If the other check has failed, you need to check <em>gcp_int  </em>instead.</p><p /><h1>Ringmaster-stg check failing</h1><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"78\" ac:original-width=\"777\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-01-28 at 4.56.14 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>When merging to main/master from develop branch, this check is required which sees health of staging deployables. There are multiple ways this can fail, you need to check which failure it was by clicking on <strong>ringmaster-stg-deployment-status</strong> </p><h2>Not deployed with latest commit</h2><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"242\" ac:original-width=\"1608\" ac:custom-width=\"true\" ac:alt=\"image-20250128-112503.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20250128-112503.png\" ri:version-at-save=\"1\" /></ac:image><p>If you get a message saying latest commit is not deployed, you can check whether this is the case by checking the latest commit of develop branch:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"179\" ac:original-width=\"921\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-01-28 at 4.57.59 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>You can see the *commit hash* inside the circle. You have to go to <a href=\"https://argocd-dev.meeshogcp.in/\">https://argocd-dev.meeshogcp.in/</a>, go to the application indicated by the failing check (on clicking view details or by searching for your repo in ringmaster gcp_stg env)</p><p>Open stg-&lt;application name&gt; in dev argocd, scroll to the right and get details of any of the latest pods.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"691\" ac:original-width=\"1393\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-01-28 at 5.02.45 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>here in the Images row, you will see an image with the same name as the pod, toward the end the second last number (lowercase hexadecimal) is an abbreviated git hash of the latest commit. This should match the value given on Github.</p><p>If it does, try deploying the application on stg <em>using ringmaster</em> and then retrigger checks.</p><h2>Deployables are missing</h2><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"381\" ac:original-width=\"1440\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-01-28 at 5.17.22 PM.png\" ri:version-at-save=\"2\" /></ac:image><p>This message says &ldquo;the following staging deployables are missing for the respective modules&hellip;&rdquo;. If you see this, that means that of the modules (defined in pom.xml for instance), there is at least one module which has no application deployed in stg. In deployments/&lt;application name&gt;.yaml there is a field called <code>module</code>, see if any applications with the missing module are available in stg.</p><p>You can deploy these via ringmaster in the gcp_stg env, and then retrigger the checks.</p><h2>Queued for very long</h2><p>If the check takes longer than 20 minutes, it might mean that your staging deployments are degraded</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"574\" ac:original-width=\"1490\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-01-22 at 12.17.35 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p>A degraded pod, this can be due to many reasons including application failure</p></ac:caption></ac:image><h2>Viewing the temporal UI</h2><p>Ringmaster uses <a href=\"https://temporal.io/\">Temporal</a>  for error-resistant execution of logic. All of the steps that might fail are run on temporal, and can be viewed in the temporal UI.</p><h3>Port-forwarding the temporal service</h3><ul><li><p>Connect to the central-prd kubernetes cluster<br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"88306740-6b21-4971-9e73-29897f4bb3fa\"><ac:parameter ac:name=\"language\">bash</ac:parameter><ac:plain-text-body><![CDATA[gcloud container clusters get-credentials k8s-central-prd-ase1 --region asia-southeast1 --project meesho-central-prd-0622]]></ac:plain-text-body></ac:structured-macro></li><li><p>forward the temporal-web service<br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"6424778e-5aa6-4107-8711-1d6321253467\"><ac:parameter ac:name=\"language\">bash</ac:parameter><ac:plain-text-body><![CDATA[kubectl port-forward svc/prd-ringmaster-temporal-web 8080 -n prd-ringmaster-temporal]]></ac:plain-text-body></ac:structured-macro></li><li><p>go to <a href=\"http://localhost:8080\">http://localhost:8080</a></p></li></ul><h3>Filtering</h3><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"781\" ac:original-width=\"1716\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-04-03 at 6.54.30 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>In the search bar, add the following: <code>WorkflowType=&quot;stgDeployablesHealthStatusChildWorkflow&quot;</code></p><p>This will show the most recent staging health checks. See if the one for your service has failed.</p><h1>Service_type missing</h1><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"319\" ac:original-width=\"1387\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-01-28 at 4.43.59 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>When ringmaster deployment fails with the last column reading </p><p>&ldquo;DEPLOYMENT_STATUS_FAILED DEPLOYMENT_REASON_ARGO_APP_GIT_FAILED&ldquo;, read the message in the grey box carefully. Here it says &ldquo;service_type label is not present in deployments.yaml, please provide the params to proceed further&ldquo;.</p><p>this <code>service_type</code> field in deployments/&lt;app name here&gt;.yaml files <a href=\"https://meesho.slack.com/archives/C021QNS6JLV/p1736338944071649\">is required</a> (slack message of announcement).<br />Discussion of possible values and reason for change are here:<br /><ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"DEVOPS\" ri:content-title=\"Service-Type Label in Deployment Yaml:\" ri:version-at-save=\"6\" /><ac:link-body>Service-Type Label in Deployment Yaml:</ac:link-body></ac:link></p><h1>App sync failed, please check in argoCD UI</h1><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"903\" ac:original-width=\"1394\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-01-15 at 3.26.42 PM.png\" ri:version-at-save=\"2\" /></ac:image><p>In the screenshot we see &ldquo;App sync failed, please check in argoCD UI&rdquo;. Before this the block says something like <code>argocd app sync &lt;appname&gt;</code>. That application is the one that needs sync with pruning.</p><p>If developer does not have prune access, reach out to your pod&rsquo;s DevOps PoC. Then re-trigger CI.</p><h1>Both ringmaster checks saying &ldquo;expected&rdquo; for very long</h1><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"77\" ac:original-width=\"788\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-01-28 at 5.40.32 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>If you see this even after all other checks are passed, there might be some issue with github webhooks, try retriggering the CI once. If it fails again, reach out to devops.<br /></p><p />",
      "word_count": 989,
      "is_runbook": true
    },
    {
      "id": "3024977921",
      "title": "CoreDNS RunBook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3024977921/CoreDNS+RunBook",
      "created": "2023-09-26T11:11:58.450Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h3>How is CoreDNS deployed right now?</h3><p>In each cluster CoreDNS is deployed via ArgoCD.</p><p>The manifests are present here: <a href=\"https://github.com/Meesho/cluster-config-eks/tree/master/coredns\" data-card-appearance=\"inline\">https://github.com/Meesho/cluster-config-eks/tree/master/coredns</a> </p><p>And ArgoCD apps are here:</p><ul><li><p><a href=\"https://prod-ops-argocd.meesho.com/applications/p-demand-coredns\">https://prod-ops-argocd.meesho.com/applications/p-demand-coredns</a></p></li><li><p><a href=\"https://prod-ops-argocd.meesho.com/applications/p-central-coredns\">https://prod-ops-argocd.meesho.com/applications/p-central-coredns</a></p></li><li><p><a href=\"https://prod-ops-argocd.meesho.com/applications/p-supply-coredns\">https://prod-ops-argocd.meesho.com/applications/p-supply-coredns</a></p></li><li><p><a href=\"https://prod-ops-argocd.meesho.com/applications/p-datascience-coredns\">https://prod-ops-argocd.meesho.com/applications/p-datascience-coredns</a></p></li><li><p><a href=\"https://prod-ops-argocd.meesho.com/applications/p-dataplatform-coredns\">https://prod-ops-argocd.meesho.com/applications/p-dataplatform-coredns</a></p></li></ul><h3>How Does Rewriting Work at CoreDNS Level</h3><p>Read about it in detail here: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Service to Service Communication and CoreDNS [EKS]\" ri:version-at-save=\"10\" /><ac:link-body>Service to Service Communication and CoreDNS [EKS]</ac:link-body></ac:link> </p><h3>How to ensure that everything is right at DNS level</h3><ol start=\"1\"><li><p>Check whether node-local-dns pod is running fine on the node where issues are being experience. It runs as a daemonset. The pod should be healthy and logs should appear normal.</p></li><li><p>Then check whether all CoreDNS pods are running fine in the above mentioned argocd Link.</p></li><li><p>Finally check the <a href=\"https://grafana.meesho.com/d/wY4blRMGza/coredns?orgId=1&amp;var-cluster=p-demand-cluster&amp;var-instance=All\">CoreDNS Grafana dashboard</a>, important things to note:</p><ol start=\"1\"><li><p>Local Response Latency should be below <code>600 microseconds</code></p></li><li><p>Upstream Response Latency should be around or below <code>20 milliseconds</code></p></li></ol></li></ol><h3>How to Quickly Fallback to ALB and Route53 by skipping CoreDNS rewrites</h3><h4>Temporary</h4><ul><li><p>Locate the configmap here for the cluster where the calling service (not the target service) is  situated. In other words, if service A calls service B and the requirement is that A should call B via ALB, then A is the calling service and B is the target service. Changes should be done in the cluster where A is located, regardless of the location of B.</p></li><li><p>In the correct ConfigMap, remove the rewrite rule for the target service. In our example, remove the rewrite rule for service B&rsquo;s hostname.</p></li><li><p>Go to ArgoCD for the respective CoreDNS and sync it. Wait for sometime, nothing else needs to be done.</p></li></ul><ac:structured-macro ac:name=\"warning\" ac:schema-version=\"1\" ac:macro-id=\"60256b4e-e521-4355-b007-156e5036c4cb\"><ac:rich-text-body><p><strong><span style=\"color: rgb(191,38,0);\">DO NOT SEND A LOT OF TRAFFIC TO THE ALB AT ONCE. ALBs need time to scale. So Either pre-warm the ALB or go cluster-by-cluster so that traffic from all clusters does not hit the ALB at once.</span></strong></p></ac:rich-text-body></ac:structured-macro><h4>Permanent</h4><ul><li><p>Do all the steps for the Temporary changes above</p></li><li><p>Finally remove the block for the DNS of target service from <a href=\"https://github.com/Meesho/cluster-config-eks/blob/master/coredns/global/global.yaml\">global.yaml</a></p></li></ul><h3>How to Onboard A New Service to CoreDNS</h3><p>Read about it here: <a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2879160330/Service+to+Service+Communication+and+CoreDNS+EKS#Automation-Around-Rule-Addition\" data-card-appearance=\"inline\">https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2879160330/Service+to+Service+Communication+and+CoreDNS+EKS#Automation-Around-Rule-Addition</a> </p><p />",
      "word_count": 332,
      "is_runbook": true
    },
    {
      "id": "2987753889",
      "title": "EKS platform sale readiness runbook - AWS-EKS",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2987753889/EKS+platform+sale+readiness+runbook+-+AWS-EKS",
      "created": "2023-09-20T08:12:20.502Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"b8d28f97-8b25-4a61-a9ff-1f52dba62578\"><tbody><tr><th><p><strong>Runbook Name</strong></p></th><th><p>EKS platform sale readiness </p></th></tr><tr><td><p>Runbook description</p></td><td><p>This runbook guides through the sale readiness checklist that we do before and after monthly sales. Devops on-call will execute this checklist just before the sale date i.e before one day once announced.</p></td></tr><tr><td><p>Author</p></td><td><p><ac:link><ri:user ri:account-id=\"61724e3aa9897100700dee57\" /></ac:link> <ac:link><ri:user ri:account-id=\"61c95b057c6f980070d8c4ec\" /></ac:link> <ac:link><ri:user ri:account-id=\"624ecdf21da0e100713e6e0c\" /></ac:link> </p></td></tr><tr><td><p>Version</p></td><td><p>1.0</p></td></tr><tr><td><p>On this page</p></td><td><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"c51988c2-93c0-4a29-b547-a7eabe3702f1\" ac:macro-id=\"a2335410-2a13-4ae3-b04b-a8ba75fe1bf8\" /></td></tr></tbody></table><h3><strong>Disable node consolidation job</strong></h3><p>Job link: <a href=\"http://ci.meesho.co/view/automation/job/automation/job/node-consolidation/job/node-consolidation-downstream/\"><u>http://ci.meesho.co/view/automation/job/automation/job/node-consolidation/job/node-consolidation-downstream/</u></a></p><p>The above job is the one which triggers all the consolidation jobs for each cluster as cron based triggers i.e everyday at 11:30 PM. So in order to disable node consolidation for all the clusters we disable the above job.</p><p>Once disabled, the job should look like below shared screenshot:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"429\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"mLJWxA3VeUM1GbY71Os9VQPMqBdU-fstHVS9L_eZLv1Kap8rfUteBcAExdcax-PRhq6zRw3x4PCrcgZH6ZF7bGnCpxNUAUENnti-BiVeBO07D03aP74ygChRkz_3pVI361-PRsarRdJ_U-mTa2tDvv4\" ri:version-at-save=\"1\" /></ac:image><h3><strong>Scale pause containers </strong></h3><p>We should have certain nodes as standby nodes in order to schedule the pods immediately instead of waiting for around 90 seconds. In order to achieve this we do run pause containers with low-priority class and these will be scheduled on new nodes and in any of the scale up events, the application pods have priority defined as high-priority and hence pause containers will be evicted and application pods will be scheduled.</p><p>Since we have moved most of our application pods to common-node pool, we do scale only the common-node pool pause containers deployment.</p><p><strong>Steps</strong> <br />Example of&nbsp; scaling&nbsp; for <code>demand</code> cluster is shown, same needs to be followed for other(<code>central, supply, datascience, dataplatform</code>) clusters</p><ol start=\"1\"><li><p>Switch to context of <code>demand</code> cluster in your terminal and switch to namespace <code>pause-container</code></p></li><li><p>List the deployments in this namespace, you will be able to see the below list</p></li></ol><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"694\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"WHw4XfcT-E4NhrJbkHOiitq6Ssa1ul8nMVljgZ3JGo95X7auCFkHzPqpNLZLbZrf83Yj0QxqVGO_xPMmj_dLPrdGGJVITOBtl63bgHW74smbb2PdIFtk15FrAHuP78gOGLxNJmlhDx3Ghc-u_Ee1ocI\" ri:version-at-save=\"1\" /></ac:image><ol start=\"1\"><li><p>We need to scale only the common node-pool ones i.e</p></li></ol><p> p-demand-cluster-hypercore&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br /> p-demand-cluster-hypercore-arm64&nbsp; &nbsp; &nbsp; <br /> p-demand-cluster-hypermem &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br /> P-demand-cluster-hypermem-arm64</p><p><strong>Command</strong>: <code>kubectl scale deployment p-demand-cluster-hypercore --replicas=5</code></p><p>The number of replicas depends on the amount of sale count numbers. Usually on monthly sales we keep the count as 5</p><p>List of deployments in each cluster which needs to be scaled:</p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"6d79cf94-a2ca-4e93-8df1-d24291dbb4ed\"><colgroup><col style=\"width: 173.0px;\" /><col style=\"width: 439.0px;\" /><col style=\"width: 147.0px;\" /></colgroup><tbody><tr><td><p><strong>Cluster name</strong></p></td><td><p><strong>Deployment Name</strong></p></td><td><p><strong>Replicas</strong></p></td></tr><tr><td><p>Demand</p></td><td><p>p-demand-cluster-hypercore<br />p-demand-cluster-hypercore-arm64&nbsp; &nbsp; &nbsp; <br />p-demand-cluster-hypermem <br />p-demand-cluster-hypermem-arm64&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p></td><td><p>5</p></td></tr><tr><td><p>Supply</p></td><td><p>p-supply-cluster-hypercore &nbsp; &nbsp; &nbsp; <br />p-supply-cluster-hypercore-arm64&nbsp; &nbsp; &nbsp; <br />p-supply-cluster-hypermem <br />p-supply-cluster-hypermem-arm64</p></td><td><p>5</p></td></tr><tr><td><p>Central</p></td><td><p>p-central-cluster-hypercore <br />p-central-cluster-hypermem&nbsp;</p></td><td><p>5</p></td></tr><tr><td><p>Datascience</p></td><td><p>p-datascience-cluster-ml-platform</p></td><td><p>5</p></td></tr><tr><td><p>Dataplatform</p></td><td><p>p-dataplatform-cluster-data-platform</p></td><td><p>5</p></td></tr></tbody></table><h3><strong>Scaling Envoy Pods</strong></h3><p>The envoy proxy pods in each of the EKS clusters need to be scaled. Each cluster has multiple deployments of Contour/Envoy and all of them should be scaled.</p><p>Contour Pods do not need to be scaled as they are just observers and 3 pods run without any need of scaling.</p><p>We do the scaling of each envoy by increasing the min count of the deployment. This step is done by editing the values in the below mentioned repo, and once merged - syncing the changes in the argocd. </p><p>For this, the BAU max desired count is found from the dashboard <a href=\"https://grafana.meesho.com/d/sipO9ZA4k/contour-global-health-metrics\">https://grafana.meesho.com/d/sipO9ZA4k/contour-global-health-metrics</a> , based on the requirements of the sale, the minimum is set to 1.5X or 2X of the BAU-max-desired. This needs to be done for each contour and then a PR can be raised against the files in this repo <a href=\"https://github.com/Meesho/cluster-config-eks/tree/master/contour/values\" data-card-appearance=\"inline\">https://github.com/Meesho/cluster-config-eks/tree/master/contour/values</a> .</p><p>The PR should be peer reviewed. Once the PR is merged, syncing should be done in ArgoCD <strong>(but complete sync should NOT be done, ONLY the HPA/ScaledObject should by synced)</strong>.\\</p><p><em>NOTE: An automation is being written for this. The doc will updated accordingly once done.</em></p><h3><strong>Sufficient IP&rsquo;s &amp; provisioner limits check for each cluster</strong></h3><p>We can monitor the availability of IP&rsquo;s for each cluster and the provisioner limits for each cluster in the below dashboard. Just before the sale starts, all the applications would have done the pre-scale activity. Once that is done we need to monitor for IP&rsquo;s availability and karpenter provisioner limit breach(alerts are set onPD) as precautionary measure.</p><p>Grafana dashboard:&nbsp;</p><p><a href=\"https://grafana.meesho.com/d/66Hk-b1Vz/eks-devops-infra?orgId=1&amp;refresh=5s&amp;var-cluster=p-demand-cluster&amp;var-namesapce=aws-nth\"><u>https://grafana.meesho.com/d/66Hk-b1Vz/eks-devops-infra?orgId=1&amp;refresh=5s&amp;var-cluster=p-demand-cluster&amp;var-namesapce=aws-nth</u></a></p><h3><strong>Post sale action items</strong></h3><p>Once the sale period is over, i.e if the sale ends on Sunday night at 12PM, then we need to work on the action items before 10AM of Monday.</p><ol start=\"1\"><li><p>Enable the node consolidation job.</p></li><li><p>Scale down the pause containers in each cluster by setting replica as zero(0)</p></li><li><p>Revert the changes in min count for each envoy of each cluster, merge the changes and sync in the argocd.</p></li></ol><p><strong>Owners</strong> -  Devops team</p>",
      "word_count": 697,
      "is_runbook": true
    },
    {
      "id": "2987884905",
      "title": "EKS application debug runbook - AWS EKS",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2987884905/EKS+application+debug+runbook+-+AWS+EKS",
      "created": "2023-09-20T08:44:03.570Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"b8d28f97-8b25-4a61-a9ff-1f52dba62578\"><tbody><tr><th><p><strong>Runbook Name</strong></p></th><th><p>EKS application debug runbook</p></th></tr><tr><td><p>Runbook description</p></td><td><p>This runbook guides through the initial debugging of application service we normally get in #devops-tech channel. This runbook also have few day to day issues faced and solutions debugging steps.</p><p>This runbook helps both devops and developers.  </p></td></tr><tr><td><p>Author</p></td><td><p><ac:link><ri:user ri:account-id=\"61724e3aa9897100700dee57\" /></ac:link> <ac:link><ri:user ri:account-id=\"61c95b057c6f980070d8c4ec\" /></ac:link></p></td></tr><tr><td><p>Version</p></td><td><p>1.0</p></td></tr><tr><td><p>On this page</p></td><td><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"5b126987-7504-472b-952e-c3be71848ce5\" ac:macro-id=\"a2335410-2a13-4ae3-b04b-a8ba75fe1bf8\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">6</ac:parameter><ac:parameter ac:name=\"outline\">false</ac:parameter><ac:parameter ac:name=\"type\">list</ac:parameter><ac:parameter ac:name=\"printable\">false</ac:parameter></ac:structured-macro></td></tr></tbody></table><h2>Horizontal Pod Autoscaler (HPA) common issues </h2><ol start=\"1\"><li><p><strong>General query: </strong>Scale up of pods is not happening.</p><ol start=\"1\"><li><p>Debugging steps:</p><ol start=\"1\"><li><p>If ringmaster was used for this service then please do use ringmaster only for further scaleup or scale down as the values present in deployment.yaml / values_properties.yaml lose precedence over ringmaster.</p><p>Solution: To proceed only with the ringmaster if anytime the ringmaster was used for scale-up/down.</p></li></ol></li></ol></li><li><p><strong>General query: </strong>Pods didn&rsquo;t scale up as requested for scheduled based scaling for deployments (without using ringmaster anytime)</p><ol start=\"1\"><li><p>Debugging steps: </p><ol start=\"1\"><li><p>To check the min-max replica set in deployment.yaml, if desired is set more than max value, then scale up won't happen.&nbsp;</p></li><li><p>To check max replica count in values_properties.yaml, if the values are added here, deployment.yaml will lose precedence so it needs to be checked here.</p></li><li><p>Cron syntax to be validated, argo should not show any errors.</p></li></ol></li></ol></li><li><p><strong>General query:</strong> Pods are not coming up, pods are remaining in pending state</p><ol start=\"1\"><li><p>Debugging steps: </p><ol start=\"1\"><li><p>Check the node-selector of the app in values_properties.yaml and values.yaml file.</p></li><li><p>Check the cpu request and limit of that provisioner in below dashboard: <a href=\"https://grafana.meesho.com/d/66Hk-b1Vz/eks-devops-infra?orgId=1&amp;var-cluster=p-demand-cluster&amp;var-namesapce=aws-nth&amp;from=now-1h&amp;to=now&amp;refresh=5s\"><u>LINK</u></a></p></li><li><p>If the cpu and memory usage of the provisioner are well within the limits, check the karpenter logs</p></li><li><p>For ex: demand karpenter pod logs : <a href=\"https://argocd-infraops.meesho.co/applications/karpenter-p-demand-cluster?resource=&amp;node=%2FPod%2Fkarpenter%2Fkarpenter-77d448db87-g96xd%2F0&amp;tab=logs\"><u>LINK</u></a> </p></li><li><p>Check if the no of available IP&rsquo;s are exhausted in the requested zone using the below dashboard: <a href=\"https://grafana.meesho.com/d/66Hk-b1Vz/eks-devops-infra?orgId=1&amp;var-cluster=p-demand-cluster&amp;var-namesapce=aws-nth&amp;from=now-1h&amp;to=now&amp;refresh=5s\"><u>LINK</u></a></p></li></ol></li></ol></li></ol><h2>Argo app is in degraded state </h2><ol start=\"1\"><li><p><strong>General query: </strong>Argo health status is showing degraded state</p><ol start=\"1\"><li><p>Debugging steps:</p><ol start=\"1\"><li><p>Check in the argo Ui which of the resource is in degraded state<br />Ex <a href=\"https://prod-ops-argocd.meesho.com/applications/argocd/prd-address-search?view=tree&amp;resource=\"><u>LINK</u></a>:&nbsp; </p></li><li><p>Also click on the canary resource in the argo ui and check the events:<br />Ex <a href=\"https://prod-ops-argocd.meesho.com/applications/argocd/prd-address-search?view=tree&amp;resource=&amp;node=flagger.app%2FCanary%2Fprd-address-search%2Fprd-address-search%2F0&amp;tab=events\"><u>LINK</u></a>:&nbsp;</p></li><li><p>If the canary is in a degraded state, that means canary progress didn&rsquo;t happen. Please check the events tab in canary resource or you can check the success rate of canary in envoy-grafana dashboard for canary. If canary has failed due to high error rate then please do check the code &amp; re-deploy.<br />Ex of canary dashboard: <a href=\"https://grafana.meesho.com/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-cart_prd-cart-canary_80&amp;var-hosts=All\"><u>LINK</u></a></p></li><li><p>If it is not a canary deployment and the new pods coming up are not starting then please do check the pod logs and check the code changes. If any code changes are required please do the changes and re-deploy</p></li></ol></li></ol></li></ol><p>                  Ex of pod logs: <a href=\"https://prod-ops-argocd.meesho.com/applications/argocd/prd-address-search?view=tree&amp;resource=&amp;node=%2FPod%2Fprd-address-search%2Fprd-address-search-primary-659744bb8f-2f8wg%2F0&amp;tab=logs\"><u>Link</u></a></p><p style=\"margin-left: 60.0px;\">   Ref screenshot:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"864\" ac:original-width=\"1600\" ac:width=\"680\"><ri:attachment ri:filename=\"HfiARFBw8XiaTtZfdRzd7firkmw1NiW9MiAU-nafaJGZSQedAkRmu1FuWyD4sNz8UxIYg2jhGynm0tuCdp-Pt37OV0VwmIz2zUeNFtiIgm_XrQ5bTBIkQwObIJKAfUz-UlEw-VCXw8Ty_t0m287sryA\" ri:version-at-save=\"1\" /></ac:image><h2>Argo app out of sync</h2><ol start=\"1\"><li><p><strong>General query:</strong> Argo app is showing out of sync</p><ol start=\"1\"><li><p>Debugging steps </p><ol start=\"1\"><li><p>In the argo UI we will be able to figure out which resource is actually out of sync<br />Ex <a href=\"https://prod-ops-argocd.meesho.com/applications/argocd/prd-ads-credit?view=tree&amp;resource=\"><u>LINK</u></a><br />Ref screenshot:</p></li><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"724\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"veZbkQCKjRTyGXDQX0TIUMvbLghDDK_cuJI7U6Dd2lsjeaz-qXjcSCoBlOjK9Yb6aaHmIDFVsRTdFf-l-n-SJhrAy-ElEHjvVFv676fGSFpjvH6CxZ76kD-JYojpFaeGjelasJytV9vSa5sPspPgh68\" ri:version-at-save=\"1\" /></ac:image></li><li><p>If the resource object is the deployment file, then please do check the image version or diff of the resource object as to what has changed. If the diff is about version change of image then please do check and sync<br />Ref screenshot:</p></li><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"940\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"LheKY5fK7HLwK4zVpTZuz_kqDn_S-FWnFAyorsHVNRCUb6ikI-B7HHDd3znBKzpHfcpVet7IGB6YyjMWqsBz6wzeOO4G2tLXMnXei4UdAYcj8_46lVo2VeJr2fbOmtyxnDeRwz466iqv-guxO949-74\" ri:version-at-save=\"1\" /></ac:image></li></ol></li><li><p>If the resource object is HPA, there might be chance that the min and max replica was changed in the cluster either using jenkins job or using kubectl command by devops-poc &amp; hence it might be showing the out of sync as to what values is present in values.yaml and cluster.</p></li><li><p>If the resource object is about ingress object prune, then check with your devops-poc to prune nginx ingress for the services serving traffic with contour(mostly prod services) rest(pre-prd) all should be using nginx.</p></li></ol></li></ol><h2>503 errors in envoy dashboard</h2><ol start=\"1\"><li><p><strong>General query:</strong> Service is seeing 503&rsquo;s (This may not the case every time)</p><ol start=\"1\"><li><p>Debugging steps:</p><ol start=\"1\"><li><p>Check if any new pod of service having 503&rsquo;s has come up recently</p></li><li><p>If the pod has any network issues, these cases may occur &rarr; Envoy dashboard - <a href=\"https://grafana.meesho.com/d/000000003/envoy-proxy?orgId=1\"><u>Link</u></a></p></li></ol></li><li><p>Please try to delete the new pods and check if the errors are reduced</p><ol start=\"1\"><li><p>You can do it vai argoCD UI only. Even devs have the access to delete pods and restart deployment as well.&nbsp;</p></li></ol></li></ol></li></ol><h2>Pod getting restarted in loop&nbsp;</h2><ol start=\"1\"><li><p><strong>General query:</strong> Pod getting restarted </p><ol start=\"1\"><li><p>Go to infra monitoring dashboard and check the service&rsquo;s CPU and Memory utilization</p><ol start=\"1\"><li><p>Infra monitoring dashboard - <a href=\"https://grafana.meesho.com/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=1m\"><u>Link</u></a>&nbsp;</p></li></ol></li><li><p>Example of such case - <a href=\"https://grafana.meesho.com/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;var-service=prd-return-recon-low-resource-cron-worker&amp;var-cluster=kube-state-metrics-p-supply-cluster&amp;var-namespace=prd-return-recon-low-resource-cron-worker&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-p-supply-cluster&amp;var-copy_of_namespace=prd-return-recon-low-resource-cron-worker&amp;from=1695137982722&amp;to=1695162121216&amp;viewPanel=129\"><u>Link</u></a></p><ol start=\"1\"><li><p>Here CPU was at 500%&nbsp;</p></li><li><p>Here is the PR for CPU req and limit update - <a href=\"https://github.com/Meesho/ReturnReconService/commit/e2be6ba4db9cde25b478881da43a8cf6456d5792\"><u>Link</u></a></p></li></ol></li></ol></li></ol><h2>Latency issues</h2><ol start=\"1\"><li><p><strong>General query:</strong> Latency issue in any service</p><ol start=\"1\"><li><p>Check the latency of your service from the below dashboard<br />Ex: <a href=\"https://grafana.meesho.com/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-cart_prd-cart-primary_80&amp;var-hosts=All&amp;from=now-3h&amp;to=now&amp;viewPanel=27\"><u>Link</u></a></p></li><li><p>If the latency has increased, please do check the path on which the service has become latent. This can be checked in the telegraf dashboard<br />Ex: <a href=\"https://grafana.meesho.com/d/m992dA14z/telegraf-services-dashboard-eks?orgId=1&amp;var-service=cart&amp;var-datasource=Prometheus&amp;viewPanel=10\"><u>Link</u></a></p></li><li><p>Developers needs to check with downstream services for further issues and fix it</p></li></ol></li></ol><h2>Pending connections error on contour envoy</h2><ol start=\"1\"><li><p><strong>General query:</strong> Latency issue in any service</p><ol start=\"1\"><li><p>If we are seeing circuit breaker limits increase in the below panel - &gt; <a href=\"https://grafana.meesho.com/d/000000003/envoy-proxy?orgId=1&amp;from=now-15m&amp;to=now&amp;viewPanel=32\"><u>Link</u></a></p></li><li><p>Refer this confluence doc: <a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2787049826/Contour+Migration+and+Slow+Start#Contour-Circuit-Breaker%2C-Client-Abort-and-Other-Configurations%3A\"><u>Link</u></a></p></li><li><p>In this case we will have to increase the connections limit by adding below annotations in the values_properties.yaml for that particular service:</p><ol start=\"1\"><li><p>Provide proper values for all these requests</p></li><li><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"94773a57-fc73-45b8-ac77-59b3e207eaf2\"><ac:plain-text-body><![CDATA[service_annotations:\n\n  projectcontour.io/max-connections: '1000000000'\n\n  projectcontour.io/max-pending-requests: '1000000000'\n\n  projectcontour.io/max-requests: '1000000000']]></ac:plain-text-body></ac:structured-macro></li></ol></li></ol></li></ol><p>This doc will be continued with new issues and solutions. </p><p />",
      "word_count": 898,
      "is_runbook": true
    },
    {
      "id": "3019341829",
      "title": "Karpenter Runbook - AWS EKS",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3019341829/Karpenter+Runbook+-+AWS+EKS",
      "created": "2023-09-22T06:58:26.878Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"b8d28f97-8b25-4a61-a9ff-1f52dba62578\"><tbody><tr><th><p><strong>Runbook Name</strong></p></th><th><p>EKS application debug runbook</p></th></tr><tr><td><p>Runbook description</p></td><td><p>This runbook guides through the karpenter files that we maintained and how to update the provisioners.</p></td></tr><tr><td><p>Author</p></td><td><p><ac:link><ri:user ri:account-id=\"61724e3aa9897100700dee57\" /></ac:link> <ac:link><ri:user ri:account-id=\"61c95b057c6f980070d8c4ec\" /></ac:link></p></td></tr><tr><td><p>Version</p></td><td><p>1.0</p></td></tr><tr><td><p>On this page</p></td><td><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"07a619e7-6b51-483a-b805-4131d050b61c\" ac:macro-id=\"a2335410-2a13-4ae3-b04b-a8ba75fe1bf8\"><ac:parameter ac:name=\"minLevel\">2</ac:parameter><ac:parameter ac:name=\"maxLevel\">6</ac:parameter><ac:parameter ac:name=\"outline\">false</ac:parameter><ac:parameter ac:name=\"type\">list</ac:parameter><ac:parameter ac:name=\"printable\">false</ac:parameter></ac:structured-macro></td></tr></tbody></table><h2>Karpenter provisioners list</h2><p>All the info about the kaprneter provisioners of all the clusters can be found under this repo<br /><strong>Repo</strong>: <a href=\"https://github.com/Meesho/cluster-config-eks/tree/master/eks-argo-apps/prod\" data-card-appearance=\"inline\">https://github.com/Meesho/cluster-config-eks/tree/master/eks-argo-apps/prod</a> <br /><strong>Path</strong>: &lt;cluster-name&gt;/karpenter/&lt;bu-name&gt;/*.yaml</p><p>Currently under Demand, Supply and Central clusters we have moved all the services from dedicated node pool to common node pool.</p><p>So below are the provisioners which are in use for Demand, Supply and Central clusters</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c6827794-1b20-4264-b155-99a0d175ef07\"><ac:plain-text-body><![CDATA[<cluster-name>-hypercore.yaml\n<cluster-name>-hypercore-arm64.yaml\n<cluster-name>-hypermem.yaml\n<cluster-name>-hypermem-arm64.yaml]]></ac:plain-text-body></ac:structured-macro><p /><p>In Datascience and Dataplatform we are still using a dedicated node-pool and hence the provisioners used are as per team names.</p><h2>Monitoring karpenter limits</h2><p>As we know that in each of the provisioner yaml we provide the CPU and memory resource limits. So we should monitor these limits and if any of the provisioner has reached its limit, karpenter will not provision nodes from that provisioner.</p><p>We do have setup alerts for limit breaching at 80%, devops-oncall will get pagerduty call and alert will also be sent to slack channel eks-infra-alerts</p><ul><li><p>To check the current usage of cpu or memory of the provisioner we can use below kubectl commands<br />Switch to the context of the cluster in which provisioner is deployed</p><ul><li><p>For example: p-demand-cluster<br /><code># kubectl config use-context arn:aws:eks:ap-southeast-1:847438129436:cluster/p-demand-cluster</code></p></li><li><p> Below command helps in listing the provisioners present in the cluster<br /><code># kubectl get provisioners</code></p></li><li><p>Below command helps in describing the particular provisioner<br /><code># kubectl describe provisioner p-demand-cluster-hypercore</code></p></li></ul></li></ul><p>Screenshot of the status section is shown below from the output of above command:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"316\" ac:original-width=\"1202\"><ri:attachment ri:filename=\"HQL4sm6jH-5Ku-st5pTop42qgk-wJkCYpX-YBBEFr15H9LPysExXD81ZI0-F8qx8EFmrqmaNsw3Kqf8FdND0Njpb8NtQ9zw4pZc6Ln5CfC_HevC1RMYD5uTkh90gJwKGnzGd3pfgicca27DmFhL6puo\" ri:version-at-save=\"1\" /></ac:image><h2>Grafana dashboard to monitor the cpu and memory usage </h2><ul><li><p>Grafana link for cpu: <a href=\"https://grafana.meesho.com/d/66Hk-b1Vz/eks-devops-infra?orgId=1&amp;var-cluster=p-demand-cluster&amp;var-namesapce=aws-nth&amp;from=now-1h&amp;to=now&amp;refresh=5s&amp;viewPanel=54\"><u>Link</u></a></p></li><li><p>Grafana link for memory: <a href=\"https://grafana.meesho.com/d/66Hk-b1Vz/eks-devops-infra?orgId=1&amp;var-cluster=p-demand-cluster&amp;var-namesapce=aws-nth&amp;from=now-1h&amp;to=now&amp;refresh=5s&amp;viewPanel=55\"><u>Link</u></a></p></li></ul><h2>Update karpeter limits</h2><p>Steps:</p><ol start=\"1\"><li><p>Increase the limits of cpu and memory for the required provisioner </p><ol start=\"1\"><li><p>Ex: <a href=\"https://github.com/Meesho/cluster-config-eks/blob/master/eks-argo-apps/prod/p-demand-cluster/karpenter/demand/p-demand-cluster-hypercore.yaml#L39C1-L40C1\"><u>Link</u></a></p></li></ol></li><li><p>Raise PR, get it approved from any of the two devops people</p></li><li><p>Merge the PR, it gets synced automatically post 3minutes.</p></li></ol>",
      "word_count": 332,
      "is_runbook": true
    },
    {
      "id": "3020226565",
      "title": "Jenkins Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3020226565/Jenkins+Runbook",
      "created": "2025-02-21T08:00:14.112Z",
      "author": "Arnav Andrew Jose",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p>To troubleshoot common issues with Jenkins main and worker nodes</p><h2>Memory issues</h2><p>The most frequent issue is disk fill up, this is more prominent is worker nodes, the memory gets filled up for following reason</p><ul><li><p>m2 cache, located at <code>~/.m2</code> folder</p></li><li><p>workspace, located at <code>$WORKSPACE</code> folder</p></li><li><p>Docker images</p></li></ul><p>You can check the free disk space, using <code>df -kh</code> command</p><p>For m2 cache we can delete older pkgs, before removing these check the pkg is available in remote repository, if it isn&rsquo;t with the service owner if it okay to remove it.</p><p>For docker images we can use <code>docker rmi &lt;image_id&gt;</code> to remove docker image <code>docker image prune</code> cmd also can be used here</p><p>For workspace folder, is configured at <code>Remote root directory</code>, refer the below screenshot for more details, after getting the workspace folder, it can deleted safely</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"861\" ac:original-width=\"1764\"><ri:attachment ri:filename=\"image-20230921-081437.png\" ri:version-at-save=\"1\" /></ac:image><p /><p>If the memory is still not free, please increase the disk space. You can take help from <a href=\"https://meesho.atlassian.net/people/team/32d57d1d-2aab-4ae1-9cf6-0927f78f9ac8\">app-support</a> (<ac:link><ri:user ri:account-id=\"6385b840489de2f7f46598ca\" /></ac:link>)team for this activity</p><h2>CPU issues</h2><p>CPU issues are more frequent in main/master node of Jenkins. Following are the debugging steps for each one respectively</p><h3>Worker Node</h3><p>The worker node mostly the CPU consumption is either due to a zombie process, a job stuck and consuming CPU or due to excessive load. For zombie process follow below steps:</p><ul><li><p>Run <code>htop</code> cmd</p></li><li><p>Check the process consuming most CPU</p></li><li><p>Get the PID of that process</p></li><li><p>Kill using <code>kill -9 &lt;PID&gt;</code></p></li></ul><p>If CPU load is more due to excessive load, please decrease the number of executors. Follow below steps for decreasing the count:</p><ul><li><p>Go to <code>Manage Jenkins -&gt; Manage Nodes -&gt; &lt;Desired Node&gt; -&gt; Configure</code></p></li><li><p>Check field <code>Number of executors</code> (Refer Screenshot below)</p></li><li><p>Decrease value of <code>Number of executors</code></p></li><li><p>Click Save</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"861\" ac:original-width=\"1764\"><ri:attachment ri:filename=\"image-20230921-081611.png\" ri:version-at-save=\"1\" /></ac:image><p>If above methods doesn&rsquo;t help, considering upscaling the instance type of worker node.</p><h3>Main/Master Node</h3><p>As there are only selected jobs being executed in Jenkins main node, we don&rsquo;t see much CPU issues here, and if present are hardly related to Jenkins process. Follow below steps</p><ul><li><p>Check the CPU consumption by running <code>htop</code> cmd</p></li><li><p>If Jenkins process is taking more CPU, restart Jenkins service and observe if CPU goes down, refer section <code>Restarting Jenkins service</code> for more details</p></li><li><p>If CPU doesn&rsquo;t go down even after restarting consider upscaling the ec2 instance</p></li><li><p>If any other process is consuming more CPU, kill the process using <code>kill -9 &lt;PID&gt;</code> command</p></li></ul><h2>Restarting Jenkins service</h2><p>To check the status of Jenkins service. Follow below steps:</p><ul><li><p>login via ssh/ssm to Jenkins main/master node</p></li><li><p>Run cmd <code>service jenkins status</code> to get the status of jenkins</p></li><li><p>Run cmd <code>service jenkins restart</code> to restart the jenkins service</p></li></ul><h1>Troubleshooting Pipeline errors</h1><p><ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Jenkins pipeline failures runbook\" ri:version-at-save=\"2\" /><ac:link-body>Jenkins pipeline failures runbook</ac:link-body></ac:link> </p>",
      "word_count": 428,
      "is_runbook": true
    },
    {
      "id": "3021307937",
      "title": "Cluster Mesh - AWS EKS",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3021307937/Cluster+Mesh+-+AWS+EKS",
      "created": "2023-09-22T07:04:03.603Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"b8d28f97-8b25-4a61-a9ff-1f52dba62578\"><tbody><tr><th><p><strong>Runbook Name</strong></p></th><th><p>Cilium cluster mesh runbook</p></th></tr><tr><td><p>Runbook description</p></td><td><p>This runbook guides through the cilium cluster component and its configurations. This doc also helps in understanding the alert set for api server. </p></td></tr><tr><td><p>Author</p></td><td><p><ac:link><ri:user ri:account-id=\"61724e3aa9897100700dee57\" /></ac:link> <ac:link><ri:user ri:account-id=\"61c95b057c6f980070d8c4ec\" /></ac:link></p></td></tr><tr><td><p>Version</p></td><td><p>1.0</p></td></tr><tr><td><p>On this page</p></td><td><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"710ab671-f33e-4ba2-ab86-6e2f644211ac\" ac:macro-id=\"a2335410-2a13-4ae3-b04b-a8ba75fe1bf8\"><ac:parameter ac:name=\"minLevel\">2</ac:parameter><ac:parameter ac:name=\"maxLevel\">6</ac:parameter><ac:parameter ac:name=\"outline\">false</ac:parameter><ac:parameter ac:name=\"type\">list</ac:parameter><ac:parameter ac:name=\"printable\">false</ac:parameter></ac:structured-macro></td></tr></tbody></table><h2>Current Cilium clustermesh connectivity&nbsp;</h2><p>In our production environment we have the connectivity between each of them as shown below:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"235\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"xRPbNSbLULE-pZ47ZQ4hUOb98AXm4wDKjSREPcOGvWcrhBxFmdd6BWJR9HohSA-gYg64UTGvQjp7EltyUUihvREmmWfpOBgoZloswjTGY6v-y789UpZxtuX2C6hlLIazuWzbAKQN8bn8LrWZaZScZkg\" ri:version-at-save=\"1\" /></ac:image><p>prod-ops-cluster is connected with all rest 7 prod clusters.<br />Prd-memcached-cluster is only connected with prod-ops-cluster as of now.<br />Rest our application clusters are connected with each other and prod-ops. Hence the number of connected clusters for these are 6.</p><h2>Cilium clustermesh status</h2><p>To check the status of the clustermesh of any particular cluster, we can execute below cilium commands</p><ul><li><p>Switch the context to particular cluster</p><ul><li><p>For example p-demand-cluster<br /># <code>kubectl config use-context arn:aws:eks:ap-southeast-1:847438129436:cluster/p-demand-cluster</code></p></li><li><p>Execute below command<br /><code># cilium clustermesh status</code><br />Below is the output of the command</p><p>This command shows the connectivity of the demand cluster with all other clusters</p></li></ul></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"714\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"SeUWiUtxgIJRFxkTRmAJ_fjfBLCgXCh3bknE5mUyd35sQP65NXbm2pxx73he3n9qGWCfPMzuMpwR9pBxTq_jpSYfYlycdT6gM2qlZQ0wCcEH8eQRqdF_CNRjbcrQLhcyFbMjmV15AuF_QFf01WLbzOA\" ri:version-at-save=\"1\" /></ac:image><p>In addition to this we also have a grafana dashboard where we can see the number of clusters connected with each cluster.<br />Link: <a href=\"https://grafana.meesho.com/d/7mDPN7B4k/cilium-clustermesh-status?orgId=1\"><u>Dashboard</u></a></p><h2>Cilium clustermesh enabling steps</h2><p>Each cluster connectivity is managed using terraform code and within the cilium helm file.</p><p>Steps:</p><ul><li><p>Initially when we enable clustermesh in the cilium helm file, it will generate below secrets Clustermesh-apiserver-server-certs</p><ul><li><p>We copy these secrets and store it in the aws secrets manager service.</p></li></ul></li><li><p>Naming of the aws secret manager would be &lt;cluster-name&gt;-clustermesh-secrets</p></li><li><p>The above steps also creates an classic loadbalancer which will be exposed for other clusters to connect with this cluster.</p></li><li><p>We have configured custom domain for each cluster and hence an aws record should be created in the hosted zone of `meesho.co`</p></li><li><p>The clustermesh-apiserver secrets should be imported from data block in terraform</p></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"b36e36b7-7d51-48e5-b1a8-5913835d01d9\"><ac:plain-text-body><![CDATA[#p-central\ndata \"aws_secretsmanager_secret\" \"p-central-cluster-clustermesh-secrets\" {\nname = \"p-central-cluster-clustermesh-secrets\"\n}\ndata \"aws_secretsmanager_secret_version\" \"p-central-cluster-clustermesh-secrets-current\" {\nsecret_id = data.aws_secretsmanager_secret.p-central-cluster-clustermesh-secrets.id\n}]]></ac:plain-text-body></ac:structured-macro><ul><li><p>Cilium clustermesh conifg should be added as shown below</p></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"228e0e8b-f686-48be-945b-29672f7b34d7\"><ac:plain-text-body><![CDATA[\"clustermesh.config.clusters[0].name\" = \"p-central-cluster\"\n\"clustermesh.config.clusters[0].address\" = \"p-central-cluster-clustermesh.internal.meesho.co\"\n\"clustermesh.config.clusters[0].port\" = \"2379\"\n\"clustermesh.config.clusters[0].tls.cacert\" = jsondecode(data.aws_secretsmanager_secret_version.p-central-cluster-clustermesh-secrets-current.secret_string)[\"ca_crt\"]\n\"clustermesh.config.clusters[0].tls.cert\" = jsondecode(data.aws_secretsmanager_secret_version.p-central-cluster-clustermesh-secrets-current.secret_string)[\"tls_crt\"]\n\"clustermesh.config.clusters[0].tls.key\" = jsondecode(data.aws_secretsmanager_secret_version.p-central-cluster-clustermesh-secrets-current.secret_string)[\"tls_key\"]]]></ac:plain-text-body></ac:structured-macro><h2>Cilium clustermesh alerting</h2><p>We have below alerts setup for clustermesh connectivity monitoring. These alerts will be triggered to pagerduty to devops-oncall and also will sent on the slack channel eks-infra-alerts</p><p>Alerts info: <br /><strong>AlertName</strong>: CiliumClusterMeshFailureAlert</p><p>Query: <code>cilium_clustermesh_remote_clusters{source_cluster=~&quot;p-.*.-cluster&quot;} &lt; 6</code> Explanation:</p><ul><li><p>6 is a constant value because the cluster connectivity currently links our production <code>bu</code> clusters with each other.</p></li><li><p>Our prod-ops cluster is connected to one additional cluster, namely the new <code>prd-memcached</code> cluster.</p></li><li><p>Currently, the <code>prd-memcached</code> cluster is only connected to the prod-ops cluster.</p></li><li><p>Therefore, for our <code>bu</code>based connections, we should trigger an alert if the connection value drops below 6.</p></li></ul><p><strong>AlertName</strong>: CiliumClusterMeshFailureAlertForProdOpsCluster</p><p>Query: <code>cilium_clustermesh_remote_clusters{source_cluster=&quot;prod-ops-cluster&quot;} &lt; 7</code> Explanation:</p><ul><li><p>Since Prod Ops is connected to one more cluster, the value is 7.</p></li></ul><p><strong>AlertName</strong>: CiliumClusterMeshFailureAlertForPrdMemcachedCluster</p><p>Query: <code>cilium_clustermesh_remote_clusters{source_cluster=&quot;prd-memcached-cluster&quot;} &lt; 1</code> Explanation:</p><ul><li><p>As the <code>prd-memcached</code> cluster is connected to only one cluster, the value is 1.</p></li></ul><p>PR: <a href=\"https://github.com/Meesho/terraform-infra/pull/4173\" data-card-appearance=\"inline\">https://github.com/Meesho/terraform-infra/pull/4173</a> </p>",
      "word_count": 476,
      "is_runbook": true
    },
    {
      "id": "3030712394",
      "title": "Service backup - Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3030712394/Service+backup+-+Runbook",
      "created": "2023-10-03T09:31:10.017Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"b8d28f97-8b25-4a61-a9ff-1f52dba62578\"><tbody><tr><th><p><strong>Runbook Name</strong></p></th><th><p>Cilium cluster mesh runbook</p></th></tr><tr><td><p>Runbook description</p></td><td><p>This runbook guides to create service in the backup cluster in case any overload on existing BU clusters wrt to nodes/pods.</p></td></tr><tr><td><p>Author</p></td><td><p><ac:link><ri:user ri:account-id=\"61724e3aa9897100700dee57\" /></ac:link> <ac:link><ri:user ri:account-id=\"61c95b057c6f980070d8c4ec\" /></ac:link></p></td></tr><tr><td><p>Version</p></td><td><p>1.0</p></td></tr><tr><td><p>On this page</p></td><td><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"d2385a5d-c261-4450-99ea-a3953b77038d\" ac:macro-id=\"a2335410-2a13-4ae3-b04b-a8ba75fe1bf8\"><ac:parameter ac:name=\"minLevel\">2</ac:parameter><ac:parameter ac:name=\"maxLevel\">6</ac:parameter><ac:parameter ac:name=\"outline\">false</ac:parameter><ac:parameter ac:name=\"type\">list</ac:parameter><ac:parameter ac:name=\"printable\">false</ac:parameter></ac:structured-macro></td></tr></tbody></table><h2>Purpose</h2><ul><li><p>If cluster is overloaded with any of the service and those services can be shifted to backup cluster <code>p-backup-cluster</code></p></li><li><p>A backup cluster is similar to our BU clusters but it would be a mixed of all services that will be moved in case of emergency due to cluster overload</p></li><li><p>Services created in the backup cluster only receive traffic. No deployments are supported here but deployment image can be patched using the existing Jenkins pipeline we have.&nbsp;</p></li></ul><p /><p>Pipeline to run to create application service to backup cluster - <a href=\"http://ci.meesho.co/view/automation/job/automation/job/backup-cluster-migration/job/backup-cluster-migration/\"><u>Link</u></a></p><p>Pipeline to update/patch the deployment -&nbsp;<a href=\"http://cicd-prod.meeshoint.in/job/devops-eks-deployment-full-image-update/\">Link</a></p><h2>Jenkins pipeline use </h2><ul><li><p>Input parameter</p><ul><li><p>Application name&nbsp;</p></li></ul></li></ul><p>Trigger the job. This job will create an argo app of service in the backup cluster. </p><h2>DNS configurations </h2><ul><li><p>Identify the DNS name of the service and update core dns file in respective BU clusters to disable svc-svc communication so that traffic will route via LB.&nbsp;</p></li><li><p>For single host single path </p><ul><li><p>Update DNS in route53 to weighted routing if the service has a single host and single path in httpproxy and point DNS records to BU cluster LB and backup cluster LB accordingly.</p></li></ul></li><li><p>For single host multiple path</p><ul><li><p>We have cloned backup-cluster tg&rsquo;s in all BU contour LB&rsquo;s. If there are multiple paths using single host and one service to be moved to backup cluster then update respective BU Lb&rsquo;s and route traffic to backup-cluster cloned tg&rsquo;s. </p></li></ul></li></ul>",
      "word_count": 266,
      "is_runbook": true
    },
    {
      "id": "3252977700",
      "title": "Telegraf",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3252977700/Telegraf",
      "created": "2024-03-20T10:35:20.649Z",
      "author": "Former user (Deleted)",
      "labels": [
        "troubleshooting"
      ],
      "space": "DEVOPS",
      "content": "<p> It has been observed in the past that pods are getting crash loopback due to the telegraf container sidecar resource issue. This can happen mainly due to the reason more metrics getting pushed out of telegraf containers.</p><p /><ul><li><p>To identify the issue with your service telegraf container, use argocd to view the logs . Click on the pods boxes and use telegraf from the drop-down menu. </p><ul><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"211\" ac:original-width=\"1332\" ac:custom-width=\"true\" ac:width=\"712\"><ri:attachment ri:filename=\"Screenshot 2024-03-20 at 3.44.16 PM.png\" ri:version-at-save=\"2\" /></ac:image><p> <br /></p></li></ul></li></ul><p /><ul><li><p>To identify the resource utilization of telegraf containers associated with your service go to this dashboard.&nbsp;</p></li></ul><p><a href=\"http://grafana-prd.meeshogcp.in/d/TdnCANL4z/telegraf-container-dashboard?orgId=1\"><u>http://grafana-prd.meeshogcp.in/d/TdnCANL4z/telegraf-container-dashboard?orgId=1</u></a></p><p /><ul><li><p>By default telegraf container&rsquo;s request for resources is limited to the following values.</p></li></ul><p>&nbsp;<br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"a66bef34-bc8f-4fc4-b8e9-25760ef9b34e\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[    Limits:\n      cpu:     200m\n      memory:  200Mi\n    Requests:\n      cpu:     10m\n      memory:  10Mi]]></ac:plain-text-body></ac:structured-macro><p /><p>To change the values to desired resources add the following in values_properties.yaml under podAnnotations filed, changing the values of CPU and Memory values as desired. </p><p>e.g:- </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c307d6a4-9614-47c3-9968-e9b9ac9533ed\"><ac:plain-text-body><![CDATA[podAnnotations:\n    telegraf.influxdata.com/limits-cpu: 1000m\n    telegraf.influxdata.com/limits-memory: 500Mi\n    telegraf.influxdata.com/requests-cpu: 200m\n    telegraf.influxdata.com/requests-memory: 200Mi]]></ac:plain-text-body></ac:structured-macro><p />",
      "word_count": 166,
      "is_runbook": true
    },
    {
      "id": "3580690467",
      "title": "Flagger - Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3580690467/Flagger+-+Runbook",
      "created": "2024-09-09T11:13:31.031Z",
      "author": "Vignesh Ganesan",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Hosting details:</strong></p><ul><li><p>Clusters : All clusters</p></li><li><p>Namespace : flagger-&lt;bu&gt;-prd</p></li><li><p>No. of replicas : 1 </p></li><li><p>Rollout-strategy : Recreate</p></li><li><p>Repo: <a href=\"https://github.com/Meesho/devops-flagger\" data-card-appearance=\"inline\">https://github.com/Meesho/devops-flagger</a> </p></li></ul><p><strong>Common Issues,usecases and their fixes:</strong></p><p><u>Not able to promote or approve canary:</u></p><ul><li><p>Usually in slack the canary-bot times out after 10mins, so it&rsquo;s possible to approve it via slack, so follow the manual approval process, Referral doc: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Canary manual promotion - How our bot works\" ri:version-at-save=\"10\" /><ac:link-body>Canary manual promotion - How our bot works</ac:link-body></ac:link> </p></li><li><p>Once after you approve via slack or api call, it can take atmost 120sec to proceed further.</p></li></ul><p><u>503&rsquo;s In canary pods:</u></p><ul><li><p>This behaviour is usually observed in cases where analysis interval is set to 60sec, This happens due to the reason that Keda is taking more than 60sec to create the HPA and scale the canary deployment,<br /><strong>To Resolve</strong> : Increasing the analysis interval to 120sec will fix this. (giving enough time for keda to create HPA and scale the canary deployment before it starts serving the live traffic).<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"99\" ac:original-width=\"301\" ac:custom-width=\"true\" ac:width=\"301\"><ri:attachment ri:filename=\"Screenshot 2024-09-09 at 3.33.06 PM.png\" ri:version-at-save=\"1\" /></ac:image><p> </p></li></ul><p><u>Triggering Canary Manually:</u></p><ul><li><p>For any reason if we want to start the canary process manually, Restart of the canary deployment will retrigger the canary process again.</p></li></ul><p><u>Degraded App State due to canary: (Not an issue)</u></p><ul><li><p>Due to canary the app health can show <strong>degraded</strong>.</p></li><li><p>This can occur due to 2 reasons:</p><ul><li><p>The analysis failed for the canary version (or)</p></li><li><p>The developer aborts the canary while it&rsquo;s asking for approval</p></li></ul></li><li><p>In this case the canary version is not promoted to primary</p></li><li><p>If we want to retrigger the canary for the same deployment then follow the above steps</p></li></ul><p><u>Rollback of Canary deployment:</u> </p><ul><li><p>If we want to rollback canary deployment in case of any issues, </p><ul><li><p>The first way we can do this, is by aborting the canary process</p></li><li><p>But if u have already promoted then it&rsquo;s not possible to directly rollback, in this case we need to update the spec in the canary deployment to the older version and then usual canary process starts, and promotes it to prod</p></li><li><p> <strong>If we want Immediate rollback </strong>, First we need to modify skip-analysis as <strong>true </strong> in canary CRD, and then update the deployment to the intended version.<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"79\" ac:original-width=\"246\" ac:custom-width=\"true\" ac:width=\"246\"><ri:attachment ri:filename=\"Screenshot 2024-09-09 at 3.57.02 PM.png\" ri:version-at-save=\"1\" /></ac:image></li></ul></li></ul><ac:structured-macro ac:name=\"note\" ac:schema-version=\"1\" ac:macro-id=\"4394a77c-1c06-4dfc-89c4-fe0964e9afb6\"><ac:rich-text-body><p><strong>Do not do this unless absolutely necessary!!!!</strong><br />If the old version(image) is not backwards compatible with the rest of the spec, it could break things.</p></ac:rich-text-body></ac:structured-macro><p /><p /><p /><p /><p />",
      "word_count": 401,
      "is_runbook": true
    },
    {
      "id": "3581116446",
      "title": "Rollout Service - Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3581116446/Rollout+Service+-+Runbook",
      "created": "2024-09-09T11:07:24.144Z",
      "author": "Vignesh Ganesan",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Hosting details:</strong></p><ul><li><p>Clusters: Demand, Supply &amp; Farmiso</p></li><li><p>Namespace: rollout-service</p></li><li><p>No. of replicas: 1</p></li><li><p>Rollout Strategy: rollingUpdate</p></li><li><p>Repo: <a href=\"https://github.com/Meesho/rollout-service\" data-card-appearance=\"inline\">https://github.com/Meesho/rollout-service</a> </p></li></ul><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"c415e146-78a5-4603-93e5-f850a583fbf4\"><ac:rich-text-body><p>First check if the <code>rollourServiceEnabled</code> flag is set to true in the canary object <code>status</code></p><p>If yes, it is using rollout-service else it is using flagger</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"380\" ac:original-width=\"587\" ac:custom-width=\"true\" ac:alt=\"image-20240909-103352.png\" ac:width=\"587\"><ri:attachment ri:filename=\"image-20240909-103352.png\" ri:version-at-save=\"1\" /></ac:image></ac:rich-text-body></ac:structured-macro><h2><strong>Common Issues/use cases and their fixes:</strong></h2><ol start=\"1\"><li><p>What is analysis approval?</p><ol start=\"1\"><li><p>In rollout-service in addition to promotion approval there is a new step where you need to give approval for analysis to start.</p></li><li><p>Unless this approval is given, canary analysis won&rsquo;t start.</p></li></ol></li><li><p>Not able to approve analysis/promotion with canary-bot</p><ol start=\"1\"><li><p>Rollout-service uses a different set of apis for approval: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Approval via API\" ri:version-at-save=\"8\" /><ac:link-body>Rollout Service - Manual Approval</ac:link-body></ac:link> </p></li></ol></li><li><p>If the above curl command return an error saying <code>Workflow not found</code></p><ol start=\"1\"><li><p> Check the canary object and verify if it has reached this approval stage (Should be in WaitingAnalysis/WaitingPromotion).</p></li><li><p>Unless it has reached approval stage, analysis approval curl command won&rsquo;t work.</p></li></ol></li><li><p>How to do a <strong>Soft Rollback</strong>?</p><ol start=\"1\"><li><p>Sending false in any of the above approvals commands will result in soft rollback.</p></li></ol></li><li><p>Update to new version when analysis is happening</p><ol start=\"1\"><li><p> Just deploy the new version. Within 120 seconds it will start rolling back the previously deployed version and start rolling out the new version.</p></li></ol></li><li><p>How to do a <strong>Immediate Rollback?</strong></p><ol start=\"1\"><li><p>First set <code>skipAnalysis</code> to <strong>true </strong>in canary object and then update the canary deployment spec.</p></li></ol></li></ol><ac:structured-macro ac:name=\"note\" ac:schema-version=\"1\" ac:macro-id=\"4fbe8a71-3911-474d-9aca-b0db121f5403\"><ac:rich-text-body><p><strong>Do not do this unless absolutely necessary!!!!</strong><br />If the old version(image) is not backwards compatible with the rest of the spec, it could break things.</p></ac:rich-text-body></ac:structured-macro><ol start=\"7\"><li><p>To offboard a service from rolout-service and back to flagger</p><ol start=\"1\"><li><p>Go the the argo service page of that particular BU&rsquo;s flagger</p></li><li><p>There should be a config map called <code>rollout-service</code></p></li><li><p>There should be a config.json file with the list of service names</p></li><li><p>You can either set it to false or remove it entirely</p></li><li><p>Then restart flagger. </p></li><li><p>Once restart wait for 2-3 mins, check if the <code>rollourServiceEnabled</code> flag is set to <strong>false</strong> in the canary object <code>status</code></p></li></ol></li></ol>",
      "word_count": 328,
      "is_runbook": true
    },
    {
      "id": "3582754819",
      "title": "Logs",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3582754819/Logs",
      "created": "2024-09-18T11:30:30.667Z",
      "author": "Aryaman Subhas Parida",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p>We have EFK( Elastic fluentd kibana) setup for logging. we are pulling prod and pre-prod logs over ES with retention of <strong>3 days</strong> and over GCS with retention <strong>180 days</strong>.</p><p>Refer these docs to find logs over GCS and Elastics seacrh.</p><ul><li><p><ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"SRE\" ri:content-title=\"Querying logs using Bigquery\" ri:version-at-save=\"9\" /><ac:link-body>Querying logs using Bigquery</ac:link-body></ac:link><br /><ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"EW\" ri:content-title=\"Service Observability/Logs\" ri:version-at-save=\"3\" /><ac:link-body>Service Observability/Logs</ac:link-body></ac:link> <br /><br /></p></li><li><p>In Elastics search we are collecting <strong>ERROR|WARN</strong> logs in prd environment however INFO can also be found over GCS only.<br /></p></li><li><p>Suppose in a particular cluster someone wants to enable <strong>DEBUG logs/INFO logs</strong> and wants to search through them. We can go in<span style=\"color: rgb(255,240,179);\"> </span><a href=\"https://github.com/Meesho/devops-infra-helm-charts\"><span style=\"color: rgb(76,154,255);\"><u>devops-infra-helm-charts</u></span></a> repo and in <a href=\"https://github.com/Meesho/devops-infra-helm-charts/blob/044518505c96bdc0eb8198b1ac99dddffc0fb081/helm-overrides/k8s-farmiso-prd-ase1/fluentd/custom-values.yaml#L571\">helm-overrides</a> folder based on the cluster go inside <a href=\"https://github.com/Meesho/devops-infra-helm-charts/blob/044518505c96bdc0eb8198b1ac99dddffc0fb081/helm-overrides/k8s-farmiso-prd-ase1/fluentd/custom-values.yaml#L571\"><u>fluentd/custom-values.yaml</u></a>. Now since we want to include debug logs we can include it as shown below :</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"178\" ac:original-width=\"490\" ac:custom-width=\"true\" ac:width=\"490\"><ri:attachment ri:filename=\"Screenshot 2024-09-09 at 7.12.12 PM.png\" ri:version-at-save=\"1\" /></ac:image><p> </p><p><strong>PR for reference</strong>: <a href=\"https://github.com/Meesho/devops-infra-helm-charts/pull/740/files\">https://github.com/Meesho/devops-infra-helm-charts/pull/740/files</a></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"824\" ac:original-width=\"3072\" ac:custom-width=\"true\" ac:alt=\"image-20240918-080649.png\" ac:width=\"736\"><ri:attachment ri:filename=\"image-20240918-080649.png\" ri:version-at-save=\"1\" /></ac:image><p> </p><ul><li><p>Additionally <strong>timekey</strong> is <u>30 min</u> so every 30 min it sends logs to <strong>gcs bucket</strong>.<br /></p></li></ul>",
      "word_count": 183,
      "is_runbook": true
    },
    {
      "id": "3605102604",
      "title": "RingMaster - Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605102604/RingMaster+-+Runbook",
      "created": "2024-09-18T07:53:44.194Z",
      "author": "Rahul Kumar",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Downscaling Freeze</strong> - This will first find the highest number of desired pods for the service in last 48 hours and then will update the minimum count to the same. </p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"279\" ac:original-width=\"1434\" ac:custom-width=\"true\" ac:alt=\"image-20240918-053315.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240918-053315.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Bulk Min Max Update</strong> - One of the most used feature for sale. It can help update min/max of multiple applications in one go. After things are updated here: as_min and as_max in deployment yaml will go down in precedence, and control will go to values_properties in devops-helm-charts repo.</p><p>(<span style=\"color: rgb(255,153,31);\">This action is supported only for superadmin users</span>).</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"365\" ac:original-width=\"1454\" ac:custom-width=\"true\" ac:alt=\"image-20240918-053632.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240918-053632.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Min Max Update -</strong> For one particular applications ( opposite of the above feature ), this api can be used. This is update min/max of desired applications and control will go to values.yaml and values_properties.yaml from depolyment.yaml</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"211\" ac:original-width=\"715\" ac:custom-width=\"true\" ac:alt=\"image-20240918-073100.png\" ac:width=\"715\"><ri:attachment ri:filename=\"image-20240918-073100.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Fetch secrets and Restart canary- </strong>This will fetch secrets and restart the canary object. The new pods of  canary will be created from the new secrets fetched from vault. Depending in the analysis result, it will be proceed accordingly.</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"111\" ac:original-width=\"746\" ac:custom-width=\"true\" ac:alt=\"image-20240918-073523.png\" ac:width=\"746\"><ri:attachment ri:filename=\"image-20240918-073523.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Zookeeper Dynamic configs: </strong>This allows you to view and compare differences between the GitHub configurations and the current Zookeeper settings.(this will work if config-as-code is onboarded)</p><p>path: repo_name/configs/module_name/application-dyn-env.yml</p><p>Other features: CPU based scaling change, Scheduled Autoscaling which can also be used as per need. </p>",
      "word_count": 248,
      "is_runbook": true
    },
    {
      "id": "3605561355",
      "title": "KEDA Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605561355/KEDA+Runbook",
      "created": "2024-09-18T07:40:48.791Z",
      "author": "Yash Nandwana",
      "labels": [],
      "space": "DEVOPS",
      "content": "<hr /><h4><strong>1. Issue: Service Not Scaling Up Despite Threshold Breach</strong></h4><p><strong>Issue Reported:</strong></p><ul><li><p>The service crosses the scaling threshold but does not scale up immediately.</p></li><li><p>Metrics fluctuate frequently, causing KEDA to not have enough time to trigger a scaling action.</p></li></ul><p><strong>Root Cause:</strong></p><ul><li><p>Metrics fall back to normal levels too quickly, not allowing KEDA to initiate the scale-up process.</p></li></ul><p><strong>Resolution:</strong></p><ul><li><p>Adjust the metric collection window or threshold to smooth out spikes in the data. This can help reduce false positives or negatives in the scaling mechanism.</p></li><li><p>Add a delay to the downscale threshold to avoid rapid changes.<br />Example:</p><ul><li><p>Modify the <code>cooldownPeriod</code> in the <code>ScaledObject</code> definition to give KEDA more time to stabilize metrics before deciding to scale.<br /><code>cooldownPeriod: 300</code></p></li></ul></li></ul><p /><hr /><p /><h4><strong>2. Issue: 503 Errors in Canary Due to Delayed Scaling</strong></h4><p><strong>Issue Reported:</strong></p><ul><li><p>503 errors are observed during a canary deployment phase.</p></li><li><p>The HPA is delayed in bringing up new pods, causing the service to be temporarily unavailable.</p></li></ul><p><strong>Root Cause:</strong></p><ul><li><p>The Horizontal Pod Autoscaler takes time to respond and spin up new pods. The canary analysis interval is too short for the HPA to effectively adjust.</p></li></ul><p /><p /><p><strong>Resolution:</strong></p><ul><li><p>Increase the canary analysis interval to allow enough time for the autoscaler to react to the changing load.<br />Example:</p><ul><li><p>Set the analysis interval to 120 seconds to provide sufficient time for the new pods to spin up before the analysis evaluates success.</p></li></ul></li></ul><p /><p><code>canaryAnalysis:</code></p><p><code>&nbsp;&nbsp;interval: 120s</code></p><p /><hr /><p /><p /><p /><p /><p />",
      "word_count": 221,
      "is_runbook": true
    },
    {
      "id": "3605430289",
      "title": "NodePool runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605430289/NodePool+runbook",
      "created": "2024-09-23T10:38:45.837Z",
      "author": "Pavan Kumar Reddy M",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Issue: GCE Out of Resources</strong></p><p>While pods are being scheduled on new nodes, we occasionally encounter the error message <code>ZONE_RESOURCE_POOL_EXHAUSTED</code>. This indicates that the resource pool in the current zone is depleted.</p><p><strong>Steps to Resolve the Issue:</strong></p><ol start=\"1\"><li><p><strong>Check the nodeSelector</strong>:</p><ul><li><p>Inspect the <code>nodeSelector</code> on the deployment/pod to see which node pool the service is using.</p></li></ul></li><li><p><strong>Enable Additional Zones</strong>:</p><ul><li><p>Open the GCP console / terraform-gcp-infra  repo and navigate to the node pool .</p></li><li><p>If only Zone A is enabled, enable additional zones such as Zone B and Zone C to increase capacity.</p></li></ul></li><li><p><strong>Create a New Node Pool</strong> (if capacity is exhausted in all zones):</p><ul><li><p>If capacity is unavailable across all enabled zones, create a new node pool with the same taints and labels as the existing one.</p></li><li><p>Consider adding a different machine type in the new node pool to better meet the resource demands.</p></li></ul></li><li><p><strong>Rescue-Nodepool: </strong><br />we are having one rescue node pool with taints ( <strong>sale-rescue</strong> ) which will spin up<strong> c3-standard-22</strong> instance types. We can move to this node pool incase any specific instance type not available in any of the zones.<br />This node pool is added in <strong>Demand</strong> , <strong>Supply</strong> , <strong>DataEngg</strong> and <strong>Datascience</strong> and <strong>Central</strong> BUs<br /></p></li></ol><h3>Issue: Optimizing Node Allocation to a Single Zone for Cost Savings (this activity should be done in night time)</h3><p>To minimize network costs, the goal is to consolidate nodes into a single zone once the capacity in <strong>Zone A</strong> has been updated. The following steps outline the process:</p><ol start=\"1\"><li><p><strong>List Nodes in Other Zones</strong>:</p><ul><li><p>Identify and list all nodes that are currently part of <strong>Zones B and C</strong>.</p></li></ul></li><li><p><strong>Pre-scale Nodes in Zone A</strong>:</p><ul><li><p>Increase the node capacity in <strong>Zone A</strong> by pre-scaling the number of nodes as needed.</p></li><li><p>Disable the autoscaling in the node pool.</p></li></ul></li><li><p><strong>Cordon the Nodes in Other Zones</strong>:</p><ul><li><p>Mark the nodes in <strong>Zones B and C</strong> as unschedulable by cordoning them. This ensures that no new pods are scheduled on these nodes.</p></li></ul></li><li><p><strong>Drain the Nodes</strong>:</p><ul><li><p>Safely drain the cordoned nodes to migrate running workloads to nodes in <strong>Zone A</strong>. This process will ensure that workloads are rescheduled without disruption.</p></li></ul></li><li><p><strong>Disable Zones B and C</strong>:</p><ul><li><p>Once all nodes in <strong>Zones B and C</strong> have been successfully drained, we can disable these zones to prevent any further resource allocation, thus achieving the goal of reducing network costs.</p></li></ul></li></ol><p />",
      "word_count": 366,
      "is_runbook": true
    },
    {
      "id": "3605266537",
      "title": "GKE Multi-Cluster Services (MCS) Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605266537/GKE+Multi-Cluster+Services+MCS+Runbook",
      "created": "2024-09-18T08:20:29.069Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p /><h4><strong>1. Issue: Multiple Connection Timeouts Across Services</strong></h4><p><strong>Symptoms:</strong></p><ul><li><p>Multiple services are experiencing connection timeouts.</p></li><li><p>These timeouts are occurring across almost every service within the GKE MCS architecture.</p></li></ul><p><strong>Root Cause:</strong></p><ul><li><p>A race condition happens when a pod's IP changes. MCS processes these IP changes to update the service routing across clusters.</p></li><li><p>In the event of an error during this process, MCS temporarily removes all retrieved IPs from the system, causing a network blip as it tries to fetch and update the IPs again.</p></li></ul><p><strong>Resolution:</strong></p><ul><li><p>Reach out to Google Support team. Keep mcs-importer logs handy.</p></li><li><p>We can find MCS logs over <a href=\"https://0b6ea1325493466faf2f9be526283f5a.psc.asia-southeast1.gcp.elastic-cloud.com/s/devopssre/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-15m,to:now))&amp;_a=(columns:!(),filters:!(),hideChart:!f,index:a97e1a9e-72ea-455b-924e-a9a58f1c3c50,interval:auto,query:(language:kuery,query:'application_name%20:%22gke-mcs%22%20and%20host:%20!'supply!'%20'),sort:!(!('@timestamp',desc)))\">ES</a>.</p></li><li><p>GCP team may ask for a faulty node sos report in some cases, follow the <a href=\"https://cloud.google.com/container-optimized-os/docs/how-to/sosreport\">steps</a> to get the same.</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"wide\" ac:original-height=\"853\" ac:original-width=\"1600\" ac:custom-width=\"true\" ac:width=\"862\" ac:src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe0Wcv8egSGK65w4XxJg_lQ2VhFOZSglH8H54d_lCrDC1IjcvPm3NVjqSv_ofPGV7WvOwdNuI-dRfVFq9ZSkoH7rKJhf7F2vDiP-I4fFblNdO2DjzK-gXq6oCaBAKaBQDCcSq0dnWuyfT_-c-nc-KbQCYk?key=3dBnc4c-tCPhmV5gU7ardw\"><ri:url ri:value=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe0Wcv8egSGK65w4XxJg_lQ2VhFOZSglH8H54d_lCrDC1IjcvPm3NVjqSv_ofPGV7WvOwdNuI-dRfVFq9ZSkoH7rKJhf7F2vDiP-I4fFblNdO2DjzK-gXq6oCaBAKaBQDCcSq0dnWuyfT_-c-nc-KbQCYk?key=3dBnc4c-tCPhmV5gU7ardw\" /></ac:image>",
      "word_count": 122,
      "is_runbook": true
    },
    {
      "id": "3605889053",
      "title": "Service Accounts and IAM",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605889053/Service+Accounts+and+IAM",
      "created": "2024-09-18T10:38:16.542Z",
      "author": "Former user (Deleted)",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Manual Firewall change Restriction:</strong></p><p><strong>&rarr; How it&rsquo;s been implemented?</strong></p><p>Following Deny policy has been implemented at org level:</p><p>Policy Id: prd-no-manual-firewall-change</p><p>To list the policy, use the following command:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"8ef0c003-c0e2-4fd4-8215-96591242a67d\"><ac:plain-text-body><![CDATA[gcloud iam policies list \\\n    --attachment-point=cloudresourcemanager.googleapis.com/organizations/589969641884 \\\n    --kind=denypolicies \\\n    --format=json]]></ac:plain-text-body></ac:structured-macro><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"528\" ac:original-width=\"1324\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-09-18 at 3.16.51 PM.png\" ri:version-at-save=\"1\" /></ac:image><p><u>To view the policy use the following command:</u></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"025fff44-e1d4-413a-8d82-9afbe8f46a7f\"><ac:plain-text-body><![CDATA[gcloud iam policies get prd-no-manual-firewall-change \\\n    --attachment-point=cloudresourcemanager.googleapis.com/organizations/589969641884 \\\n    --kind=denypolicies \\\n    --format=json]]></ac:plain-text-body></ac:structured-macro><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"967\" ac:original-width=\"1515\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-09-18 at 3.23.12 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>This policy has been applied to all those projects where tag value for key(&ldquo;env&ldquo;) is present as &ldquo;prd&ldquo;. For Eg.</p><p>For the admin folder projects tag is attached at Folder(Admin) level, which is getting inherited to it&rsquo;s child projects.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"823\" ac:original-width=\"1727\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-09-18 at 3.36.45 PM.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>&rarr; How to remove this restriction from a project?</strong></p><p>Just override the tag(env: prd) which is getting inherited from it&rsquo;s parent, for eg. if i want to remove the restriction from meesho-sec-admin-0622 project. For doing this follow the below steps:</p><ol start=\"1\"><li><p>Select the project as shown in image above.</p></li><li><p>Under &ldquo;Direct tags&ldquo; select scope as &ldquo;meesho.com&rdquo; organisation.</p></li></ol><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"268\" ac:original-width=\"730\" ac:custom-width=\"true\" ac:width=\"730\"><ri:attachment ri:filename=\"Screenshot 2024-09-18 at 3.52.04 PM.png\" ri:version-at-save=\"1\" /></ac:image><ol start=\"3\"><li><p>Now select the Tag value for key(&ldquo;env&ldquo;) as &ldquo;No Tag&ldquo;.</p></li></ol><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"993\" ac:original-width=\"1727\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-09-18 at 3.53.36 PM.png\" ri:version-at-save=\"1\" /></ac:image><p />",
      "word_count": 223,
      "is_runbook": true
    },
    {
      "id": "3606282242",
      "title": "MLP Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3606282242/MLP+Runbook",
      "created": "2024-09-18T11:18:43.226Z",
      "author": "Shivam Gupta1",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Issue: Models not able to be fetched from gcs bucket </strong></p><p>we have to bind the sa of gcp with kubernetes service account  </p><p>sample:<br />gcloud iam service-accounts add-iam-policy-binding&nbsp; <a href=\"mailto:sa-dsci-model-inference-servic@meesho-datascience-prd-0622.iam.gserviceaccount.com\">sa-dsci-model-inference-servic@meesho-datascience-prd-0622.iam.gserviceaccount.com</a> \\</p><p>&nbsp; &nbsp; --role roles/iam.workloadIdentityUser \\ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p>&nbsp; &nbsp; --member &quot;serviceAccount:meesho-datascience-prd-0622.svc.id.goog[prd-model-inference-service-search-ad-gpu/prd-model-inference-service-search-ad-gpu]&quot;<br /><br /></p><p><strong>Issue:</strong> <strong>Mis services onboarding can not be onboarded  via  ringmaster .</strong></p><p><strong>resolution: </strong>have created a job to onboard the svc quickly<br />just in case new deployment is required at the sale time use this job to onboard the service for model inference : <br /><a href=\"https://jenkins-prd.meeshogcp.in/job/mis-onboard/\">https://jenkins-prd.meeshogcp.in/job/mis-onboard/</a><br /><br /><br /></p>",
      "word_count": 95,
      "is_runbook": true
    },
    {
      "id": "3828613336",
      "title": "Ringmaster-logging",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3828613336/Ringmaster-logging",
      "created": "2025-01-17T12:04:38.914Z",
      "author": "Aryaman Subhas Parida",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p>How to Use Logging in RingMaster with Zap Logger</p><p>In the RingMaster project, logging is primarily handled using Zap, a high-performance, structured logging library developed by Uber. This document provides an overview of how logging is set up and how to use it throughout the project.</p><ol start=\"1\"><li><p><strong>Setting Up the Zap Logger</strong></p></li></ol><p>Before using the logger, you need to initialize and configure the Zap logger in your application. </p><p>In the ringmaster-backend, the Zap logger is already pre-configured in the pkg/logger. This logger is used throughout the application for structured logging, allowing you to log detailed and consistent information about the state of the application.</p><ol start=\"2\"><li><p><strong>Importing the Logger Package</strong></p></li></ol><p>To use Zap logging in your code, you need to import two packages:</p><p>RingMaster/pkg/logger: This is the central package for logging in the RingMaster project.</p><p><a href=\"http://go.uber.org/zap:\">go.uber.org/zap:</a> This is the Zap logging library used to log messages in a structured format.</p><p><span style=\"color: rgb(54,179,126);\">import ( &quot;RingMaster/pkg/logger&quot; &quot;go.uber.org/zap&quot; )</span></p><ol start=\"3\"><li><p><strong>Logging Levels in Zap</strong></p></li></ol><p>Zap supports different logging levels, which help categorize log messages based on their severity. The most commonly used log levels are:</p><p><strong> Info Level Logs</strong></p><p>These logs provide general information about the application's behavior. They are useful for tracking the flow of execution or recording important events.</p><p>Example: </p><p><span style=\"color: rgb(54,179,126);\">logger.Info(&quot;User requested to trigger build&quot;, zap.String(&quot;repoName&quot;, repoName), zap.String(&quot;branch&quot;, branch))</span></p><p><strong> Error Level Logs</strong></p><p>These logs are used when an error occurs. Logging errors is important for troubleshooting and identifying issues that may require attention.</p><p>Example:</p><p><span style=\"color: rgb(54,179,126);\">logger.Error(&quot;Failed to trigger CI build&quot;, zap.Error(err), zap.String(&quot;repoName&quot;, repoName))</span></p><p><strong> Debug Level Logs (Not enabled now in configuration)</strong></p><p>Debug logs are detailed logs, typically used for debugging purposes. These logs include granular information such as variable values, function parameters, or processing stages.</p><p>Example:</p><p><span style=\"color: rgb(54,179,126);\">logger.Debug(&quot;Fetching build details&quot;, zap.String(&quot;repoName&quot;, repoName), zap.Int(&quot;buildNo&quot;, buildNo))</span></p><p><strong>Warn Level Logs</strong></p><p>Warning logs indicate potential problems that may not be critical but could lead to issues in the future. Warnings are often used to highlight misconfigurations or non-critical failures.</p><p>Example:</p><p><span style=\"color: rgb(54,179,126);\">logger.Warn(&quot;Repository does not have a Jenkinsfile&quot;, zap.String(&quot;repoName&quot;, repoName))</span></p><ol start=\"4\"><li><p><strong>Structured Logging with Zap</strong></p></li></ol><p>Structured logging allows you to log key-value pairs, which helps organize and query logs more effectively. The Zap logger allows you to add context to your logs with structured data. This can be useful for tracking specific pieces of information (e.g., repository name, user email) across different log entries.</p><p>Common Zap Log Fields</p><p><span style=\"color: rgb(54,179,126);\">zap.String(key, value): Logs a string value with the specified key.</span></p><p><span style=\"color: rgb(54,179,126);\">zap.Int(key, value): Logs an integer value with the specified key.</span></p><p><span style=\"color: rgb(54,179,126);\">zap.Any(key, value): Logs any data type (useful for complex objects).</span></p><p><span style=\"color: rgb(54,179,126);\">zap.Error(err): Logs an error with its stack trace.</span></p><p />",
      "word_count": 415,
      "is_runbook": true
    },
    {
      "id": "3948544320",
      "title": "Jenkins pipeline failures runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3948544320/Jenkins+pipeline+failures+runbook",
      "created": "2025-02-21T08:00:45.324Z",
      "author": "Arnav Andrew Jose",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h2>Dockerfile failed to build</h2><h3><code>ERROR: failed to solve: process &quot;/bin/sh -c go build -tags musl --ldflags \\&quot;-extldflags -static\\&quot; -v -o /app/server cmd/price-aggregator-go/main.go&quot; did not complete successfully: exit code: 1</code></h3><h2>Skip analysis</h2><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"fedbbe7a-c892-458e-ba66-1443254bf71d\"><ac:plain-text-body><![CDATA[ERROR: Error Updating Helm Repo hudson.AbortException: Error: Update the skipAnalysis parameter\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] echo\n15:01:50  ERROR: hudson.AbortException: Error: Update the skipAnalysis parameter]]></ac:plain-text-body></ac:structured-macro><p /><h2>Validation failed for schema</h2><p><code>Validation failed for configs/Cart-Service/application-dyn-dev.yml against schema application-schema.yml</code></p><p /><h2>Error in cloning the repo</h2><p>If the error occurs in a <code>Multibranch Pipeline</code> while cloning the repo. Please follow below steps:</p><ul><li><p>Ensure that the credential being used here are of Github App. Refer Screenshot below</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"923\" ac:original-width=\"1776\" ac:width=\"1700\"><ri:attachment ri:filename=\"image-20230921-082942.png\" ri:version-at-save=\"1\" /></ac:image><p /></li></ul><p>If that is correct, reindex the repo by clicking on <code>Scan Repository now</code> at Job page</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"524\" ac:original-width=\"1777\" ac:width=\"1700\"><ri:attachment ri:filename=\"image-20230921-083118.png\" ri:version-at-save=\"1\" /></ac:image><p /><h2>Github 422 error (commit not found for SHA)</h2><p>This happens when your PR is behind the branch you are trying to merge to, and it cannot be fast-forwarded. Jenkins tries to rebase, creating a new commit which is not pushed to github.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"437ae1f1-d4bb-4c04-a33e-e0f309b893a1\"><ac:plain-text-body><![CDATA[Error during Quality Gate check: \nRequestError [HttpError]: No commit found for SHA: 7c50b5e31d844899e4af7353df5ce1fa0c71b2a1 \n  at /usr/src/app/node_modules/@octokit/request/dist-node/index.js\n  at processTicksAndRejections (node:internal/process/task_queues:96:5) { \n    status: 422,\n    ...\n    }]]></ac:plain-text-body></ac:structured-macro><h3>Solution:</h3><p>Pull the base branch, merge manually, push to github, and retrigger jenkins.</p><h2>Github 4xx error (not 422) while PR creation and merge</h2><p>This is the most reoccurring known error in pipeline. Looks like following</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"444\" ac:original-width=\"1433\" ac:width=\"1700\"><ri:attachment ri:filename=\"image-20230921-083354.png\" ri:version-at-save=\"1\" /></ac:image><h3>Solution</h3><ul><li><p>Get the branch name from the console output for which PR creation/merge is failed</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"894\" ac:original-width=\"1444\" ac:width=\"1700\"><ri:attachment ri:filename=\"image-20230921-083528.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>Go to devops-helm-chart repo in github</p></li><li><p>Click on branches</p></li><li><p>Select the branch copied from console output</p></li><li><p>Now there are two way to resolve the error</p></li><li><p>Delete the branch and any PR associate with it, and run the CICD job again</p></li><li><p>Create the PR if not created, merge and sync manually from ArgoCD</p></li><li><p>OR, merge the PR if created, and sync manually form ArgoCD</p></li><li><p>Both of the above options will resolve the above error</p></li></ul><p /><p />",
      "word_count": 326,
      "is_runbook": true
    },
    {
      "id": "4051337217",
      "title": "External Name Service for MCS and Cluster Fallback",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4051337217/External+Name+Service+for+MCS+and+Cluster+Fallback",
      "created": "2025-04-01T06:24:45.483Z",
      "author": "Mahak Jain",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h2><strong>1. Overview</strong></h2><p>To achieve multi-cluster service (MCS) communication and fallback in case of failures, we can utilize <strong>ExternalName Services</strong> setup in Kubernetes. <br />This setup will allows services to resolve dynamically based on <strong>CoreDNS rewrite rules</strong> and external name service configurations.</p><h3><strong>What is an ExternalName Service?</strong></h3><p>An <code>ExternalName</code> service in Kubernetes maps a service name to an external DNS name instead of a standard cluster-internal endpoint. This enables seamless redirection to different backends based on configurations.<br /></p><h2><strong>2. Setup in Meesho Admin Project</strong><br /></h2><ul><li><p><strong>An </strong><code>external-name-service</code><strong> is created for all MCS Contour imported services</strong>.</p></li><li><p>template - <a href=\"https://github.com/Meesho/devops-infra-argo-config/blob/main/external-name-service-template/templates/service.yaml\">https://github.com/Meesho/devops-infra-argo-config/blob/main/external-name-service-template/templates/service.yaml</a> </p></li><li><p>These services are defined in the <a href=\"https://github.com/Meesho/devops-infra-argo-config/tree/main/external-name-service-incubator/prd/ase1/zone-a\"><strong>incubator repository</strong></a> and deployed across all the BU clusters.</p></li><li><p><strong>CoreDNS Rewrite Rule</strong> is used to route traffic from:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"3d61be15-a996-4964-b7de-63ea29c11087\"><ac:plain-text-body><![CDATA[# for http\nservice.prd.meesho.int.svc.cluster.local  \n → externalName-namespace-envoy.namespace.cluster.local ]]></ac:plain-text-body></ac:structured-macro><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"f378132f-ebcc-49fd-a480-2bb2a156bcb6\"><ac:plain-text-body><![CDATA[# for grpc\nservice.prd.meesho.int.svc.cluster.local  \n → externalName-namespace-envoy-headless.namespace.cluster.local ]]></ac:plain-text-body></ac:structured-macro><p>The backend of the <code>externalName</code> service points to the <strong>Contour imported service and ILB FQDN</strong>, dynamically resolving based on the incubator settings defined in<a href=\"https://github.com/Meesho/devops-infra-argo-config/blob/main/external-name-service-values/prd/ase1/zone-a/external-name-service-incubator-common-prd-ase1a-values.yaml\"> incubator values yaml</a> .<br /></p></li></ul><h2><strong>3. How Backend Selection Works</strong></h2><p>To modify the backend, update the <strong>External Name Service values file</strong>. This file is <strong>common across all clusters</strong>, meaning changes apply cluster-wide.</p><h3><strong>Key Parameters in </strong><code>values.yaml</code></h3><ul><li><p><code>mcs</code>: Controls if the service should use <strong>Multi-Cluster Services (MCS)</strong>.</p></li><li><p><code>ilb.enabled</code>: Determines whether to use an <strong>Internal Load Balancer (ILB)</strong>.</p></li><li><p><code>zone</code>: Specifies the zone in which the ILB should be resolved.</p></li><li><p><code>svcSpec</code>: Defines service-specific configurations, including namespaces and protocol types.</p></li><li><p><code>labels</code>: From where Region and default Zone is selected</p></li></ul><h3><strong>Backend Selection Logic</strong></h3><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"315d0e6f-5e33-4365-b1e7-3ee0d4da1d5a\"><tbody><tr><th data-highlight-colour=\"#b3f5ff\"><p><strong>Condition</strong></p></th><th data-highlight-colour=\"#b3f5ff\"><p><strong>Backend Resolution</strong></p></th></tr><tr><td><p><code>mcs.central: on</code>, <code>supply-ilb.enabled: off</code></p></td><td><p><code>&lt;mcs-imported-service-name-for-supply&gt;.&lt;namespace&gt;.svc.clusterset.local</code></p></td></tr><tr><td><p><code>mcs.central: on</code>, <code>supply-ilb.enabled: on</code></p></td><td><p><code>&lt;namespace&gt;-&lt;region&gt;&lt;zone&gt;.&lt;env&gt;.meesho.int</code></p></td></tr><tr><td><p><code>mcs.central: off</code>, <code>supply-ilb.enabled: off/on</code></p></td><td><p><code>&lt;namespace&gt;-&lt;region&gt;&lt;zone&gt;.&lt;env&gt;.meesho.int</code></p></td></tr></tbody></table><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"706\" ac:original-width=\"1372\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-04-01 at 11.30.18 AM.png\" ri:version-at-save=\"2\" /></ac:image><h2><strong>4. Example Scenario</strong></h2><h3><strong>Example:</strong></h3><ul><li><p><strong>BU (Business Unit)</strong>: <code>supply</code></p></li><li><p><strong>Service Called</strong>: <code>devops-golang-app</code></p></li><li><p><strong>Service Running In</strong>: <code>central</code> BU</p></li><li><p><strong>Region</strong>: <code>ase1</code></p></li><li><p><strong>Zone</strong>: <code>a</code></p></li></ul><h4><strong>CoreDNS Rewrite Rule</strong></h4><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"f25a7c96-a6d4-442c-b8bd-862f044acf59\"><ac:plain-text-body><![CDATA[devops-golang-app.prd.meesho.int.svc.cluster.local   \n→ externalName-contour-internal-0-central-prd-envoy.contour-internal-0-central-prd.svc.cluster.local ]]></ac:plain-text-body></ac:structured-macro><p /><p>This routes traffic through the <strong>ExternalName Service</strong>, which then resolves based on <code>mcs</code> and <code>ilb</code> settings.</p><h3><strong>Backend Resolutions Based on </strong><code>mcs</code><strong> and </strong><code>ilb</code><br /><br /><strong>For </strong><code>http</code><strong> Protocol</strong></h3><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"3ed851c1-0c2c-4abc-bdcb-79e2f95e6724\"><colgroup><col style=\"width: 190.0px;\" /><col style=\"width: 190.0px;\" /><col style=\"width: 190.0px;\" /><col style=\"width: 190.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"#b3f5ff\"><p><code>mcs.supply</code></p></th><th data-highlight-colour=\"#b3f5ff\"><p><code>central-ilb.enabled</code></p></th><th data-highlight-colour=\"#b3f5ff\"><p><strong>Zone</strong></p></th><th data-highlight-colour=\"#b3f5ff\"><p><strong>Backend Resolved To</strong><br />(for http)</p></th></tr><tr><td><p><code>on</code></p></td><td><p><code>off</code></p></td><td><p><code>a</code></p></td><td><p><code>contour-internal-0-central-prd-envoy.contour-internal-0-central-prd.svc.clusterset.local</code></p></td></tr><tr><td><p><code>on</code></p></td><td><p><code>on</code></p></td><td><p><code>a</code></p></td><td><p><code>contour-internal-0-central-prd-ase1a.prd.meesho.int</code></p></td></tr><tr><td><p><code>off</code></p></td><td><p><code>off</code></p></td><td><p><code>a</code></p></td><td><p><code>contour-internal-0-central-prd-ase1a.prd.meesho.int</code></p></td></tr><tr><td><p><code>off</code></p></td><td><p><code>on</code></p></td><td><p>b<br /></p></td><td><p><code>contour-internal-0-central-prd-ase1b.prd.meesho.int</code></p></td></tr></tbody></table><p><br /><strong>For </strong><code>grpc</code><strong> Protocol</strong></p><p>When <code>protocol_type</code> is <code>grpc</code>, the service name follows a different naming convention. Instead of <code>&lt;service&gt;-envoy</code>, it uses <code>&lt;service&gt;-envoy-headless</code>.</p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"e385ee66-6fb0-446e-90f2-8f4226c62c6e\"><tbody><tr><th data-highlight-colour=\"#b3f5ff\"><p><code>mcs.supply</code></p></th><th data-highlight-colour=\"#b3f5ff\"><p><code>central-ilb.enabled</code></p></th><th data-highlight-colour=\"#b3f5ff\"><p><strong>Zone</strong></p></th><th data-highlight-colour=\"#b3f5ff\"><p><strong>Backend Resolved To </strong><br />(for grpc)</p></th></tr><tr><td><p><code>on</code></p></td><td><p><code>off</code></p></td><td><p><code>a</code></p></td><td><p><code>contour-internal-0-central-prd-envoy-headless.contour-internal-0-central-prd.svc.clusterset.local</code></p></td></tr><tr><td><p><code>on</code></p></td><td><p><code>on</code></p></td><td><p><code>a</code></p></td><td><p><code>tcp-contour-internal-0-central-prd-ase1a.prd.meesho.int</code></p></td></tr><tr><td><p><code>off</code></p></td><td><p><code>off</code></p></td><td><p><code>a</code></p></td><td><p><code>tcp-contour-internal-0-central-prd-ase1a.prd.meesho.int</code></p></td></tr><tr><td><p><code>off</code></p></td><td><p><code>on</code></p></td><td><p>b<br /></p></td><td><p><code>tcp-contour-internal-0-central-prd-ase1b.prd.meesho.int</code></p></td></tr></tbody></table><p><br /></p><hr /><h2><strong>5. How to Change the Backend?</strong></h2><p>To modify the backend:</p><ol start=\"1\"><li><p><strong>Update </strong><code>values.yaml</code><strong> in the External Name Service chart</strong> (common for all clusters).</p></li><li><p><strong>Ensure ArgoCD syncs the changes</strong> for all the BU incubator .<br /><a href=\"https://argocd-prd.meeshogcp.in/applications/externalname-incubator-k8s-central-prd-ase1a\">central</a> , <a href=\"https://argocd-prd.meeshogcp.in/applications/externalname-incubator-k8s-datascience-prd-ase1a\">datascience</a> , <a href=\"https://argocd-prd.meeshogcp.in/applications/externalname-incubator-k8s-supply-prd-ase1a\">supply</a> , <a href=\"https://argocd-prd.meeshogcp.in/applications/externalname-incubator-k8s-demand-prd-ase1a\">demand</a>, <a href=\"https://argocd-prd.meeshogcp.in/applications/externalname-incubator-k8s-dataengg-prd-ase1a\">dataengg</a>,  <a href=\"https://argocd-prd.meeshogcp.in/applications/externalname-incubator-k8s-farmiso-prd-ase1a\">farmiso</a></p></li></ol><h3><strong>Example Configuration Change</strong></h3><p>If <code>central-ilb.enabled: off</code> is changed to <code>on</code>, the backend will shift from:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"cf53ffde-a77f-4231-832b-d7100367324c\"><ac:plain-text-body><![CDATA[contour-internal-0-central-prd-envoy.contour-internal-0-central-prd.svc.clusterset.local ]]></ac:plain-text-body></ac:structured-macro><p>To:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"9f0a9d93-dec3-43b5-8049-e8f7b89d9a9c\"><ac:plain-text-body><![CDATA[contour-internal-0-central-prd-ase1a.prd.meesho.int ]]></ac:plain-text-body></ac:structured-macro><p>This ensures traffic routes to the correct ILB instead of <code>clusterset.local</code>.</p><hr /><h2><strong>6. Summary</strong></h2><ul><li><p>ExternalName services allow dynamic backend resolution across clusters.</p></li><li><p>CoreDNS rewrites <code>service.prd.meesho.int.svc.cluster.local</code> to an external service name.</p></li><li><p>Backend resolution depends on <code>mcs</code> and <code>ilb</code> settings.</p></li><li><p>To change the backend, update the <strong>values file</strong> and sync <strong>ArgoCD</strong>.</p></li></ul>",
      "word_count": 480,
      "is_runbook": true
    },
    {
      "id": "4054122534",
      "title": "Dashboards Index",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4054122534/Dashboards+Index",
      "created": "2025-06-16T10:25:06.863Z",
      "author": "Arnav Andrew Jose",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;var-service=prd-agent-support-aggregator&amp;var-cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-namespace=prd-agent-support-aggregator&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-copy_of_namespace=prd-agent-support-aggregator&amp;from=now-1h&amp;to=now\">Pod and container level metrics</a></p><p><a href=\"https://console.cloud.google.com/storage/browser/gcs-infr-dvps-meesho-logs-prd;tab=objects?forceOnBucketsSortingFiltering=true&amp;invt=AbuR0g&amp;orgonly=true&amp;project=meesho-admin-prd-0622&amp;supportedpurview=organizationId,project&amp;prefix=&amp;forceOnObjectsSortingFiltering=false\">Rotated logs after 3 days</a></p><p><a href=\"https://grafana-prd.meeshogcp.in/dashboards/f/Dr-R37A4k/contour-envoy-eks\">Contour and Envoy Proxy metrics</a></p><p />",
      "word_count": 17,
      "is_runbook": true
    },
    {
      "id": "4182868102",
      "title": "Skipping CI checks",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4182868102/Skipping+CI+checks",
      "created": "2025-06-09T08:00:19.179Z",
      "author": "Arnav Andrew Jose",
      "labels": [],
      "space": "DEVOPS",
      "content": "",
      "word_count": 0,
      "is_runbook": true
    },
    {
      "id": "4230021642",
      "title": "Runbook Template",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4230021642/Runbook+Template",
      "created": "2025-07-03T10:52:10.548Z",
      "author": "Kandhavelu Muthuvelan",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h3><strong>&nbsp;1. Metadata</strong></h3><ul><li><p><strong>Category</strong>:</p></li><li><p><strong>Subcategory</strong>:</p></li><li><p><strong>Severity</strong>: (P0 / P1 / P2 / P3)</p></li><li><p><strong>Frequency</strong>: (Frequent / Occasional / Rare)</p></li><li><p><strong>Author</strong>: (Person)</p></li><li><p><strong>Last Updated</strong>: (YYYY-MM-DD)</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;2. Problem Statement</strong></h3><ul><li><p><strong>What is the issue?</strong><br /><em>(Provide a brief summary of the problem and its relevance)</em></p></li><li><p><strong>How is it detected?</strong></p><ul><li><p>Alerts (monitoring tools)</p></li><li><p>Log anomalies</p></li><li><p>User complaints or tickets</p></li><li><p>Dashboard irregularities</p></li></ul></li><li><p><strong>Symptoms</strong>:<br /><em>(Describe typical indicators or manifestations of the problem)</em></p></li></ul><p /><hr /><p /><h3><strong>&nbsp;3. Root Cause Identification</strong></h3><ul><li><p><strong>Possible root causes</strong>:<br /><em>(List known or likely contributing factors)</em></p></li><li><p><strong>Diagnostic steps</strong>:</p><ul><li><p>Review recent changes or deployments</p></li><li><p>Analyze related logs or events</p></li><li><p>Compare against working environments</p></li><li><p>Validate configurations and integrations</p></li></ul></li></ul><p /><hr /><p /><h3><strong>&nbsp;4. Troubleshooting Steps</strong></h3><p><em>Provide a structured approach to investigating the issue.</em></p><ul><li><p>Identify the scope and impact</p></li><li><p>Review system metrics and alerts</p></li><li><p>Check configuration settings and recent modifications</p></li><li><p>Consult related runbooks or historical incidents</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;5. Resolution Steps</strong></h3><p><em>Outline clear, step-by-step instructions for resolving the issue.</em></p><ul><li><p>Apply configuration or code fixes</p></li><li><p>Restart or redeploy affected components</p></li><li><p>Re-run failed processes or workflows</p></li><li><p>Notify relevant stakeholders as needed</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;6. Verification</strong></h3><ul><li><p>Confirm resolution via:</p><ul><li><p>Service health checks</p></li><li><p>Monitoring dashboards</p></li><li><p>Absence of recurring alerts or symptoms</p></li><li><p>Functional validation tests</p></li></ul></li><li><p><strong>Expected outcome</strong>:<br /><em>(Describe what success looks like)</em></p></li></ul><p /><hr /><p /><h3><strong>&nbsp;7. References</strong></h3><ul><li><p>Internal documentation</p></li><li><p>Service wikis</p></li><li><p>Monitoring dashboards</p></li><li><p>Incident history or related tickets</p></li></ul><p />",
      "word_count": 174,
      "is_runbook": true
    },
    {
      "id": "4229726814",
      "title": "Runbook: Pod Not Starting",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4229726814/Runbook+Pod+Not+Starting",
      "created": "2025-07-03T10:54:10.083Z",
      "author": "Kandhavelu Muthuvelan",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h3>1. Metadata</h3><ul><li><p><strong>Category</strong>: Deployment</p></li><li><p><strong>Subcategory</strong>: Pod Not Starting</p></li><li><p><strong>Severity</strong>: P1 (depending on the service)</p></li><li><p><strong>Frequency</strong>: Frequent</p></li><li><p><strong>Impacted Environments</strong>: Dev / Staging / Production</p></li><li><p><strong>Primary Owner</strong>: DevOps / Platform Team</p></li><li><p><strong>Last Updated</strong>: 2025-07-03</p></li><li><p><strong>Related Services/Components</strong>: Kubernetes, ArgoCD, Helm, CI/CD Pipeline</p></li></ul><hr /><h3>2. Problem Statement</h3><ul><li><p><strong>What is the issue?</strong><br />One or more Kubernetes pods fail to transition into a running state. This typically results in service unavailability or degraded functionality.</p></li><li><p><strong>How is it detected?</strong></p><ul><li><p>Monitoring alerts for pod crash or deployment failures</p></li><li><p>Kubernetes dashboards or CLI showing pods in <code>Pending</code>, <code>CrashLoopBackOff</code>, or <code>Error</code> states</p></li><li><p>Failed health checks or service endpoints not responding</p></li></ul></li><li><p><strong>Symptoms</strong>:</p><ul><li><p>Service is unreachable or degraded</p></li><li><p>Application errors due to missing dependencies</p></li><li><p>Continuous pod restarts</p></li></ul></li></ul><hr /><h3>3. Root Cause Identification</h3><ul><li><p><strong>Possible root causes</strong>:</p><ul><li><p>Misconfigured container image or tag</p></li><li><p>Missing environment variables or secrets</p></li><li><p>Node resource exhaustion</p></li><li><p>Network policy restrictions</p></li><li><p>Init container or volume mount issues</p></li><li><p>Crash due to application error on startup</p></li></ul></li><li><p><strong>Diagnostic steps</strong>:</p><ul><li><p>Review recent deployments or pipeline runs</p></li><li><p>Inspect events, pod status, and container logs</p></li><li><p>Check resource limits and requests</p></li><li><p>Validate configMaps, secrets, and service bindings</p></li><li><p>Compare behavior across environments</p></li></ul></li></ul><hr /><h3>4. Troubleshooting Steps</h3><ul><li><p>Determine whether the failure is at the scheduling, container startup, or readiness stage</p></li><li><p>Evaluate pod and node events to identify resource or permission issues</p></li><li><p>Investigate if any dependencies (e.g., services, secrets, volumes) are misconfigured or unavailable</p></li><li><p>Correlate with any recent config or image changes</p></li></ul><hr /><h3>5. Resolution Steps</h3><ul><li><p>Correct configuration or environment variables if incorrect</p></li><li><p>Ensure image exists and is accessible</p></li><li><p>Adjust resource limits if pods are unschedulable</p></li><li><p>Restore any deleted or rotated secrets/configs</p></li><li><p>Roll back to a previously working version if needed</p></li></ul><hr /><h3>6. Verification</h3><ul><li><p>Confirm pods reach <code>Running</code> state and stay stable</p></li><li><p>Service endpoints are reachable and healthy</p></li><li><p>No restart loops or crash logs</p></li><li><p>Relevant alerts are cleared</p></li><li><p><strong>Expected outcome</strong>:<br />The pod starts successfully and serves traffic without errors or instability</p></li></ul><hr /><h3>7. References</h3><ul><li><p>[Kubernetes Troubleshooting Guide]</p></li><li><p>[ArgoCD Sync Failure Docs]</p></li><li><p>[Service Configuration Checklist]</p></li><li><p>[Recent Incident RCA or Logs]</p></li></ul>",
      "word_count": 273,
      "is_runbook": true
    },
    {
      "id": "4236050521",
      "title": "Devops Buddy usage",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4236050521/Devops+Buddy+usage",
      "created": "2025-07-07T08:24:36.104Z",
      "author": "Arnav Andrew Jose",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p>The devops buddy user interface consists of a slack bot and a web UI.</p><p>The web UI is at <a href=\"https://devops-buddy.meeshogcp.in\">https://devops-buddy.meeshogcp.in</a>, and the bot watches for messages on <a href=\"https://meesho.slack.com/archives/C021QNS6JLV\">#devops-tech</a> and <a href=\"https://meesho.slack.com/archives/C03K57Y163S\">#sre-tech</a></p><h1>Web UI features</h1><h2>Home page</h2><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"428\" ac:original-width=\"1728\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-07-07 at 12.50.33 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>By default, the home page opens filtered to tickets assigned to the logged-in user, and the last day.</p><p>The &ldquo;Load older messages (1 day)&rdquo; is a shortcut to increase the filtered time to a day before, that is, if we are seeing messages for July 6 and 7, we will then see messages for July 5</p><h3>Filters</h3><ul><li><p>Assignee: Can select individual devops engineers, view all, or view only unassigned tickets</p></li><li><p>Env: Staging, Pre-prod, and prod</p></li><li><p>Status: We can see messages that are unacknowledged, unresolved, or resolved</p></li><li><p>From and To: Set the datetimes for when the messages were sent (inclusive)</p></li></ul><p>After changing the filter parameters, we can click the &ldquo;Apply filters&rdquo; button, or press the enter key on the keyboard</p><h1>Bot interaction</h1><h2>Initial issue creation</h2><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"585\" ac:original-width=\"490\" ac:custom-width=\"true\" ac:width=\"490\"><ri:attachment ri:filename=\"Screenshot 2025-07-07 at 1.19.28 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p>Initial message by Infra buddy bot</p></ac:caption></ac:image><p>Based on the impact, the priority needs to be chosen. The information for the priorities is given in the bot message:</p><ul><li><p><em>Prod Impacting</em>: Direct and immediate impact on production services leading to downtime or degradation of service. Pod crashes, evictions, CDN and DNS misconfigurations</p></li><li><p><em>Prod Risk</em>: Potential risk to service but no immediate disruption. Latency, Hotfix stuck.</p></li><li><p><em>Infra support</em>: Any other task, that is not of immediate concern.</p></li></ul><p>These are also referred to respectively as &lsquo;P0&rsquo;, &lsquo;P1&rsquo;, &lsquo;P2&rsquo;.</p><p>P0 and P1 <em>only</em> go to devops-oncall, while P2 is assigned to non-oncall.</p><h2>Assignee and ticket</h2><p>After the issue is created, the bot posts a message filled with the information the user supplied to the form.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"225\" ac:original-width=\"492\" ac:custom-width=\"true\" ac:width=\"492\"><ri:attachment ri:filename=\"Screenshot 2025-07-07 at 1.25.15 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>The message contains a Jira link, the category, optionally the service name if needed, and the assignee.</p><h2>Acknowledging and resolving</h2><p>We track time taken to acknowledge and resolve, and so this is important. This can be done in 3 ways:</p><h3>On the original message</h3><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"510\" ac:original-width=\"356\" ac:custom-width=\"true\" ac:width=\"356\"><ri:attachment ri:filename=\"Screenshot 2025-07-07 at 1.29.15 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>The first emoji, with shortcode <code>:ack:</code> is used to acknowledge a message, and the second emoji in the list, with shortcode <code>:white_tick:</code>, is used to resolve the message. The message is also automatically acknowledged once the devops team responds to it. </p><p>Acknowledgement and resolution can only be done once the initial form is filled, and acknowledgement <strong>MUST</strong> be done before resolution</p><h3>On the Jira link</h3><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"641\" ac:original-width=\"1117\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-07-07 at 1.33.32 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>On the Jira ticket linked in the message, the drop-down next to the summary contains some options. &ldquo;In Progress&rdquo; corresponds to acknowledgement, and &ldquo;Completed&rdquo; corresponds to resolution.</p><h3>On the web UI</h3><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"168\" ac:original-width=\"1486\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2025-07-07 at 1.36.03 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>You can filter to messages that you want to acknowledge/resolve using the <a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4236050521/Devops+Buddy+usage#Filters\">tutorial in the Web UI section</a>, and use the buttons to acknowledge/resolve. The buttons have titles that show on hover. On the left, below the message, you can see text that says &ldquo;acknowledged in 8 minutes&rdquo;. This will say &ldquo;resolved&rdquo; if the message is resolved, and will be empty if it is neither acknowledged or resolved.</p><h2>Escalation and de-escalation</h2><p>Under a message, you can mention the bot and write &lsquo;esc&rsquo; to escalate, and &lsquo;desc&rsquo; to de-escalate.</p><p>For either, a new priority and reason for priority change must be chosen. If priority cannot be changed in the desired direction, you will get an error message</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"165\" ac:original-width=\"477\" ac:custom-width=\"true\" ac:width=\"477\"><ri:attachment ri:filename=\"Screenshot 2025-07-07 at 1.44.28 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p>Choosing new priority</p></ac:caption></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"110\" ac:original-width=\"463\" ac:custom-width=\"true\" ac:width=\"463\"><ri:attachment ri:filename=\"Screenshot 2025-07-07 at 12.34.34 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p>Failure to de-escalate</p></ac:caption></ac:image><p /><p /><p />",
      "word_count": 617,
      "is_runbook": true
    },
    {
      "id": "4241457165",
      "title": "5XX errors runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4241457165/5XX+errors+runbook",
      "created": "2025-07-09T09:07:47.642Z",
      "author": "Arnav Andrew Jose",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h3><strong>&nbsp;1. Metadata</strong></h3><ul><li><p><strong>Category</strong>: App Stability</p></li><li><p><strong>Subcategory</strong>: Service Unavailable</p></li><li><p><strong>Severity</strong>: P0</p></li><li><p><strong>Frequency</strong>: Occasional</p></li><li><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"712020:3872595d-d4f7-4295-ae8f-60c8b18019cb\" ri:local-id=\"dd0102ef-08f9-4e73-b21d-ad41b87cc5cd\" /></ac:link> </p></li><li><p><strong>Last Updated</strong>: <time datetime=\"2025-07-09\" /> </p></li></ul><p /><hr /><p /><h3><strong>&nbsp;2. Problem Statement</strong></h3><ul><li><p><strong>What is the issue?</strong><br /><em>(Provide a brief summary of the problem and its relevance)</em></p></li><li><p><strong>How is it detected?</strong></p><ul><li><p>Alerts (monitoring tools)</p></li><li><p>Log anomalies</p></li><li><p>User complaints or tickets</p></li><li><p>Dashboard irregularities</p></li></ul></li><li><p><strong>Symptoms</strong>:<br /><em>(Describe typical indicators or manifestations of the problem)</em></p></li></ul><p /><hr /><p /><h3><strong>&nbsp;3. Root Cause Identification</strong></h3><ul><li><p><strong>Possible root causes</strong>:</p></li></ul><ol start=\"1\"><li><p><strong>CPU spike</strong>: which can be because of:</p><ul><li><p><strong>Thread creation </strong>- Initialising internal data structures and thread pools</p></li><li><p><strong>GC trigger</strong> due to class loading and threads creation on startup</p></li><li><p><strong>JIT compilation</strong> - JVM loads and compiles classes on first calls</p></li><li><p><strong>Downstream connection creation</strong> - overhead due to pool creation for databases and caches</p></li></ul></li><li><p><strong>Lazy initialization</strong>: of any downstream connection - New connections may be getting created under load, causing timeouts? (Check the connection creation process - eager or lazy). Meaning connection pool creation is not complete and traffic starts on the pod.</p></li><li><p><strong>Cold new pods</strong>: Pods not warmed up enough on start and starts getting heavy traffic (check aggression values)</p></li><li><p><strong>Incomplete JIT warmup</strong>: JIT warmup (converts frequently used methods to native machine code from byte code) not completed before traffic hits the pods - (can check JIT level logs)</p></li><li><p><strong>Node under load</strong>: Pods coming up on nodes which are already under heavy load or high cpu utilization (check cpu cputilization of nodes where new pods are coming up)</p></li></ol><ul><li><p><strong>Diagnostic steps</strong>:</p><ul><li><p>Review recent changes or deployments</p></li><li><p>Analyze related logs or events</p></li><li><p>Compare against working environments</p></li><li><p>Validate configurations and integrations</p></li></ul></li></ul><p /><hr /><p /><h3><strong>&nbsp;4. Troubleshooting Steps</strong></h3><p><em>Provide a structured approach to investigating the issue.</em></p><ul><li><p>Identify the scope and impact</p></li><li><p>Review system metrics and alerts</p></li><li><p>Check configuration settings and recent modifications</p></li><li><p>Consult related runbooks or historical incidents</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;5. Resolution Steps</strong></h3><p><em>Outline clear, step-by-step instructions for resolving the issue.</em></p><ul><li><p>Apply configuration or code fixes</p></li><li><p>Restart or redeploy affected components</p></li><li><p>Re-run failed processes or workflows</p></li><li><p>Notify relevant stakeholders as needed</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;6. Verification</strong></h3><ul><li><p>Confirm resolution via:</p><ul><li><p>Service health checks</p></li><li><p>Monitoring dashboards</p></li><li><p>Absence of recurring alerts or symptoms</p></li><li><p>Functional validation tests</p></li></ul></li><li><p><strong>Expected outcome</strong>:<br /><em>(Describe what success looks like)</em></p></li></ul><p /><hr /><p /><h3><strong>&nbsp;7. References</strong></h3><p>Incident:</p><ul><li><p>This is not a canary issue (This is happening while the time of rolling out canary deployment to primary)</p></li></ul><p>Why it happend:</p><ul><li><p>Basically i can see this is the usual deployment rollout process there is no deviation from it, there is no suddenly 2 pods went down simultaneously,</p></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"26f31bfc-f80f-44c7-ab77-1f4154dfc90a\"><ac:plain-text-body><![CDATA[    rollingUpdate:\n      maxSurge: 5%\n      maxUnavailable: 0%]]></ac:plain-text-body></ac:structured-macro><p>since they configured this 2 pods came up in the new replicaset and 2 pods went down in old replica set,</p><ul><li><p>why 5xx:</p></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"49fb6bc2-6944-4dcf-9acf-6f311b27de57\"><ac:plain-text-body><![CDATA[          slowStartPolicy:\n            aggression: '0.1'\n            minWeightPercent: 1\n            window: 120s]]></ac:plain-text-body></ac:structured-macro><p>since they configured this in the slow start policy, the traffic will eventually start to the new pods but old pods will pods terminate once the new pods became healthy, It doesn't wait for the new pods to reach 100% traffic</p><ul><li><p>why this is not happening usually?</p></li></ul><p>As mentioned by shubam,&nbsp; The thing is here when this rollout process is happening, the traffic to the service also got increased, so the other pods can't able to handle the surge which caused this 5xx&nbsp;(edited)&nbsp;</p><p><a href=\"https://meesho.slack.com/archives/C02MCALCX3R/p1752038477963179?thread_ts=1752033148.471069&amp;cid=C02MCALCX3R\">10:51</a></p><p><a href=\"https://grafana-prd.meeshogcp.in/d/0000000034/envoy-proxy-nav?orgId=1&amp;var-cluster=prd-rx-feed-aggregator_prd-rx-feed-aggregator-primary_80&amp;var-hosts=All&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.90&amp;var-latency_percentile_period=1m&amp;var-latency_percentile_threshold=0.90&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d&amp;from=1751266480195&amp;to=1751272290340\">https://grafana-prd.meeshogcp.in/d/0000000034/envoy-proxy-nav?orgId=1&amp;var-cluster=prd-rx-[&hellip;]old_min_over_time=7d&amp;from=1751266480195&amp;to=1751272290340</a><br />U can see clearly there is an increase of around 200 RPS exactly at 1.16 when 5xx started happening</p><ul><li><p>Internal documentation</p></li><li><p>Service wikis</p></li><li><p>Monitoring dashboards</p></li><li><p>Incident history or related tickets</p></li></ul><p />",
      "word_count": 513,
      "is_runbook": true
    },
    {
      "id": "4262494815",
      "title": "Runbook: Akamai Config Onboarding and Modififcations",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4262494815/Runbook+Akamai+Config+Onboarding+and+Modififcations",
      "created": "2025-07-20T17:02:21.183Z",
      "author": "Amul P Kesrani",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h3><strong>&nbsp;1. Metadata</strong></h3><ul><li><p><strong>Category</strong>: Akamai</p></li><li><p><strong>Subcategory</strong>: Config Management</p></li><li><p><strong>Severity</strong>:  P3</p></li><li><p><strong>Frequency</strong>: Occasional</p></li><li><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"6257b98b9770e600716a3d75\" ri:local-id=\"dd0a8d54-0ca6-4dcc-9e16-3eed85dd80ac\" /></ac:link> </p></li><li><p><strong>Last Updated</strong>: (2025-07-20)</p></li></ul><p>&nbsp;</p><hr /><p>&nbsp;</p><h3><strong>&nbsp;2. Problem Statement</strong></h3><ul><li><p><strong>What is the issue?</strong><br />New domain onboarding, changes required in caching behaviour or traffic routing via Akamai</p></li><li><p><strong>How is it detected?</strong></p><ul><li><p>Developer requests</p></li></ul></li><li><p><strong>Symptoms</strong>:<br /><em>N/A</em></p></li></ul><p>&nbsp;</p><hr /><p>&nbsp;</p><h3><strong>&nbsp;3. Root Cause Identification</strong></h3><ul><li><p><strong>Possible root causes</strong>:<br /><em>N/A</em></p></li><li><p><strong>Diagnostic steps</strong>:</p><ul><li><p>Identify the property that is being used for the hostname that has pending change requests</p></li></ul></li></ul><p>&nbsp;</p><hr /><p>&nbsp;</p><h3><strong>&nbsp;4. Troubleshooting Steps</strong></h3><p><em>Provide a structured approach to investigating the issue.</em></p><ul><li><p>Identify the scope and impact, if the domain in request new or an existing domain</p></li><li><p>For new domains make sure we have approval from security team to make the domain public</p></li></ul><p>&nbsp;</p><hr /><p>&nbsp;</p><h3><strong>&nbsp;5. Resolution Steps</strong></h3><ul><li><p>Identify if the domain already exists or is a new domain</p></li><li><p>For new domain clone base_property_default_configs property in Akamai</p></li><li><p>Add hostname(select enhanced TLS certificate), update origin and cp-code in default rule.</p></li><li><p>Add any additional behaviour like caching rules, path based origin updates, websockets, chunked encoding, increased timeouts using new blank rules along with match criteria.</p></li><li><p>Save the config and Activate on staging and production network</p></li><li><p>Once activated go to the datastream page, edit DS2-TrafficPeak-PoC and add the new hostname and save with no additional changes to datastream.</p></li><li><p>Now we need to edit a new version of the property and enable datastream on the default rule, make sure sampling is enabled and set at 3%</p></li><li><p>Once updated activate the new version on both networks.</p></li><li><p>Go to security configurations, create a new version of Meesho WAF Security File </p></li><li><p>Click edit on match criteria and add the new hostname.</p></li><li><p>Now we need to add the domain to a security policy, select a security policy under the same config file and add the domain to the match criteria of the same.</p></li><li><p>Once added activate the config.</p></li></ul><p>&nbsp;</p><hr /><p>&nbsp;</p><h3><strong>&nbsp;6. Verification</strong></h3><ul><li><p>Confirm resolution via:</p><ul><li><p>Update DNS CNAME record&rsquo;s value to the edge hostname visible on the property page</p></li><li><p>Validate the config is routing to the service by running a healthcheck/api&rsquo;s</p></li></ul></li><li><p><strong>Expected outcome</strong>:<br />Traffic being routed via Akamai along with the expected caching and other behaviours </p></li></ul><p>&nbsp;</p><hr /><p>&nbsp;</p><h3><strong>&nbsp;7. References</strong></h3><ul><li><p>Base property - <a href=\"https://control.akamai.com/apps/property-manager/#/property-version/11525192/1/edit?gid=256284&amp;rule=default\">https://control.akamai.com/apps/property-manager/#/property-version/11525192/1/edit?gid=256284&amp;rule=default</a></p></li></ul>",
      "word_count": 322,
      "is_runbook": true
    },
    {
      "id": "4246307100",
      "title": "Runbook: K8S Cleanup",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4246307100/Runbook+K8S+Cleanup",
      "created": "2025-07-18T15:54:36.199Z",
      "author": "Shruti Koshta",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h3><strong>1. Metadata</strong></h3><ul><li><p><strong>Category</strong>: K8s Cleanup</p></li><li><p><strong>Subcategory</strong>: Unused Services/Deployable's Removal</p></li><li><p><strong>Severity</strong>: P0</p></li><li><p><strong>Frequency</strong>: Rare</p></li><li><p><strong>Author</strong>: Shruti Koshta</p></li><li><p><strong>Last Updated</strong>: 11/07/2025</p></li></ul><p /><hr /><p /><h3>2. Problem Statement</h3><h4>❓ What is the issue?</h4><p>The service or deployable is no longer required and needs to be decommissioned from the system. Leaving unused services active leads to unnecessary resource consumption, increased maintenance overhead, and potential security risks.</p><h4>⚠️ Symptoms</h4><ul><li><p>No active traffic to the service.</p></li><li><p>Helm chart and ArgoCD config exists but serves no purpose.</p></li><li><p>Service is not required anymore.</p></li></ul><p /><hr /><p /><h3><strong>3. Resolution Steps</strong></h3><h4>✅ Step 1: Create Jira Ticket</h4><ul><li><p>Raise a Jira ticket detailing the service/deployable you want to remove.</p></li><li><p>Mention the affected environment and justification for deletion.</p></li><li><p>Attach this runbook to the ticket.</p></li></ul><hr /><h4>✅ Step 2: Get Required Approvals in the  ticket</h4><ul><li><p><strong>Mandatory Approvals Required From:</strong></p><ul><li><p>Service Owners</p></li><li><p>Engineering Managers</p></li><li><p>DevOps Oncall (for deletion PRs)</p></li></ul></li></ul><hr /><h4>🛑 Step 3: Pre-check for Production Environment</h4><ul><li><p>If the target environment is <strong>Production</strong>:</p><ul><li><p>Ensure that <strong>no traffic is flowing</strong> to the service.</p></li><li><p>Verify with monitoring/observability dashboards or service owners.</p></li></ul></li></ul><hr /><h4>🔥 Step 4: Delete Application from ArgoCD</h4><ul><li><p>Go to ArgoCD UI.</p></li><li><p>Choose the app and click <strong>Delete</strong>.</p></li></ul><hr /><h4>🧹 Step 5: Delete Helm Chart Values</h4><p><strong>Repository:</strong> <code>https://github.com/Meesho/devops-helm-charts/tree/main/values_v2</code></p><ul><li><p>Navigate to <code>values_v2</code> directory.</p></li><li><p>Based on the environment, switch to the correct branch:</p><ul><li><p><code>prd</code> &rarr; <code>main</code></p></li><li><p><code>int</code> &rarr; <code>pre-prod</code></p></li><li><p><code>stg/dev</code> &rarr; <code>develop</code></p></li></ul></li><li><p>Delete the following files for the target service:</p><ul><li><p><code>values.yaml</code></p></li><li><p><code>values_properties.yaml</code></p></li></ul></li><li><p>Raise a <strong>PR</strong> for deletion.</p></li><li><p>Tag <strong>Service Owners</strong> and <strong>DevOps Oncall</strong> for review and approval.</p></li></ul><hr /><h4>🧽 Step 6: Delete ArgoCD Application Config</h4><p><strong>Repository:</strong> <code>https://github.com/Meesho/devops-argo-config/tree/main/applications_v2</code></p><ul><li><p>Navigate to <code>applications_v2</code> directory.</p></li><li><p>Based on the environment, switch to the correct branch:</p><ul><li><p><code>prd</code> &rarr; <code>main</code></p></li><li><p><code>int</code> &rarr; <code>pre-prod</code></p></li><li><p><code>stg/dev</code> &rarr; <code>develop</code></p></li></ul></li><li><p>Delete the respective <code>&lt;service-name&gt;.yaml</code> file from this directory.</p></li><li><p>Raise a <strong>PR</strong> for deletion.</p></li><li><p>Get approvals from <strong>Service Owners</strong> and <strong>DevOps Oncall</strong>.</p></li></ul><p /><hr /><p /><h3><strong>4. Verification</strong></h3><h4>🔍  Final Verification in ArgoCD</h4><ul><li><p>After PR merges and deletions are complete:</p><ul><li><p>Open ArgoCD dashboard.</p></li><li><p>Verify the service is <strong>completely removed</strong> from the environment.</p></li></ul></li></ul>",
      "word_count": 275,
      "is_runbook": true
    },
    {
      "id": "4252500413",
      "title": "Runbook: Custom jenkins job issues",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4252500413/Runbook+Custom+jenkins+job+issues",
      "created": "2025-07-15T12:27:19.243Z",
      "author": "Shivam Gupta1",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Category:</strong> Jenkins</p><p><strong>Subcategory:</strong> Heap Dump, Profiling, Node cordon,faulty nodes pipeline </p><p><strong>Severity:</strong> P2 (High Impact, Medium Urgency)</p><p><strong>Frequency:</strong> Rare</p><p><strong>Author:</strong> Shivam Gupta</p><p><strong>Last Updated:</strong> July 15, 2025</p><hr /><ol start=\"1\"><li><p><strong>Metadata</strong></p></li></ol><p style=\"margin-left: 30.0px;\">Category: Jenkins</p><p style=\"margin-left: 30.0px;\">Subcategory: Heap Dump , CleanupJob Etc. </p><p style=\"margin-left: 30.0px;\">Severity: P2</p><p style=\"margin-left: 30.0px;\">Frequency: Rare</p><p style=\"margin-left: 30.0px;\">Author: Shivam Gupta</p><p style=\"margin-left: 30.0px;\">Last Updated: 15/07/2025</p><ol start=\"2\"><li><p><strong>Problem Statement</strong></p></li></ol><p><strong>❓ What is the issue?</strong></p><p>Required to take profiling , heap dump , goroutine dumps , node sos reports tcp dump </p><p>Faulty Nodes: Seeing latency spikes  on all the pods of a specific gke node . </p><ol start=\"3\"><li><p><strong>Resolution Steps</strong></p></li></ol><p style=\"margin-left: 30.0px;\">  <strong>3.1 Faulty Node Pipeline: </strong></p><p style=\"margin-left: 60.0px;\">    In case of Faulty nodes use the below pipeline to take node dumps for gcp sos reports and creating a gcs support ticket for the same. Job will cordon the node and remove pods from faulty node as well </p><p style=\"margin-left: 60.0px;\">    <a href=\"https://jenkins-infra.admin.meeshogcp.in/job/faulty-node-end-to-end-immediate-mitigation/135/console\">https://jenkins-infra.admin.meeshogcp.in/job/faulty-node-end-to-end-immediate-mitigation/135/console</a></p><p style=\"margin-left: 60.0px;\">    Note: This job only works for java app as of now</p><p style=\"margin-left: 30.0px;\">  <strong>3.2 Use Below Jobs for profiling: </strong></p><p style=\"margin-left: 60.0px;\">    JAVA: </p><p style=\"margin-left: 60.0px;\">    <a href=\"https://jenkins-infra.admin.meeshogcp.in/job/heap-dump-job/\">https://jenkins-infra.admin.meeshogcp.in/job/heap-dump-job/</a></p><p style=\"margin-left: 60.0px;\">    GO: </p><p style=\"margin-left: 60.0px;\">        Prod: <a href=\"https://jenkins-infra.admin.meeshogcp.in/job/heap-dump-job-go/\">https://jenkins-infra.admin.meeshogcp.in/job/heap-dump-job-go/</a></p><p style=\"margin-left: 60.0px;\">        pre-prod: <a href=\"https://jenkins-infra.admin.meeshogcp.in/job/heap-dump-job-go-preprod/\">https://jenkins-infra.admin.meeshogcp.in/job/heap-dump-job-go-preprod/</a></p><p style=\"margin-left: 60.0px;\">    Python: </p><p style=\"margin-left: 60.0px;\">        we dont support this as of now .</p><p style=\"margin-left: 30.0px;\">  <strong>3.3  If go lang profile job is failing : </strong></p><p style=\"margin-left: 60.0px;\">If it is a go lang heap job that is failing : </p><p style=\"margin-left: 90.0px;\">Please check if params given are correct or not for the specific cluster</p><p style=\"margin-left: 90.0px;\">For go services to obtain profiling via job profiling needs to be enabled at app level as well </p><p style=\"margin-left: 90.0px;\">Make sure profiling port is correct .on which pprof has been enabled .</p><p style=\"margin-left: 60.0px;\"><br /><strong>If it is a dataengg job :</strong></p><p style=\"margin-left: 60.0px;\">Reach out to devops-oncall. Steps for devops debug: </p><p style=\"margin-left: 60.0px;\"> Check if jenkins job is hardcode inside jenkins pipeline  .</p><p style=\"margin-left: 90.0px;\">- check the nodeselector of the job and check the corresponding nodepool if autoscaling is off or on and nodes are able to spawn up on the nodepool</p><p style=\"margin-left: 60.0px;\" /><hr />",
      "word_count": 337,
      "is_runbook": true
    },
    {
      "id": "4252958846",
      "title": "Runbook Akamai IP/Path Whitelisting",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4252958846/Runbook+Akamai+IP+Path+Whitelisting",
      "created": "2025-07-15T14:23:51.863Z",
      "author": "Amul P Kesrani",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h3><strong>&nbsp;1. Metadata</strong></h3><ul><li><p><strong>Category</strong>: CDN</p></li><li><p><strong>Subcategory</strong>: Akamai</p></li><li><p><strong>Severity</strong>:  P1 </p></li><li><p><strong>Frequency</strong>: Occasional</p></li><li><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"6257b98b9770e600716a3d75\" ri:local-id=\"1f54601f-d912-489c-a345-4f9f2a0a95e4\" /></ac:link> </p></li><li><p><strong>Last Updated</strong>: (2025-07-15)</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;2. Problem Statement</strong></h3><ul><li><p><strong>What is the issue?</strong><br />Issue could be due to a new path, 3p or change in IP of the 3P. As Akamai only allows certain IP&rsquo;s for specific paths for gateway domains any change in either will result in the request getting blocked</p></li><li><p><strong>How is it detected?</strong></p><ul><li><p>We can go to edge diagnostics &gt; translate error string &gt; provide the error string received in the response of the failed request. This will help identify the rule that is leading to the request getting blocked.</p></li><li><p>User complaints or tickets</p></li></ul></li><li><p><strong>Symptoms</strong>:<br />403&rsquo;s received when calling the api</p></li></ul><hr /><p /><h3><strong>&nbsp;3. Root Cause Identification</strong></h3><ul><li><p><strong>Possible root causes</strong>:<br /><em>(List known or likely contributing factors)</em><br />Ip/Path not whitelisted</p></li><li><p><strong>Diagnostic steps</strong>:</p><ul><li><p>Review recent changes in IP&rsquo;s that are calling the domain </p></li></ul></li></ul><p /><hr /><p /><h3><strong>&nbsp;4. Troubleshooting Steps</strong></h3><p><em>Provide a structured approach to investigating the issue.</em></p><ul><li><p>Go to edge diagnostics on Akamai</p></li><li><p>Open Translate error string</p></li><li><p>Provide the error string received in the response of the failed request</p></li><li><p>Identify the rule that is leading to the block</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;5. Resolution Steps</strong></h3><p><em>Outline clear, step-by-step instructions for resolving the issue.</em></p><ul><li><p>Identify the config and policy that covers the domain in question</p></li><li><p>If the custom rules contains a client list and we need to whitelist an IP, click on shared resources &gt; client lists &gt; add the ip to the appropriate list and activate the list.</p></li><li><p>If a path rule is leading to block</p></li><li><p>Create a new version</p></li><li><p>Go to custom rules and click modify the custom rule</p></li><li><p>Add the path to list of allowed paths on the custom rule</p></li><li><p>Since the path is new there should be a rule that whitelists the path for a set of IP&rsquo;s</p></li><li><p>Identify the existing rule corresponding to the existing 3p or create a new rule to whitelist the path along with IP&rsquo;s for the new 3p onbaording</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;6. Verification</strong></h3><ul><li><p>Confirmation required from 3p that they are not receiving 403</p></li><li><p><strong>Expected outcome</strong>:<br />Non 403 responses being served to the clients who&rsquo;s ip were whitelisted</p></li></ul><p /><hr /><p /><h3><strong>&nbsp;7. References</strong></h3><ul><li><p><a href=\"https://control.akamai.com/apps/security-analytics?accountId=F-AC-2178592&amp;contractTypeId=1-2RBL&amp;configSelector=103945&amp;view=&amp;hash=ec0a5d3a-c788-4f34-bb4f-64bf6b4b18f1\">Monitoring dashboard</a></p></li></ul><p />",
      "word_count": 336,
      "is_runbook": true
    },
    {
      "id": "4254957569",
      "title": "Runbook: Pull Request Merge Requests",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4254957569/Runbook+Pull+Request+Merge+Requests",
      "created": "2025-07-16T06:00:34.186Z",
      "author": "Shivam Gupta1",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p /><hr /><ol start=\"1\"><li><p><strong>Metadata</strong></p></li></ol><p style=\"margin-left: 30.0px;\">Category: Github</p><p style=\"margin-left: 30.0px;\">Subcategory: PR Merge issue</p><p style=\"margin-left: 30.0px;\">Severity: P1</p><p style=\"margin-left: 30.0px;\">Frequency: Very Frequent </p><p style=\"margin-left: 30.0px;\">Author: Shivam Gupta</p><p style=\"margin-left: 30.0px;\">Last Updated: 15/07/2025</p><p /><ol start=\"2\"><li><p><strong>Problem Statement</strong></p></li></ol><p><strong>❓ What is the issue?</strong></p><p>A Lot of Pr requests when raised from develop to main and feature branch to develop gets stuck during merging process because of various jenkins  and ringmaster checks that we have implemented in merging process<br /><br />Common issues: <br />  <em>  Quality Gate fails.</em><br /><em>    Services are not onboarded properly in staging and pre-prod environment</em><br /><em>    Pods not coming healthy for int or stg environment</em><br /><em>    PR raised from not supported branches</em><br /><em>    Ringmaster Checks timed out</em><br /><em>    wrong configurations in config.yaml</em><br /></p><ol start=\"3\"><li><p><strong>Resolution Steps</strong><br /><br />Please check what checks are failing at github PR .</p></li></ol><p style=\"margin-left: 30.0px;\">PR raised from develop to main : <br /><br />Check if staging and int environments have been deployed with latest commit and pods are in healthy state<br /><br />Deployment takes place on all the deployables mentioned under prod environment for staging and pre-prod environments <br /><br />If a deployment exist in prod it is mandatory that it exist in staging and pre-prod environment as well.<br /><br />Check for  PR raised from feature branch to main branch  will always fail  as we dont support feature branch deployments . <br /><br />Only pr from develop &rarr; main , develop &rarr; master , develop &rarr; gcp_main are allowed for production deployments<br /><br />In Certain scenarios  : <br />Ringmaster checks timeout which causes pr to remain in stuck state as checks show in queue and pending state, in order to resolve this <br />    Mark the pr as draft and reopen it as ready for review .. Checks will retrigger again and it will start proceeding with deployment and then merge button will be enabled upon successful completion of checks </p><p style=\"margin-left: 30.0px;\"><br />If quality gate check is failing  ,this means new code has not passed the code quality coverage mark which need to be fixed as sonar scan is a mandatory check for all java applications .<br /></p><p /><p /><p style=\"margin-left: 30.0px;\" />",
      "word_count": 351,
      "is_runbook": true
    },
    {
      "id": "4254957583",
      "title": "Runbook: General/Informational",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4254957583/Runbook+General+Informational",
      "created": "2025-07-16T06:55:41.418Z",
      "author": "Mohammed Eliyas",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Runbook: General/Informational</strong></p><p><strong>Metadata</strong><br /><strong>Category:</strong> General<br /><strong>Subcategory:</strong> DevOps Support Queries<br /><strong>Severity:</strong> P1/P2/P3 (Case-specific)<br /><strong>Frequency:</strong> Frequent<br /><strong>Author:</strong> DevOps Team<br /><strong>Last Updated:</strong> 2025-07-16</p><ol start=\"1\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Users are unable to access configs on Vault or Ringmaster for any service, which blocks testing and deployment.</p><p><strong>How is it detected?</strong></p><ul><li><p>User complaints on Slack</p></li><li><p>Configs missing in Ringmaster UI</p></li><li><p>App logs showing &quot;config not found&quot;</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Configs not showing in Ringmaster UI</p></li><li><p>App failing to boot due to missing env/config</p></li></ul><ol start=\"2\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Pulse alerts are stuck in <code>Update_Awaiting_Approvals</code> and approvers cannot be selected.</p><p><strong>How is it detected?</strong></p><ul><li><p>Alert rule changes not reflected in Pulse UI</p></li><li><p>User reports inability to select approvers</p></li><li><p>Approval list dropdown is empty</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Approver dropdown is empty</p></li><li><p>Unable to apply changes to Pulse alerts</p></li></ul><ol start=\"3\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Need to create or update <code>contour-internal</code> rule for a service to enable internal routing.</p><p><strong>How is it detected?</strong></p><ul><li><p>User reports traffic not reaching internal services</p></li><li><p>Check confirms missing HTTPProxy rule in Contour</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Service not reachable via internal URLs</p></li><li><p>No traffic visible in Grafana dashboards</p></li></ul><ol start=\"4\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Client requests IP whitelisting for outbound service (e.g., voicebot calling external API).</p><p><strong>How is it detected?</strong></p><ul><li><p>Client raises request via ticket or Slack</p></li><li><p>Outbound requests fail with 403/timeout</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Requests from service blocked</p></li><li><p>DNS resolves, but no response from external system</p></li></ul><ol start=\"5\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Unable to upload files to GCS bucket due to permission errors (403 Forbidden).</p><p><strong>How is it detected?</strong></p><ul><li><p>Dev sees 403 error when using gsutil or application upload logic</p></li><li><p>IAM audit logs show denied access</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>File not appearing in bucket</p></li><li><p>Console or CLI shows access denied error</p></li></ul><ol start=\"6\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Need to enable or debug CORS policy for GCS static bucket access from browser.</p><p><strong>How is it detected?</strong></p><ul><li><p>Dev reports blocked frontend asset fetch in browser</p></li><li><p>Browser console shows CORS-related error</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>CORS error: &quot;No 'Access-Control-Allow-Origin' header&quot;</p></li><li><p>Static assets blocked in frontend</p></li></ul><ol start=\"7\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Need to update Pulse alert rules to include client-specific metrics (e.g., <code>client_id</code> in PromQL).</p><p><strong>How is it detected?</strong></p><ul><li><p>Dev request via Jira or Slack to isolate alerts</p></li><li><p>Current alerts firing for all clients</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Non-relevant alerts firing</p></li><li><p>Alert fatigue due to wide scope</p></li></ul><ol start=\"8\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Request to make bucket public and configure CDN via Google Cloud Console.</p><p><strong>How is it detected?</strong></p><ul><li><p>Dev asks for public URL support or static site hosting</p></li><li><p>403 on direct bucket object access</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Access denied for unauthenticated users</p></li><li><p>Website assets not loading properly</p></li></ul><ol start=\"9\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />User cannot SSH into a VM; access shows 'Permission Denied'.</p><p><strong>How is it detected?</strong></p><ul><li><p>Dev attempts SSH via console or gcloud CLI and fails</p></li><li><p>IAM roles or OS Login settings are missing</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Permission denied error on SSH</p></li><li><p>VM unreachable from Cloud Console</p></li></ul><p><strong>10. Problem Statement</strong><br /><strong>What is the issue?</strong><br />Need to update or upload config to Vault/Ringmaster for a service.</p><p><strong>How is it detected?</strong></p><ul><li><p>Dev request during rollout or testing phase</p></li><li><p>Config sync pipeline fails</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Ringmaster UI doesn't reflect updated config</p></li><li><p>App fails due to stale or missing config</p></li></ul><p><strong>11. Problem Statement</strong><br /><strong>What is the issue?</strong><br />DNS not resolving for internal service; gRPC or HTTP calls failing.</p><p><strong>How is it detected?</strong></p><ul><li><p>Service DNS name returns NXDOMAIN or no address</p></li><li><p>Internal service unreachable despite healthy status</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>gRPC status unavailable</p></li><li><p>DNS lookup fails in pod logs</p></li></ul><p><strong>12. Problem Statement</strong><br /><strong>What is the issue?</strong><br />Need help creating a private GCS bucket in production.</p><p><strong>How is it detected?</strong></p><ul><li><p>Dev creates request via form or ticket</p></li><li><p>Bucket doesn&rsquo;t exist or lacks correct IAM bindings</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>No bucket found in GCP console</p></li><li><p>Access denied when testing object upload</p></li></ul><p><strong>13. Problem Statement</strong><br /><strong>What is the issue?</strong><br />Superset dashboard is not opening or shows loading issues.</p><p><strong>How is it detected?</strong></p><ul><li><p>User opens dashboard but it spins indefinitely</p></li><li><p>Browser shows timeout or 504 error</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>504 Gateway Timeout</p></li><li><p>Loading spinner persists, no visual content</p></li></ul><p><strong>14. Problem Statement</strong><br /><strong>What is the issue?</strong><br />Dev needs help with traffic shifting for a Weighted DNS entry during rollout.</p><p><strong>How is it detected?</strong></p><ul><li><p>Traffic metrics show full load on old service version</p></li><li><p>DNS entry not reflecting recent update</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Load remains on old target</p></li><li><p>No change in traffic pattern despite update</p></li></ul><p><strong>15. Problem Statement</strong><br /><strong>What is the issue?</strong><br />Request to move folders or perform copy operations inside a GCS bucket.</p><p><strong>How is it detected?</strong></p><ul><li><p>Manual request from dev team</p></li><li><p>Lack of automation/scripts for internal transfers</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Dev cannot complete deployment or access required objects</p></li><li><p>Buckets show incorrect folder structure</p></li></ul>",
      "word_count": 662,
      "is_runbook": true
    },
    {
      "id": "4255645840",
      "title": "Runbook: Image Pull Issues",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4255645840/Runbook+Image+Pull+Issues",
      "created": "2025-07-16T12:08:15.490Z",
      "author": "Mohammed Eliyas",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p /><p><strong>Metadata</strong><br /><strong>Category:</strong> Deployment<br /><strong>Subcategory:</strong> Image Pull Issues<br /><strong>Severity:</strong> P1 (depending on the service)<br /><strong>Frequency:</strong> Frequent<br /><strong>Impacted Environments:</strong> Dev / Staging / Production<br /><strong>Primary Owner:</strong> DevOps / Platform Team<br /><strong>Last Updated:</strong> 2025-07-16<br /><strong>Related Services/Components:</strong> Kubernetes, ArgoCD, Container Registry, CI/CD Pipeline</p><ol start=\"1\"><li><p><strong>Problem Statement</strong></p></li></ol><p><br /><strong>What is the issue?</strong><br />Kubernetes pods fail to start because the container image cannot be pulled. This results in a <code>ImagePullBackOff</code> or <code>ErrImagePull</code> state, leading to service unavailability.</p><p><strong>How is it detected?</strong></p><ul><li><p>Pod status shows <code>ImagePullBackOff</code> or <code>ErrImagePull</code></p></li><li><p>ArgoCD sync fails or stuck due to missing images</p></li><li><p>CI/CD pipeline logs show image not found or registry access denied</p></li></ul><p><strong>Symptoms:</strong></p><ul><li><p>Pods never transition to Running state</p></li><li><p>Kubernetes events show image pull errors</p></li><li><p>Cluster logs show image authentication or not found errors</p></li></ul><ol start=\"2\"><li><p><strong>Root Cause Identification</strong></p></li></ol><p><br /><strong>Possible root causes:</strong></p><ul><li><p>Incorrect image name or tag in deployment manifest</p></li><li><p>Image does not exist in the registry (e.g., deleted or not pushed)</p></li><li><p>Missing or expired image pull secret for private registries</p></li><li><p>Network/DNS resolution issues when accessing external container registry</p></li><li><p>Container registry rate limiting or access control errors</p></li></ul><p><strong>Diagnostic steps:</strong></p><ul><li><p>Describe the pod and inspect <code>Events</code> section</p></li><li><p>Check container registry manually for image/tag</p></li><li><p>Inspect imagePullSecrets used in the namespace or deployment</p></li><li><p>Verify ArgoCD or CI pipeline logs for push/pull activity</p></li><li><p>Confirm access to registry from cluster nodes (e.g., using test pods)</p></li></ul><ol start=\"3\"><li><p><strong>Troubleshooting Steps</strong></p></li></ol><ul><li><p>Confirm correct image name and tag in Helm or manifest</p></li><li><p>Ensure the image is present and accessible in the registry</p></li><li><p>Verify imagePullSecrets exist and are correctly referenced</p></li><li><p>Check network path from node to registry (via DNS and egress controls)</p></li><li><p>Review ArgoCD sync status and event logs for clues</p></li></ul><ol start=\"4\"><li><p><strong>Resolution Steps</strong></p></li></ol><ul><li><p>Update deployment config with correct image/tag</p></li><li><p>Re-push missing image to registry if necessary</p></li><li><p>Recreate or fix image pull secrets</p></li><li><p>Apply registry access settings or firewall rules as needed</p></li><li><p>Roll back to previous working image if applicable</p></li></ul><ol start=\"5\"><li><p><strong>Verification</strong></p></li></ol><ul><li><p>Pod transitions to Running state</p></li><li><p>No image pull errors in <code>kubectl describe pod</code></p></li><li><p>ArgoCD shows sync completed</p></li><li><p>Alerts are cleared and traffic is served</p></li></ul><p><strong>Expected outcome:</strong><br />The pod successfully pulls the container image and transitions to the Running state without further image-related failures.</p><ol start=\"6\"><li><p><strong>References</strong></p></li></ol><p><br />[Kubernetes ImagePullBackOff Docs]<br />[ArgoCD Deployment Troubleshooting]<br />[Private Registry Authentication Guide]<br />[Recent CI/CD Failure Logs]</p>",
      "word_count": 329,
      "is_runbook": true
    },
    {
      "id": "4257710325",
      "title": "Runbook: DB Issues",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4257710325/Runbook+DB+Issues",
      "created": "2025-07-18T13:28:13.036Z",
      "author": "Pratul Bhargava",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h2>MongoDB</h2><h3><br />1. CPU, Memory, Disk Issues</h3><ul><li><p><strong>Category</strong>: Database</p></li><li><p><strong>Subcategory</strong>: Resource crunch</p></li><li><p><strong>Severity</strong>: P0 (depending on the service)</p></li><li><p><strong>Frequency</strong>: Frequent</p></li><li><p><strong>Impacted Environments</strong>: Dev / Staging / Production</p></li><li><p><strong>Primary Owner</strong>: DB Team</p></li><li><p><strong>Last Updated</strong>: 2025-07-03</p></li><li><p><strong>Related Services/Components</strong>: Compute Resources, VM, Disk</p></li></ul><hr /><h3>2. Problem Statement</h3><ul><li><p><strong>What is the issue?</strong><br />Due to high load or bad/unoptimised queries the load on CPU/memory/disk utilization could spike. This could result in service becoming latent, having timeouts or even in worst scenario, downtime.</p></li><li><p><strong>How is it detected?</strong></p><ul><li><p>Through alerts set from server side as well as client side</p></li><li><p>Monitored through PMM and grafana dashboards</p></li></ul></li><li><p><strong>Symptoms</strong>:</p><ul><li><p>Service having slow db calls or getting db timeouts.</p></li><li><p>Service is latent, unreachable or degraded</p></li></ul></li></ul><hr /><h3>3. Root Cause Identification</h3><ul><li><p><strong>Possible root causes</strong>:</p><ul><li><p>Bad deployment</p></li><li><p>Cron job overlapping with traffic</p></li><li><p>High load from organic traffic</p></li><li><p>Unoptimised queries</p></li></ul></li><li><p><strong>Diagnostic steps</strong>:</p><ul><li><p>Review recent deployments or any jobs running</p></li><li><p>Monitor which resource gets impacted</p></li><li><p>Check resource limits and requests</p></li></ul></li></ul><hr /><h3>4. Troubleshooting Steps</h3><hr /><h3>5. Resolution Steps</h3><hr /><h3>6. Verification</h3><hr /><h3>7. References</h3>",
      "word_count": 138,
      "is_runbook": true
    },
    {
      "id": "4261314590",
      "title": "GCP Cost and Billing Runbook",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4261314590/GCP+Cost+and+Billing+Runbook",
      "created": "2025-07-18T15:20:21.743Z",
      "author": "Yash Nandwana",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h2><strong>GCP Cost and Billing Runbook</strong></h2><hr /><h3><strong>1. Metadata</strong></h3><ul><li><p><strong>Category:</strong> GCP Cost Management</p></li><li><p><strong>Subcategory:</strong> Billing Insights, Cost Optimization</p></li><li><p><strong>Severity:</strong> P3</p></li><li><p><strong>Frequency:</strong> Rare</p></li><li><p><strong>Author:</strong> Yash Nandwana</p></li><li><p><strong>Last Updated:</strong> 2025-07-18</p></li></ul><hr /><h2><strong>2. Problem Statement</strong></h2><h3><strong>What is the issue?</strong></h3><p>Teams need clarity on GCP costs associated with their workloads and want to analyze cost trends by different business segments (BU, DOE, SKU, Project, etc.). There's also a need to understand how to troubleshoot anomalies or request billing support.</p><h3><strong>How is it detected?</strong></h3><ul><li><p>User complaints or cost anomalies reported on <strong>#devops-tech</strong> Slack channel.</p></li><li><p>Unexpected costs on specific SKUs or services noticed via billing dashboard.</p></li></ul><h3><strong>Symptoms:</strong></h3><ul><li><p>Unusual spike in billing.</p></li></ul><hr /><h2><strong>3. Root Cause Identification</strong></h2><h3><strong>Possible root causes:</strong></h3><ul><li><p>Missing or incorrect <strong>resource labels</strong> (e.g., no BU or team label).</p></li><li><p>Misconfigured workloads leading to inefficient usage (e.g., high disk IOPS, excessive egress).</p></li><li><p>Unexpected provisioning (e.g., accidentally running a VM or GPU instance).</p></li><li><p>Logging or monitoring set to high ingestion levels.</p></li></ul><h3><strong>Diagnostic steps:</strong></h3><ul><li><p>Navigate to <strong>GCP Billing &gt; Reports</strong>, filter by <strong>Project</strong>, <strong>Labels</strong>, <strong>Service</strong>, or <strong>SKU</strong>.</p></li><li><p>Use <strong>Looker Studio Dashboard</strong> to view cost breakdowns by BU, DOE, SKU, etc.<br /><a href=\"https://lookerstudio.google.com/reporting/dee80432-a410-4d16-b142-1d9c8931b582\">LookerStudio Dashboard</a></p></li><li><p>Check for unlabelled resources in <strong>GCP &gt; Asset Inventory &gt; Labels</strong>.</p></li><li><p>Review recent infra changes in the project repo or deployment history.</p></li></ul><hr /><h2><strong>4. Troubleshooting Steps</strong></h2><ol start=\"1\"><li><p><strong>Identify scope and impact:</strong></p><ul><li><p>Which project/team is affected?</p></li><li><p>Is the cost spike sudden or consistent?</p></li></ul></li><li><p><strong>Use GCP Billing UI:</strong></p><ul><li><p>Go to <strong>Billing &gt; Reports</strong>.</p></li><li><p>Filter by <strong>Label Key: </strong><code>team</code>, <code>bu</code>, <code>env</code>, etc.</p></li><li><p>Group by SKU, Service, and Time range.</p></li></ul></li><li><p><strong>Use Looker Studio Dashboard:</strong></p><ul><li><p>Open <a href=\"https://lookerstudio.google.com/reporting/dee80432-a410-4d16-b142-1d9c8931b582\">Dashboard</a>.</p></li><li><p>Switch to specified tabs to get specific insights based on: DOE, BU, Project, Service, and Timeframe.</p></li></ul></li></ol><hr /><h2><strong>5. Resolution Steps</strong></h2><ol start=\"1\"><li><p><strong>Optimize high-cost services:</strong></p><ul><li><p>E.g., reduce BigQuery scans, change VM type, adjust log retention.</p></li></ul></li><li><p><strong>Engage GCP support (if needed):</strong></p><ul><li><p>Contact <strong>DevOps team</strong> to file a billing support ticket on your behalf.</p></li></ul></li></ol><hr /><h2><strong>6. Verification</strong></h2><h3><strong>Confirm resolution via:</strong></h3><ul><li><p><strong>Billing &gt; Reports:</strong> Confirm cost drop or proper attribution via labels.</p></li><li><p><strong>Looker Studio Dashboard:</strong> Ensure correct data segmentation and visibility.</p></li><li><p><strong>Budget Alerts:</strong> No new alerts triggered after changes.</p></li><li><p><strong>Team feedback:</strong> No open queries from impacted teams.</p></li></ul><h3><strong>Expected outcome:</strong></h3><ul><li><p>Accurate cost attribution by BU/DOE/Project.</p></li><li><p>Improved visibility and accountability on GCP usage.</p></li><li><p>Cost anomalies resolved or reduced to acceptable thresholds.</p></li></ul><hr /><h2><strong>7. References</strong></h2><ul><li><p>📘 <a href=\"https://lookerstudio.google.com/reporting/dee80432-a410-4d16-b142-1d9c8931b582\">LookerStudio Dashboard for Cost Insights</a></p></li><li><p>📘 GCP Billing Docs</p></li><li><p>📞 Contact DevOps via Slack channel <code>#devops-tech</code></p></li></ul>",
      "word_count": 337,
      "is_runbook": true
    },
    {
      "id": "4262496260",
      "title": "Uncategorized Issues",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4262496260/Uncategorized+Issues",
      "created": "2025-07-21T16:38:39.553Z",
      "author": "Yash Nandwana",
      "labels": [],
      "space": "DEVOPS",
      "content": "<h2><strong>1. Metadata</strong></h2><ul><li><p><strong>Category:</strong> General Operations</p></li><li><p><strong>Subcategory:</strong> Miscellaneous / Uncategorized</p></li><li><p><strong>Severity:</strong> P2</p></li><li><p><strong>Frequency:</strong> Frequent</p></li><li><p><strong>Author:</strong> Yash Nandwana</p></li><li><p><strong>Last Updated:</strong> 2025-07-21</p></li></ul><hr /><h2><strong>2. Problem Statement</strong></h2><h3><strong>What is the issue?</strong></h3><p>Issues that don&rsquo;t fall under a single category, such as uploading assets to GCS, debugging rollout strategies, configuring slow start, or checking container crash patterns. These often arise during CI/CD activities or incident investigations by service owners.</p><h3><strong>How is it detected?</strong></h3><ul><li><p>User or team complaints via Slack/tickets</p></li><li><p>Metrics and logs showing unexpected behavior </p></li></ul><h3><strong>Symptoms:</strong></h3><ul><li><p>Container restart loops</p></li><li><p>Application downtime during rollout changes</p></li><li><p>Performance degradation during service scale-up</p></li><li><p>Unexpected deployment strategy behavior (e.g., surge issues)</p></li></ul><hr /><h2><strong>3. Root Cause Identification</strong></h2><h3><strong>Possible root causes:</strong></h3><ul><li><p>Improper or missing configuration (e.g., <code>maxSurge</code>, <code>slowStart</code>)</p></li><li><p>Transitioning from <strong>canary to rolling update</strong> without handling traffic shift</p></li><li><p>Stack trace not captured due to crashLoop</p></li></ul><h3><strong>Diagnostic steps:</strong></h3><ul><li><p>Check rollout settings:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"a4335f75-52a1-4345-8b50-48b898a98d45\"><ac:plain-text-body><![CDATA[bash]]></ac:plain-text-body></ac:structured-macro><p>CopyEdit</p><p><code>kubectl get deployment &lt;app&gt; -o yaml | grep maxSurge </code></p></li><li><p>Investigate container crashes:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"390fbcb0-7943-45a5-9165-3690d31d3465\"><ac:plain-text-body><![CDATA[bash]]></ac:plain-text-body></ac:structured-macro><p>CopyEdit</p><p><code>kubectl logs &lt;pod&gt; --previous </code></p></li><li><p>View stack traces:</p><ul><li><p>Use <code>kubectl exec</code> or crash dumps.</p></li><li><p>Enable debug log levels if available.</p></li></ul></li></ul><hr /><h2><strong>4. Troubleshooting Steps</strong></h2><ol start=\"1\"><li><p><strong>Debug container restarts:</strong></p><ul><li><p>Use:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"4679b75d-0951-4018-bce0-1d42c82b3d30\"><ac:plain-text-body><![CDATA[bash]]></ac:plain-text-body></ac:structured-macro><p>CopyEdit</p><p><code>kubectl describe pod &lt;pod-name&gt; kubectl logs &lt;pod-name&gt; --previous </code></p></li><li><p>Look for <code>OOMKilled</code>, <code>CrashLoopBackOff</code>, <code>fatal-error</code> etc.</p></li></ul></li><li><p><strong>Get stack traces:</strong></p><ul><li><p>If app logs don&rsquo;t show trace, enable debug mode or add panic/recover blocks (Go).</p></li><li><p>Use tooling like Sentry or Stackdriver Error Reporting for automatic collection.</p></li></ul></li><li><p><strong>Check deployment strategy:</strong></p><ul><li><p><code>devops-helm-charts</code></p><p>CopyEdit</p><p><code>maxSurge: 10-20%</code></p></li><li><p> <code>slowStart</code> is configured org-wide so <code>maxSurge</code> should be as low as possible.</p></li></ul></li><li><p><strong>Switching rollout strategies:</strong></p><ul><li><p>From <strong>canary &rarr; rolling update &rarr; canary</strong>:</p><ul><li><p>Expect <strong>short downtime</strong>.</p></li><li><p>Communicate downtime to stakeholders and pause monitoring alerts.</p></li></ul></li></ul></li></ol><hr /><h2><strong>5. Resolution Steps</strong></h2><ol start=\"1\"><li><p><strong>Fix restarts or crash loops:</strong></p><ul><li><p>Update env vars, image, or memory limits.</p></li><li><p>Re-deploy with corrected config:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c31316fc-256d-4d1b-97ad-e08433ab2700\"><ac:plain-text-body><![CDATA[bash]]></ac:plain-text-body></ac:structured-macro><p>CopyEdit</p><p><code>kubectl rollout restart deployment &lt;app&gt; </code></p></li></ul></li><li><p><strong>Reconfigure deployment strategy:</strong></p><ul><li><p>Edit Helm values or deployment YAML.</p></li><li><p>Use progressive rollout tools like Argo Rollouts or Flagger if needed.</p></li></ul></li><li><p><strong>Coordinate downtime when required:</strong></p><ul><li><p>Communicate to the team before moving from canary to rolling.</p></li><li><p>Use maintenance windows or traffic draining to fallbacks.</p></li></ul></li></ol><hr /><h2><strong>6. Verification</strong></h2><h3><strong>Confirm resolution via:</strong></h3><ul><li><p>No crash loops or restart counts increasing (<code>kubectl get pods</code>)</p></li><li><p>Application responds normally after rollout</p></li><li><p>Deployment shows complete and healthy replicas</p></li></ul><h3><strong>Expected outcome:</strong></h3><ul><li><p>Stable deployments with minimal downtime</p></li><li><p>Functional GCS integration</p></li><li><p>Clean logs and observability for further debugging</p></li><li><p>Smooth rollout transitions</p></li></ul><hr /><h2><strong>7. References</strong></h2><ul><li><p>📞 Contact DevOps via Slack channel <code>#devops-tech</code> or raise a ticket for critical changes</p></li></ul>",
      "word_count": 340,
      "is_runbook": true
    },
    {
      "id": "4287136112",
      "title": "User Access Request",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4287136112/User+Access+Request",
      "created": "2025-08-01T12:04:42.601Z",
      "author": "Ranjith Acharya",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Author:</strong> Ranjith Acharya<br /><strong>Published:</strong> Jul 14, 2025<br /><strong>Last Updated:</strong> Jul 14, 2025</p><hr /><h2>1. 📄 Metadata</h2><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"7125026d-8239-41ad-ab86-8efe14edc0e9\"><tbody><tr><th><p>Key</p></th><th><p>Value</p></th></tr><tr><td><p><strong>Category</strong></p></td><td><p>Access Management</p></td></tr><tr><td><p><strong>Subcategory</strong></p></td><td><p>Infra / Non-Infra Access Request</p></td></tr><tr><td><p><strong>Severity</strong></p></td><td><p>P3 (Can escalate to P1 for blockers)</p></td></tr><tr><td><p><strong>Frequency</strong></p></td><td><p>Daily</p></td></tr><tr><td><p><strong>Impacted Environments</strong></p></td><td><p>All (Primarily Prod/Staging)</p></td></tr><tr><td><p><strong>Primary Owner</strong></p></td><td><p>DevOps / IT Infra</p></td></tr><tr><td><p><strong>Related Services</strong></p></td><td><p>GCP, Vault, GitHub, ArgoCD, Jenkins, Superset, Grafana, BigQuery, Looker, Slack, Notion</p></td></tr></tbody></table><hr /><h2>2. ❓ Problem Statement</h2><h3>What is the issue?</h3><p>Users require access to infrastructure or non-infra tools to complete project tasks. Delay or lack of access affects delivery timelines, team collaboration, and compliance.</p><h3>How is it detected?</h3><ul><li><p>JIRA ticket or service request raised</p></li><li><p>Slack ping or email escalation from user/team</p></li><li><p>Blockers highlighted in sprint standup or retro</p></li></ul><h3>Symptoms</h3><ul><li><p>Permission denied / unauthorized errors</p></li><li><p>Cannot view dashboards, repositories, or tools</p></li><li><p>Missing group/team memberships</p></li><li><p>Access failures in pipelines or the CLI</p></li></ul><hr /><h2>3. 🕵️ Root Cause Identification</h2><h3>Possible Causes:</h3><ul><li><p>Access not yet provisioned</p></li><li><p>Incorrect IAM role or group mapping</p></li><li><p>Resource moved or deleted</p></li><li><p>Approval pending or not granted</p></li><li><p>Access expired or auto-revoked</p></li></ul><h3>Diagnostic Steps:</h3><ul><li><p>Review the original request (JIRA/email)</p></li><li><p>Check project/team mapping</p></li><li><p>Validate user ID/email</p></li><li><p>Verify current permissions via IAM/tool</p></li><li><p>Confirm compliance with the access policy</p></li></ul><hr /><h2>4. 🛠 Troubleshooting Steps</h2><ol start=\"1\"><li><p>Determine the type of access:</p><ul><li><p><strong>Infra</strong> (GCP, Vault, ArgoCD, etc.)</p></li><li><p><strong>Non-Infra</strong> (Slack, Notion, GitHub, etc.)</p></li></ul></li><li><p>Check group/team mappings for the relevant project</p></li><li><p>Verify if previous access was revoked/changed</p></li><li><p>Review tool-specific admin logs/status</p></li><li><p>Confirm user onboarding is complete</p></li></ol><hr /><h2>5. ✅ Resolution Steps</h2><h3>🔐 Infra Access</h3><p><strong>GCP</strong></p><ul><li><p>Use the console or run:<br /><code>gcloud projects add-iam-policy-binding</code></p></li><li><p>Assign roles: <code>viewer</code>, <code>editor</code>, <code>artifactregistry. reader</code>, etc.</p></li></ul><p><strong>Vault</strong></p><ul><li><p>Map user/group to policy</p></li><li><p>Reload policy if required</p></li></ul><p><strong>ArgoCD</strong></p><ul><li><p>Update <code>argocd-rbac-cm</code></p></li></ul><p><strong>Jenkins</strong></p><ul><li><p>Assign folder-level or global matrix permissions</p></li></ul><hr /><h3>🌐 Non-Infra Access</h3><p><strong>GitHub</strong></p><ul><li><p>Add to org/team via Settings &rarr; People</p></li><li><p>Assign appropriate role: <code>read</code>, <code>contributor</code>, <code>admin</code></p></li></ul><p><strong>Superset / Grafana / BigQuery / Looker</strong></p><ul><li><p>Add a user with the correct dashboard role</p></li></ul><hr /><h3>📌 Always</h3><ul><li><p>Follow the <strong>least privilege principle</strong></p></li><li><p>Document all changes in the JIRA ticket</p></li><li><p>Ensure proper approval exists before acting</p></li></ul><hr /><h2>6. 🔍 Verification</h2><ul><li><p>User can login and perform required actions</p></li><li><p>Role-based restrictions function correctly</p></li><li><p>Admin panel shows correct mapping</p></li><li><p>Ticket closed after user confirmation</p></li></ul><h3>✅ Expected Outcome:</h3><p>User has timely and compliant access to tools, scoped appropriately to their responsibilities.</p><hr /><h2>7. 📚 References</h2><ul><li><p>[Access Control Policy - Internal Security Docs]</p></li><li><p>[GCP IAM Permission Mapping]</p></li><li><p>[GitHub Team Management &amp; Roles]</p></li><li><p>[Vault Policy Mapping Reference]</p></li><li><p>[Onboarding &amp; Access Tracker - Confluence]</p></li><li><p>[Compliance Audit Guidelines - Quarterly Review]</p></li></ul>",
      "word_count": 329,
      "is_runbook": true
    },
    {
      "id": "4286841412",
      "title": "📘 Runbook: User Access Request (Infra / Non-Infra Access)",
      "type": "page",
      "status": "current",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/4286841412/Runbook+User+Access+Request+Infra+Non-Infra+Access",
      "created": "2025-08-01T12:10:28.401Z",
      "author": "Ranjith Acharya",
      "labels": [],
      "space": "DEVOPS",
      "content": "<p><strong>Author:</strong> Ranjith Acharya<br /><strong>Published:</strong> Jul 14, 2025<br /><strong>Last Updated:</strong> Jul 14, 2025</p><hr /><h2>1. 📄 Metadata</h2><table data-layout=\"default\" ac:local-id=\"397be131-8881-4767-8897-c2a39ac67658\"><tbody><tr><th><p>Key</p></th><th><p>Value</p></th></tr><tr><td><p><strong>Category</strong></p></td><td><p>Access Management</p></td></tr><tr><td><p><strong>Subcategory</strong></p></td><td><p>Infra / Non-Infra Access Request</p></td></tr><tr><td><p><strong>Severity</strong></p></td><td><p>P3 (Can escalate to P1 for blockers)</p></td></tr><tr><td><p><strong>Frequency</strong></p></td><td><p>Daily</p></td></tr><tr><td><p><strong>Impacted Environments</strong></p></td><td><p>All (Primarily Prod/Staging)</p></td></tr><tr><td><p><strong>Primary Owner</strong></p></td><td><p>DevOps / IT Infra</p></td></tr><tr><td><p><strong>Related Services</strong></p></td><td><p>GCP, Vault, GitHub, ArgoCD, Jenkins, Superset, Grafana, BigQuery, Looker, Slack, Notion</p></td></tr></tbody></table><hr /><h2>2. ❓ Problem Statement</h2><h3>What is the issue?</h3><p>Users require access to infrastructure or non-infra tools to complete project tasks. Delay or lack of access affects delivery timelines, team collaboration, and compliance.</p><h3>How is it detected?</h3><ul><li><p>JIRA ticket or service request raised</p></li><li><p>Slack ping or email escalation from user/team</p></li><li><p>Blockers highlighted in sprint standup or retro</p></li></ul><h3>Symptoms</h3><ul><li><p>Permission denied / unauthorized errors</p></li><li><p>Cannot view dashboards, repositories, or tools</p></li><li><p>Missing group/team memberships</p></li><li><p>Access failures in pipelines or CLI</p></li></ul><hr /><h2>3. 🕵️ Root Cause Identification</h2><h3>Possible Causes:</h3><ul><li><p>Access not yet provisioned</p></li><li><p>Incorrect IAM role or group mapping</p></li><li><p>Resource moved or deleted</p></li><li><p>Approval pending or not granted</p></li><li><p>Access expired or auto-revoked</p></li></ul><h3>Diagnostic Steps:</h3><ul><li><p>Review the original request (JIRA/email)</p></li><li><p>Check project/team mapping</p></li><li><p>Validate user ID/email</p></li><li><p>Verify current permissions via IAM/tool</p></li><li><p>Confirm compliance with access policy</p></li></ul><hr /><h2>4. 🛠 Troubleshooting Steps</h2><ol start=\"1\"><li><p>Determine type of access:</p><ul><li><p><strong>Infra</strong> (GCP, Vault, ArgoCD, etc.)</p></li><li><p><strong>Non-Infra</strong> (Slack, Notion, GitHub, etc.)</p></li></ul></li><li><p>Check group/team mappings for the relevant project</p></li><li><p>Verify if previous access was revoked/changed</p></li><li><p>Review tool-specific admin logs/status</p></li><li><p>Confirm user onboarding is complete</p></li></ol><hr /><h2>5. ✅ Resolution Steps</h2><h3>🔐 Infra Access</h3><p><strong>GCP</strong></p><ul><li><p>Use console or run:<br /><code>gcloud projects add-iam-policy-binding</code></p></li><li><p>Assign roles: <code>viewer</code>, <code>editor</code>, <code>artifactregistry.reader</code>, etc.</p></li></ul><p><strong>Vault</strong></p><ul><li><p>Map user/group to policy</p></li><li><p>Reload policy if required</p></li></ul><p><strong>ArgoCD</strong></p><ul><li><p>Update <code>argocd-rbac-cm</code></p></li></ul><p><strong>Jenkins</strong></p><ul><li><p>Assign folder-level or global matrix permissions</p></li></ul><hr /><h3>🌐 Non-Infra Access</h3><p><strong>GitHub</strong></p><ul><li><p>Add to org/team via Settings &rarr; People</p></li><li><p>Assign appropriate role: <code>read</code>, <code>contributor</code>, <code>admin</code></p></li></ul><p><strong>Superset / Grafana / BigQuery / Looker</strong></p><ul><li><p>Add user with correct dashboard role</p></li></ul><p><strong>Slack / Notion / Confluence</strong></p><ul><li><p>Invite to workspace</p></li><li><p>Assign to team channels or spaces</p></li></ul><hr /><h3>📌 Always</h3><ul><li><p>Follow <strong>least privilege principle</strong></p></li><li><p>Document all changes in the JIRA ticket</p></li><li><p>Ensure proper approval exists before acting</p></li></ul><hr /><h2>6. 🔍 Verification</h2><ul><li><p>User can login and perform required actions</p></li><li><p>Role-based restrictions function correctly</p></li><li><p>Admin panel shows correct mapping</p></li><li><p>Ticket closed after user confirmation</p></li></ul><h3>✅ Expected Outcome:</h3><p>User has timely and compliant access to tools, scoped appropriately to their responsibilities.</p><hr /><h2>7. 📚 References</h2><ul><li><p>[Access Control Policy - Internal Security Docs]</p></li><li><p>[GCP IAM Permission Mapping]</p></li><li><p>[GitHub Team Management &amp; Roles]</p></li><li><p>[Vault Policy Mapping Reference]</p></li><li><p>[Onboarding &amp; Access Tracker - Confluence]</p></li><li><p>[Compliance Audit Guidelines - Quarterly Review]</p></li></ul><hr /><p>Let me know if you'd like this exported to Confluence or Markdown.</p><h5>You said:</h5><p>Also write the Runbook like abov template for New Hire Onboarding. Need to provide the access to Github, Vault, ArgoCD, Pagerduty, Cronitor, Grafana, Databricks, Airflow</p><h6>ChatGPT said:</h6><p>Here is the <strong>Runbook: New Hire Onboarding Access Setup</strong>, formatted in the same style as your previous runbook:</p><hr /><h1>📘 <strong>Runbook: New Hire Onboarding (Access Provisioning)</strong></h1><p><strong>Author:</strong> Ranjith Acharya<br /><strong>Published:</strong> Jul 31, 2025<br /><strong>Last Updated:</strong> Jul 31, 2025</p><hr /><h2>1. 📄 Metadata</h2><table data-layout=\"default\" ac:local-id=\"da848d3a-ec87-4c41-b90b-97fe62f65ed6\"><tbody><tr><th><p>Key</p></th><th><p>Value</p></th></tr><tr><td><p><strong>Category</strong></p></td><td><p>Access Management</p></td></tr><tr><td><p><strong>Subcategory</strong></p></td><td><p>New Hire Onboarding</p></td></tr><tr><td><p><strong>Severity</strong></p></td><td><p>P2 (Can escalate to P1 for blockers in ramp-up)</p></td></tr><tr><td><p><strong>Frequency</strong></p></td><td><p>Weekly (based on new joiners)</p></td></tr><tr><td><p><strong>Impacted Environments</strong></p></td><td><p>All (Primarily Staging/Dev)</p></td></tr><tr><td><p><strong>Primary Owner</strong></p></td><td><p>DevOps / IT Infra</p></td></tr><tr><td><p><strong>Related Services</strong></p></td><td><p>GitHub, Vault, ArgoCD, PagerDuty, Cronitor, Grafana, Databricks, Airflow</p></td></tr></tbody></table><hr /><h2>2. ❓ Problem Statement</h2><h3>What is the issue?</h3><p>New joiners require timely access to internal tools and platforms for onboarding, training, and project contributions.</p><h3>How is it detected?</h3><ul><li><p>JIRA onboarding request ticket</p></li><li><p>Email from HR/Manager</p></li><li><p>Slack ping from hiring manager or new hire</p></li></ul><h3>Symptoms</h3><ul><li><p>User cannot access internal tools</p></li><li><p>Missing permissions in GitHub/Databricks</p></li><li><p>Failure to receive PagerDuty alerts</p></li><li><p>CLI/API authentication failures</p></li></ul><hr /><h2>3. 🕵️ Root Cause Identification</h2><h3>Possible Causes:</h3><ul><li><p>Onboarding request not completed</p></li><li><p>Manual provisioning skipped or delayed</p></li><li><p>User email/ID mismatch in tools</p></li><li><p>Group/team mapping missing</p></li><li><p>Required access tool not included in onboarding checklist</p></li></ul><h3>Diagnostic Steps:</h3><ul><li><p>Review JIRA onboarding ticket</p></li><li><p>Confirm user&rsquo;s email / GitHub handle / company ID</p></li><li><p>Check user&rsquo;s presence in tool-specific admin dashboards</p></li><li><p>Validate team/project mappings</p></li></ul><hr /><h2>4. 🛠 Troubleshooting Steps</h2><ol start=\"1\"><li><p>Confirm onboarding checklist from HR or manager</p></li><li><p>Validate required tools based on role (e.g., SDE vs Data Analyst)</p></li><li><p>Verify user exists in corporate directory (Google Workspace, Okta, etc.)</p></li><li><p>Check tool-specific admin dashboards or APIs for user presence</p></li><li><p>Ensure access requests were not blocked or denied due to policy</p></li></ol><hr /><h2>5. ✅ Resolution Steps</h2><h3>🔐 Access Provisioning by Tool</h3><hr /><p><strong>GitHub</strong></p><ul><li><p>Add to organization: <code>meesho</code></p></li><li><p>Assign to relevant teams (e.g., backend, devops)</p></li><li><p>Set permission: <code>read</code> / <code>write</code> / <code>admin</code> based on need</p></li></ul><hr /><p><strong>Vault</strong></p><ul><li><p>Map user to appropriate policy (<code>dev</code>, <code>prod-readonly</code>, etc.)</p></li><li><p>Run policy reload if needed</p></li><li><p>Test using <code>vault login</code> or token</p></li></ul><hr /><p><strong>ArgoCD</strong></p><ul><li><p>Edit <code>argocd-rbac-cm</code> ConfigMap</p></li><li><p>Assign RBAC roles like <code>role:readonly</code> or <code>role:admin</code></p></li><li><p>Apply changes and verify via UI</p></li></ul><hr /><p><strong>PagerDuty</strong></p><ul><li><p>Add user to team(s) and escalation policy</p></li><li><p>Ensure phone and email are updated</p></li><li><p>Assign on-call rotation if applicable</p></li></ul><hr /><p><strong>Cronitor</strong></p><ul><li><p>Invite user via email</p></li><li><p>Assign to team</p></li><li><p>Grant dashboard and alert permissions</p></li></ul><hr /><p><strong>Grafana</strong></p><ul><li><p>Add user via UI or provisioning script</p></li><li><p>Assign org role: <code>Viewer</code> or <code>Editor</code></p></li><li><p>Add to dashboards/folders</p></li></ul><hr /><p><strong>Databricks</strong></p><ul><li><p>Invite user via corporate email</p></li><li><p>Assign workspace role: <code>user</code> / <code>admin</code></p></li><li><p>Grant cluster/job access based on team/project</p></li></ul><hr /><p><strong>Airflow</strong></p><ul><li><p>Add user to Airflow UI or set in RBAC (if enabled)</p></li><li><p>Verify login access and DAG visibility</p></li><li><p>Add to viewer/editor/admin group based on use case</p></li></ul><hr /><h3>📌 Always</h3><ul><li><p>Follow the <strong>least privilege</strong> access principle</p></li><li><p>Document all provisioning steps in the JIRA ticket</p></li><li><p>Ensure all tool invites are accepted and confirmed</p></li></ul><hr /><h2>6. 🔍 Verification</h2><ul><li><p>The user can log in to each tool</p></li><li><p>Required roles/permissions are working as expected</p></li><li><p>Alerting tools (PagerDuty, Cronitor) are delivering events</p></li><li><p>Confirmed by user or manager in ticket/Slack</p></li><li><p>Ticket updated and closed post-confirmation</p></li></ul><h3>✅ Expected Outcome:</h3><p>The user is onboarded with timely access to all necessary tools, scoped to their responsibilities and role, and compliant with internal security policies.</p><hr /><h2>7. 📚 References</h2><ul><li><p>Jira Board Link &rarr; <a href=\"https://meesho.atlassian.net/jira/software/c/projects/ACL/boards/150\" data-card-appearance=\"inline\">https://meesho.atlassian.net/jira/software/c/projects/ACL/boards/150</a> </p></li><li><p><a href=\"https://argocd-dev.meeshogcp.in/applications\">https://argocd-dev.meeshogcp.in</a></p></li><li><p><a href=\"https://vault-prd.meeshogcp.in/\">https://vault-prd.meeshogcp.in</a></p></li><li><p><a href=\"https://meesho.pagerduty.com/users\">https://meesho.pagerduty.com</a></p></li><li><p><a href=\"https://cronitor.io/app/?env=production&amp;sort=-created&amp;time=7d\">https://cronitor.io/app/?env=production&amp;sort=-created&amp;time=7d</a></p></li><li><p><a href=\"http://ds-airflow.prd.meesho.int/home\">http://ds-airflow.prd.meesho.int/home</a></p></li></ul><p />",
      "word_count": 811,
      "is_runbook": true
    }
  ]
}