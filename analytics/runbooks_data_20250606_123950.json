{
  "metadata": {
    "fetched_at": "2025-06-06T12:39:50.329760",
    "confluence_base_url": "https://meesho.atlassian.net",
    "space_key": "DEVOPS",
    "total_pages": 50
  },
  "runbooks": [
    {
      "id": "4082204886",
      "title": "Runbook for Creating Roles for Admin Panel Access",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/4082204886",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p /><p><a href=\"https://docs.google.com/spreadsheets/d/1npL8ZDRI51Ta9foA2fnRAGSoVdr3v9goyZf_aLQExjY/edit?gid=1162573123#gid=1162573123\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1npL8ZDRI51Ta9foA2fnRAGSoVdr3v9goyZf_aLQExjY/edit?gid=1162573123#gid=1162573123</a> </p><p /><p /><h2>How RBAC works</h2><p /><ul><li><p>There are 3 entities</p><ul><li><p>User - admin panel users (employees with @meesho.com email)</p></li><li><p>Role - Role is a group of Access (Example :<code> Grocery Tech</code>,<code> Grocery Product </code>)</p></li><li><p>Access - Access is the at any operation level, which is required to do an operation in farmiso admin panel ( Example :<code> Superstore Banner Create Update </code>, <code>Superstore Discovery Category</code>  ) </p></li></ul></li><li><p>Concept of Role is has been created so that we can give multiple Access to a group of Users who  doing a similar set of operation on a day to day basis, this way we ensure, that User is not able to do any other operation on admin panel which they are not supposed to do.</p></li><li><p>Concept of Access is that , whenever we create a new dashboard on Admin Panel, the operation done in that dashboard is kept behind the access, so the User has to have this access to use this dashboard.</p></li><li><p> Some conditions</p><ul><li><p>One user can only be mapped to one Role , he will get all the Access that are present in that Role</p></li><li><p>If a user wants to get a new access, there are two ways</p><ul><li><p>Add the new Access in his current role (but this will give the access to the user who are currently have that role)</p></li><li><p>Create a new Role (will all the current Access + new Access ) in the current Role and, replace the Role of User to the new Role</p></li></ul></li><li><p>Any update in this system requires logging out and re-login in admin panel.</p></li><li><p /></li></ul></li><li><p>All Roles have been mentioned in this sheet , along with all Access they currently have</p><ul><li><p>Sheet : <a href=\"https://docs.google.com/spreadsheets/d/1npL8ZDRI51Ta9foA2fnRAGSoVdr3v9goyZf_aLQExjY/edit?gid=1162573123#gid=1162573123\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1npL8ZDRI51Ta9foA2fnRAGSoVdr3v9goyZf_aLQExjY/edit?gid=1162573123#gid=1162573123</a> </p></li><li><p>Anything getting Created and Updated here has be documented here</p></li></ul></li></ul><p /><p /><hr /><p /><h2>How to get Roles and Access created</h2><p>This process in done by Rajendra</p><ul><li><p>How to get a new Access created</p><ul><li><p>A mail needs to be sent to Raju in the following format</p></li><li><p>Subject : <code>RBAC Access Creation</code></p></li><li><p>List of Access</p><ul><li><p>description : What is the use case of the Access</p></li><li><p>Priority : </p><ul><li><p>P0 - means any PII data exposed</p></li><li><p>P1 - means any price fraud can happen</p></li><li><p>P2 - rest  <code>(for us all of them should be P2)</code></p></li></ul></li></ul></li></ul></li></ul><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"110d2b6e-a9c5-4dd4-9a33-2effd97a1616\"><tbody><tr><td><p>Action Name</p></td><td><p>Description</p></td><td><p>Priority</p></td></tr><tr><td><p>Superstore Update Consignments DP Hub</p></td><td><p>To update the DP Hub associated with the&nbsp;consignment in the last mile</p></td><td><p>P2</p><p /><p /></td></tr></tbody></table><p>also add this Access in the following Roles</p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"8adce0d5-07c4-4109-b9fd-6bb445c7aa8b\"><tbody><tr><td><p>Grocery Tech</p></td></tr><tr><td><p>Grocery Product</p></td></tr></tbody></table><p /><p /><ul><li><p>How to get a new Role created</p><ul><li><p>Subject :<code> RBAC Role creation</code></p></li><li><p>Description : use case of the role</p></li><li><p>List of all Access given to that Role</p></li></ul></li></ul><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"81df69ff-ed24-42d1-ab81-814176e92d8e\"><tbody><tr><td><p>Role Name</p></td><td><p>Description</p></td><td><p>Actions</p></td></tr><tr><td><p>Meesho Grocery Last Mile Executives</p></td><td><p>Role for all worker in the Last Mile Operations</p></td><td><ul><li><p>Superstore Image Upload<br />Make Calls View</p></li><li><p>Superstore&nbsp;Update Consignments DP Hub</p></li></ul></td></tr></tbody></table><p /><p><br /></p><hr /><h2>How to take Access</h2><p>Since, this process is now taken care by IT Team, you need to raise a ticket here : <a href=\"https://meesho.atlassian.net/servicedesk/customer/portal/23\" data-card-appearance=\"inline\">https://meesho.atlassian.net/servicedesk/customer/portal/23</a> </p><p>This process only includes assigning Role to User.</p><p /><ul><li><p>This process has some SLA</p></li><li><p>Use this sheet to find the suitable Role name for your use case : <a href=\"https://docs.google.com/spreadsheets/d/1npL8ZDRI51Ta9foA2fnRAGSoVdr3v9goyZf_aLQExjY/edit?gid=1162573123#gid=1162573123\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1npL8ZDRI51Ta9foA2fnRAGSoVdr3v9goyZf_aLQExjY/edit?gid=1162573123#gid=1162573123</a> </p></li><li><p /></li></ul><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"9e82fa70-0ae5-461e-b82d-39bd319fe767\"><ac:rich-text-body><p>NOTE : </p><ul><li><p>Please mention the name of <code>Role</code> (only one Role) which you want in the ticket</p></li><li><p>Don&rsquo;t ask the <code>Access</code> name, Access will not be given to Users directly.</p></li></ul></ac:rich-text-body></ac:structured-macro><p />",
        "representation": "storage",
        "word_count": 527
      },
      "version": {
        "number": 3,
        "when": "2025-04-20T08:00:28.339Z",
        "by": "Vikas Sao"
      },
      "labels": []
    },
    {
      "id": "4070473738",
      "title": "Agent Support Aggregator Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h2>Quick look up - </h2><ol start=\"1\"><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#Setting-Up-agent-support-aggregator\">Setting Up</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#Important-things-to-take-care-of-in-development%3A--\">Important things to take care of in development</a></p><ol start=\"1\"><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#Building-new-feature-%3A--\">Building a new feature</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#MQ-%3A--\">Adding MQ</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#Secrets-%3A--\">Adding new secrets</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#Crons-%3A--\">Crons</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#API-Migration-%3A--\">API Migration</a></p></li></ol></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#Templates-%3A--\">Templates</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/4070473738/Agent+Support+Aggregator+Runbook#Release-%3A--\">Release</a></p></li></ol><h2>Setting Up <code>agent-support-aggregator</code></h2><ol start=\"1\"><li><p><strong>Clone the Repository</strong><br />Clone the project from GitHub:<br /><code>https://github.com/Meesho/agent-support-aggregator</code></p></li><li><p><strong>Install Dependencies</strong></p><ul><li><p>Maven <code>3.5.4</code></p></li><li><p>Java <code>1.8</code> (JDK 8)</p></li></ul></li><li><p><strong>Configure Environment Variables</strong><br />Add the following exports to your <code>~/.zshrc</code> (or relevant shell config):</p><ul><li><p><code>JAVA_HOME</code></p></li><li><p><code>PATH</code></p></li><li><p><code>MAVEN_HOME</code></p></li></ul></li><li><p><strong>Project Configuration</strong></p><ul><li><p>Select the correct Java version (1.8) in your IDE's Project Structure. (settings &gt; project structure &gt; SDK)</p></li><li><p>Add a local file named <code>dev.yml</code>, and add the configs from stg-cac vault.<br />Stg Vault - <a href=\"https://vault-dev.meeshogcp.in/ui/vault/secrets/meesho/show/stg-cac/supl/xp/agent-support-aggregator\">Agent Support Aggregator STG Vault</a></p></li><li><p>Place the <code>settings.xml</code> file in <code>~/.m2</code> directory.<br />You can get this file from <ac:link><ri:page ri:content-title=\"Jfrog Migration - Developers Prerequisites\" ri:version-at-save=\"8\" /><ac:link-body>Meesho Wiki - Jfrog Migration: Developer Prerequisites</ac:link-body></ac:link>.</p></li><li><p>Run <code>mvn clean install</code> to install the dependencies for the project.</p></li><li><p>Install the <code>envFile</code> plugin in your IDE.</p></li><li><p>Open the edit configuration option</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"250\" ac:original-width=\"843\" ac:custom-width=\"true\" ac:width=\"712\"><ri:attachment ri:filename=\"Screenshot 2025-04-12 at 2.46.18 PM.png\" ri:version-at-save=\"1\" /></ac:image></li><li><p>Enable Env File option and include the <code>dev.yml</code> to import the secrets of the application.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"700\" ac:original-width=\"895\" ac:custom-width=\"true\" ac:width=\"712\"><ri:attachment ri:filename=\"Screenshot 2025-04-12 at 2.50.04 PM.png\" ri:version-at-save=\"1\" /></ac:image></li><li><p>Under the modify Option &gt; Add VM option.</p></li><li><p>Add the following in the profiles -</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"40636ac5-d9dd-4af3-a84f-db56af1a0951\"><ac:plain-text-body><![CDATA[-Dspring.profiles.active=dev,dyn-dev,agent-support-aggregator\n-Dspring.config.additional-location=configs/app/application-dev.yml,configs/app/application-dyn-dev.yml]]></ac:plain-text-body></ac:structured-macro><p>This would set your local profile to <code>dev</code>. Make sure to not use <code>int</code> and <code>prd</code> profiles.</p></li><li><p>Select the correct Java version (1.8) in here as well.</p></li><li><p>You can run the application now</p></li></ul></li><li><p>[Optional] Add <code>endpoints</code> option from the <code>More tool windows</code><br />This would help in navigating the API point to its correct function.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"602\" ac:original-width=\"436\" ac:custom-width=\"true\" ac:width=\"436\"><ri:attachment ri:filename=\"Screenshot 2025-04-12 at 3.09.47 PM.png\" ri:version-at-save=\"1\" /></ac:image></li></ol><h2>&lt;LLDs to be added&gt;</h2><p /><h2>Important things to take care of in development: -</h2><h3>Building new feature : -</h3><ol start=\"1\"><li><p><strong>Prepare a Tech Solutioning Document</strong>:<br />Draft a solution document outlining the approach, dependencies, and design decisions. This helps during implementation and facilitates smooth collaboration across teams.</p></li><li><p><strong>Define Test Coverage and Sign-offs</strong>:<br />Maintain a comprehensive test case sheet and get it reviewed and signed off by the Product team before going live. Ensure your changes are validated against existing workflows where applicable.</p></li><li><p><strong>Add Metrics and Structured Logging</strong>:<br />Incorporate meaningful metrics and structured logs to monitor the performance and behavior of your feature. This significantly aids in debugging and observability post-deployment.</p></li><li><p><strong>Use Feature Flags for Rollout Control</strong>:<br />Always wrap new functionality behind a feature flag. This ensures you can toggle the feature on/off safely and roll out in a controlled manner.</p></li><li><p>Make sure to update the MQ Version, after taking a pull from <code>develop</code> branch to confirm that your changes will create a new code image and get deployed, otherwise your changes won&rsquo;t reflect in the deployment.</p></li></ol><h3>MQ : -</h3><ol start=\"1\"><li><p>Make sure to add MQ in stg, and  prd both environment before deployment. For prd instance, first deploy your code and then add the consumer / producer in the MQ UI. If you add the MQ consumer / producer first, it will throw error in the deployment stating no listeners found for the consumer.<br />Prod - <a href=\"https://mqui.meeshogcp.in/home\">https://mqui.meeshogcp.in/home</a><br />Stg - <a href=\"https://mqui.stg.meeshogcp.in/home\">https://mqui.stg.meeshogcp.in/home</a></p></li></ol><h3>Secrets : -</h3><ol start=\"1\"><li><p>If you are incoorporating new secrets into the application, make sure to add them in the following vaults first before deploying -</p><ol start=\"1\"><li><p>stg - <a href=\"https://vault-dev.meeshogcp.in/ui/vault/secrets/meesho/show/stg-cac/supl/xp/agent-support-aggregator\">https://vault-dev.meeshogcp.in/ui/vault/secrets/meesho/show/stg-cac/supl/xp/agent-support-aggregator</a></p></li><li><p>stg-cron - <a href=\"https://vault-dev.meeshogcp.in/ui/vault/secrets/meesho/show/stg-cac/supl/xp/agent-support-aggregator-cron\">https://vault-dev.meeshogcp.in/ui/vault/secrets/meesho/show/stg-cac/supl/xp/agent-support-aggregator-cron</a></p></li><li><p>int - <a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/int-cac/supl/xp/agent-support-aggregator\">https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/int-cac/supl/xp/agent-support-aggregator</a></p></li><li><p>int-cron - <a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/int-cac/supl/xp/agent-support-aggregator-cron\">https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/int-cac/supl/xp/agent-support-aggregator-cron</a></p></li><li><p>prd - <a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd-cac/supl/xp/agent-support-aggregator\">https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd-cac/supl/xp/agent-support-aggregator</a></p></li><li><p>prd-cron - <a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd-cac/supl/xp/agent-support-aggregator-cron\">https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd-cac/supl/xp/agent-support-aggregator-cron</a></p></li></ol></li></ol><h3>Crons : - </h3><ol start=\"1\"><li><p>Always <strong>test cron jobs locally and in the pre prod environment</strong> before promoting them to production.</p></li><li><p>Plan the release to happen <strong>close to the cron's scheduled run time</strong>, so it's easier to monitor the execution and roll back if necessary.</p></li><li><p>After deployment, make sure to <strong>upsert the details of the added or updated cron</strong> in the tracking sheet. (<a href=\"https://docs.google.com/spreadsheets/d/1CVfjox-A9K5isJo5Ky_sAFvwdlQgkT3FVeMWgGYRkqE/edit?usp=sharing\">Agent Support Crons Data</a>)</p></li></ol><h3>API Migration : -</h3><ul><li><p>Deprecating an Existing API</p><ul><li><p>Annotate with <code>@Deprecated</code> and <strong>Swagger notes</strong></p></li><li><p>Communicate with all known consumers (internal/external)</p></li><li><p>Add warnings/logs when API is hit (<code>log.warn(&quot;Deprecated API called&quot;)</code>) if needed</p></li><li><p>Support <strong>grace period</strong> (usually 1-2 sprints or more)</p></li><li><p>Clean up deprecated API after the grace period</p></li></ul></li><li><p>Adding a New API (as replacement)</p><ul><li><p>Point to the new API in deprecation notice</p></li><li><p>Ensure <strong>backward compatibility</strong> in behavior/data model</p></li><li><p>Version appropriately ( <code>/new-feature</code>)</p></li><li><p>Add new API to Swagger/OpenAPI docs</p></li></ul></li></ul><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"935dfff1-f8ca-4a50-b2d3-4559afd3ca1c\"><ac:rich-text-body><p>Avoid <code>/v2</code> or anything as such, as version controlling is done on controller level.</p></ac:rich-text-body></ac:structured-macro><hr /><h2>Templates : -</h2><ul><li><p>Solutioning Doc - &lt;To be added&gt;</p></li><li><p>Testing - <a href=\"https://docs.google.com/spreadsheets/d/1Z7X12rJIb1vKpXTrKCMjQtLckIi56ZSPDaxoevi3JgY/edit?usp=sharing\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1Z7X12rJIb1vKpXTrKCMjQtLckIi56ZSPDaxoevi3JgY/edit?usp=sharing</a> </p></li></ul><hr /><h2>Release : -</h2><p>Please <strong>post your release notification</strong> in the <code>#agent-support-release</code> channel to ensure visibility and avoid deployment conflicts. Use the following format:</p><ol start=\"1\"><li><p>For feature releases : </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"45e01be9-0eb1-47df-bbef-e043469cd280\"><ac:plain-text-body><![CDATA[==============================================\n                Release Notification\n==============================================\nSubject: <Brief description of the deployment>\nImpacting Services: <e.g., agent-support-aggregator>\nPR: <Link to the Dev PR>\nRelease Owner: <@your-handle>\nCode Reviewer: <@reviewer's-handle>\ncc: @sx-xp-oncall]]></ac:plain-text-body></ac:structured-macro></li><li><p>For hotfixes : <br />Have a branch name : <code>hotfix_&lt;reason for hotfix&gt;</code></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"e5f4017c-47ae-426e-b0d2-86be5cea36d8\"><ac:plain-text-body><![CDATA[==============================================\n               Hotfix Notification\n==============================================\nSubject: <Brief description of the hotfix>\nImpacting Services: <e.g., agent-support-aggregator>\nPR: <Link to the Dev PR>\nRelease Owner: <@your-handle>\nCode Reviewer: <@reviewer's-handle>\ncc: @sx-xp-oncall]]></ac:plain-text-body></ac:structured-macro></li></ol><ac:adf-extension><ac:adf-node type=\"panel\"><ac:adf-attribute key=\"panel-type\">note</ac:adf-attribute><ac:adf-content><p>Always raise a <strong>Main &rarr; Dev PR</strong> to keep both branches in sync with the latest changes, when doing a <strong>hotfix</strong></p></ac:adf-content></ac:adf-node><ac:adf-fallback><div class=\"panel conf-macro output-block\" style=\"background-color: rgb(234,230,255);border-color: rgb(153,141,217);border-width: 1.0px;\"><div class=\"panelContent\" style=\"background-color: rgb(234,230,255);\">\n<p>Always raise a <strong>Main &rarr; Dev PR</strong> to keep both branches in sync with the latest changes, when doing a <strong>hotfix</strong></p>\n</div></div></ac:adf-fallback></ac:adf-extension><ol start=\"3\"><li><p>Once the deployment is complete and monitoring is done, <strong>update the same thread</strong> to confirm completion.</p></li></ol><p />",
        "representation": "storage",
        "word_count": 865
      },
      "version": {
        "number": 2,
        "when": "2025-04-24T10:35:23.042Z",
        "by": "Vedant Jain"
      },
      "labels": []
    },
    {
      "id": "3966927119",
      "title": "Runbook: Onboarding a new Logistics partner in Meesho",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>This runbook provides the template to be followed while onboarding a new 3p.</p><h3><strong>Table of Contents</strong></h3><ul><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#API-Contracts\">API Contracts</a></p><ul><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#PII-data-that-is-shared-with-3p%3A\">PII data shared</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Scans-Mapping%3A\">Scans mapping</a></p></li></ul></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Serviceability-Changes\">Serviceability Changes</a></p><ul><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#File-Uploads-%3A-%5BinlineCard%5D\">File uploads</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Code-Changes%5BhardBreak%5D\">Code changes</a></p></li></ul></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Manifest-Changes\">Manifestation Changes</a></p><ul><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Creation-flow%3A\">Creation</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#%5BhardBreak%5DCancellation-flow%3A\">Cancellation</a></p></li></ul></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Tracking-Changes\">Tracking Changes</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Label-Changes\">Label Changes</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Supplier-panel-changes\">Supplier panel changes</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Integration-Testing\">Integration Testing</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Post-release\">Post release</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#Issues%2Fbugs-arised-earlier-while-onboarding-3p%3A-Bluedart\">Issues/bugs arised earlier while onboarding 3p: Bluedart</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3966927119/Runbook+Onboarding+a+new+Logistics+partner+in+Meesho#FAQs\">FAQs</a></p></li></ul><p /><h2>API Contracts</h2><p>The API contract with the 3PL must be finalised before development begins.</p><p>It should specify which field will contain sender and customer details, along with their respective data types. Additionally, all shipment and order-related information, such as dimensions, price, quantity, weight, and unique order ID, should be clearly defined. The contract must also outline the field types and any validations applied at the 3PL's end to ensure data integrity and compatibility.</p><h3>PII data that is shared with 3p:</h3><p>Customer details:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c292f1c0-bb56-404c-b5b7-3a1d60ba9032\"><ac:plain-text-body><![CDATA[Customer name\nCustomer address, pincode\nCustomer mobile number]]></ac:plain-text-body></ac:structured-macro><p>Seller details:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"56b8c357-dbd7-4006-811a-b6941a348411\"><ac:plain-text-body><![CDATA[Seller name\nSeller address, pincode\nSeller mobile number]]></ac:plain-text-body></ac:structured-macro><p>Any data which is to be shared with 3PL needs to be reviewed and approved by the <strong>Security team.</strong></p><h3>Scans Mapping:</h3><ul><li><p>3PL should share scan mappings that need to be validated by the Product POC.</p><p>Validation Pointers: Ensure mappings are accurate and comprehensive.</p></li><li><p>P0 scans for us:</p><ul><li><p>Picked Up</p></li><li><p>Reached At Destination</p></li><li><p>Out For Delivery</p></li><li><p>Delivered</p></li><li><p>RTO init</p></li><li><p>RTO delivered</p></li><li><p>Cancelled</p></li></ul></li></ul><p>(make sure that mappings to these scans are correct and the order timeline is maintained)</p><p /><h2>Serviceability Changes</h2><h3>File Uploads : <a href=\"https://docs.google.com/spreadsheets/d/1XUlzb3hwlNPG-7rIWZQArGekAIc7cIwZlYdr0oMKi_I/edit?usp=sharing\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1XUlzb3hwlNPG-7rIWZQArGekAIc7cIwZlYdr0oMKi_I/edit?usp=sharing</a>  </h3><p>Mongo Collections used:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"85bd349f-8ad9-4401-8bc4-000286741543\"><ac:plain-text-body><![CDATA[available_pin_codes\ncity_city_zone_map\nzone_info\nsuppliers\npin_attributes]]></ac:plain-text-body></ac:structured-macro><p>Upload the above mentioned files in sheet in the correct format with headers.</p><ul><li><p>Serviceability data pincode wise - available pincodes collection</p></li><li><p>Rate card zone wise- zone info collection</p></li><li><p>TAT map and time map - city city zone map collection</p></li><li><p>Non preferred carriers for a supplier - supplier collection</p></li><li><p>Daily capacity - pin attributes collection</p></li></ul><p>Additionally we can enable and disable serviceability on <strong>lane level</strong> (city_city_zone level) and <strong>account_type </strong>level through carrierAccountTypeServiceabilityMap present in city_city_zone_map <br />Note- for domestic_bulk, value in domestic_surface is used</p><p /><h3>Code Changes :</h3><ol start=\"1\"><li><p><code>v2/lane/fallback/data/update/available-pincode</code></p></li></ol><p style=\"margin-left: 30.0px;\">add switch case for new carrier while saving details in DB</p><p /><ol start=\"2\"><li><p><code>v2/lane/fallback/data/update/zone-info</code></p></li></ol><p style=\"margin-left: 30.0px;\">add carrier name in <code>CARRIERS</code> list, and account type onboarded in <code>CARRIER_ACCOUNT_TYPE_LIST_ACTIVE</code></p><p /><ol start=\"3\"><li><p><code>/v2/lane/fallback/data/update/time-map</code></p></li></ol><ul><li><p>addition of new columns in file: raise a ticket to change file schema in admin file upload via CIS</p></li></ul><p style=\"margin-left: 30.0px;\">sample ticket: <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"7be8da8e-8aca-456b-b966-2841f0502635\"><ac:parameter ac:name=\"key\">CMR-5931</ac:parameter><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro>  </p><ul><li><p>make changes to set  <code>carrierExpectedTimeMap</code> for new 3p in above api flow</p></li></ul><p /><ol start=\"4\"><li><p> <code>v2/lane/fallback/data/update/tat-map</code></p></li></ol><ul><li><p>addition of new columns in file: raise a ticket to change file schema in admin file upload via CIS</p></li><li><p>similar to time map, make changes to set <code>carrierTurnAroundTimeMap</code> for new 3p in api flow</p></li></ul><p /><ol start=\"5\"><li><p><code>v1/lane/fallback/data/update/dailyCap</code></p></li></ol><p style=\"margin-left: 30.0px;\">add carrier in <code>INDIA_FORWARD_CARRIERS</code> and <code>CARRIERS</code> list</p><ol start=\"6\"><li><p><code>v2/lane/fallback/data/update/supplier</code></p></li></ol><p /><h2>Manifest Changes</h2><p><strong>(Ensure all changes are implemented behind a feature flag)</strong></p><ul><li><p>Add the new carrier to the Carriers list/enum across all relevant services, including Manifest, PDF, OSM, Cart Service, and Route Aggregator.</p></li></ul><h3>Creation flow:</h3><p>Api: <code>/v1/forward-shipments/create</code></p><p>Update <code>ManifestThreadPoolExecutor</code> to include a switch case for onboarding the new carrier. Manifestation should be attempted whenever this carrier appears in the serviceability list.</p><p>Implement a carrier-specific handler to validate requests, make HTTP calls to the 3PL, and process responses.</p><h3><br />Cancellation flow:</h3><p>Api: <code>v1/shipments/cancel</code></p><p>Add a switch case in <code>CancelThreadPoolExecutorV2</code> to handle shipment cancellations for the new carrier.</p><p>Use the carrier handler for making cancellation requests to the 3PL.</p><p>Check with the 3PL to confirm the last possible time for canceling a shipment. Usually, for other logistics partners, cancellations are allowed until the shipment is picked up from the seller.</p><p /><h2>Tracking Changes</h2><ul><li><p>Get list of 3p dev and prod ips and whitelisting them</p></li><li><p>Share authorization key with 3p for the webhook of scans push</p></li><li><p>add rate limiting for these </p></li></ul><h3>Code Changes</h3><ul><li><p>Manifest Service:</p><ul><li><p>Generic tracking consumer changes: add tracking handler for new partner</p></li><li><p>Add list of scans mapping in code</p></li></ul></li><li><p>Gateway Service:</p><ul><li><p>webhook api exposed to 3p:  <code>v1/generic/shipment/update</code><br />Add the new carrier in CarrierEnum and add one more switch case for this carrier in above api flow.</p></li><li><p>Sample PR: <a href=\"https://github.com/Meesho/gateway-service/pull/597\" data-card-appearance=\"inline\">https://github.com/Meesho/gateway-service/pull/597</a>  </p></li></ul></li></ul><p /><h2>Label Changes</h2><p>Extra label code provided in response of 3p to be used to printed on label</p><p>design of label- ask designer</p><p>which response field would give label code- ask from 3p?</p><p /><h2>Supplier panel changes</h2><p>Add filter for 3p in supplier panel -  sos team</p><p /><h2>Integration Testing</h2><p>UAT environment/someone from 3pl team required for integration testing.</p><p>Test cases:</p><ul><li><p>Waybill creation and check if all fields are set accordingly at 3p end</p></li><li><p>Cancellation flow</p></li><li><p>Prepaid and COD </p></li><li><p>Creation on different account types (surface, express, bulk)</p></li><li><p>Complete journey scans for different cases (for fwd and rto both)</p></li></ul><p>(refer to prod logs while creating sample request in staging)</p><p /><p>Do E2E testing as well from SOS side and check pdf creation flow.</p><p>Sample testing doc:  <a href=\"https://docs.google.com/spreadsheets/d/1nKnbREV1uKzHe-W4lonFYRdWIsxU1bozmjOYnq4cp5A/edit?usp=sharing\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1nKnbREV1uKzHe-W4lonFYRdWIsxU1bozmjOYnq4cp5A/edit?usp=sharing</a>  </p><p /><h2>Post release</h2><ul><li><p>Upload real time_map, tat_map and rate_card to production</p></li><li><p>Enable serviceability for pincodes according to the scale up plan and make sure to upload account type serviceability as well</p></li><li><p>Add all 3p metrics in <a href=\"https://grafana-prd.meeshogcp.in/d/8t8js7iro41l/manifestation-eks?orgId=1&amp;from=now-24h&amp;to=now\">Manifestation-EKS</a> dashboard</p></li><li><p>Set alerts on <a href=\"https://pulse.meeshogcp.in/alert/alerts\">pulse</a>  </p></li></ul><p>Monitoring:</p><ul><li><p>Check for errors in dashboard and logs</p></li><li><p>Check the <code>manifestation_errors</code> metabase table.</p></li><li><p>Verify that manifested orders have the correct pricing sent to the 3P and that labels are being printed properly.</p></li><li><p>Ensure that orders are being manifested for the intended cases (i.e., those relevant to the partner onboarding) and check for any potential leakages.</p></li></ul><p /><p>Sample release plan: <a href=\"https://docs.google.com/document/d/1XwH02h_mzPQM523OYOA2CNCYhxMiRGjQoxW2q1vMIjw/edit?usp=sharing\" data-card-appearance=\"inline\">https://docs.google.com/document/d/1XwH02h_mzPQM523OYOA2CNCYhxMiRGjQoxW2q1vMIjw/edit?usp=sharing</a>  </p><p /><h2>Issues/bugs arised earlier while onboarding 3p: Bluedart</h2><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"08cb6e9d-9c68-442c-bbc0-5df5f64c3161\"><tbody><tr><th><p><strong>Issue</strong></p></th><th><p><strong>Fix</strong></p></th></tr><tr><td><p>Bluedart has a 20-character limit for order numbers, but we were sending 25-character order numbers, causing issues.</p></td><td><p>Trimmed the order number to a maximum of 20 characters before sending.</p></td></tr><tr><td><p>The declared value for prepaid orders was being sent as 0, leading to manifestation failures.</p></td><td><p>Now sending the actual order value (Meesho price) in this field.</p></td></tr><tr><td><p>Weight precision fix<br />Direct conversion from float to double caused precision errors. For example, a 10g order (0.01 kg) was being sent as 0.0099 kg, but BD only considers values up to two decimal places, meaning they wouldn&rsquo;t manifest orders under 10g.</p></td><td><p>Raised a fix using BigDecimal</p></td></tr><tr><td><p>Regarding Scans :</p><ol start=\"1\"><li><p>The delivered scan was sometimes received after the signature image scan but timestamp earlier than the signature image scan, causing the delivered status to be missed.</p></li><li><p>Incorrect mapping for RAD scans shared by BD.</p></li><li><p>Incorrect mapping for Shipment inscan to Manifested (map it to in_transit)</p></li><li><p>Missing intermediate scans:<br />This was happening as we were receiving cancellation scan from BD and cancellation is a terminal state at our end so we were not capturing any further scans for that shipment. They were sending cancellation scan because of the same vendor code being used. Since at their end, each vendor is assigned one token per day, using the same vendor code for all vendors causes an issue&mdash;when Vendor A cancels a pickup, it also closes pickups for Vendor B, as the system treats them as the same. </p></li></ol></td><td><ol start=\"1\"><li><p> BD will stop sending Signature image scan </p></li><li><p>For now use Shipment Inscan scan which has &quot;ReachedDestinationLocation = Y&quot; </p></li><li><p>Corrected it in code</p></li><li><p>Raised a fix sending unique seller identifier in this field.</p></li></ol></td></tr><tr><td><p>We were getting domestic_express of BD as serviceable carrier x account combo for orders where carrierWeight does not lie in any of the weight buckets . This is because <code>domestic_express</code> is the default account if there no bucket found .</p></td><td><p>As a temp fix, we routed domestic_express orders to domestic_surface using zookeeper config<br /><br />Permanent fix: <br />Instead of using <code>domestic_express</code> as the default account, we will now assign the last available weight bucket if no specific bucket is found.</p></td></tr></tbody></table><p /><h2>FAQs</h2><ol start=\"1\"><li><p>Why do we check on carrierWeight and not weight ? And if we have to check on carrierWeight, why do we send weight field to some carriers ?</p></li></ol><ul><li><p>We use <code>carrierWeight</code> to determine the rate card, as carriers charge based on the weight they receive. Some carriers require the <code>weight</code> field due to API specifications, but they ultimately reweigh the order at their end.</p></li></ul><ol start=\"2\"><li><p>Also , we observed that weight parameter is passed by SOS post dividing by 2 . Reasoning?</p></li></ol><ul><li><p>This is an arbitrary choice to avoid over-reporting weight. It could have been divided by 3 or 4 as well.  However, carriers do not rely on this field entirely; they have their own mechanisms and reweigh the order at their end.</p></li></ul>",
        "representation": "storage",
        "word_count": 1308
      },
      "version": {
        "number": 43,
        "when": "2025-03-31T09:35:27.551Z",
        "by": "Komal Garg"
      },
      "labels": []
    },
    {
      "id": "3948544320",
      "title": "Jenkins pipeline failures runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3948544320",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<h2>Dockerfile failed to build</h2><h3><code>ERROR: failed to solve: process &quot;/bin/sh -c go build -tags musl --ldflags \\&quot;-extldflags -static\\&quot; -v -o /app/server cmd/price-aggregator-go/main.go&quot; did not complete successfully: exit code: 1</code></h3><h2>Skip analysis</h2><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"fedbbe7a-c892-458e-ba66-1443254bf71d\"><ac:plain-text-body><![CDATA[ERROR: Error Updating Helm Repo hudson.AbortException: Error: Update the skipAnalysis parameter\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] echo\n15:01:50  ERROR: hudson.AbortException: Error: Update the skipAnalysis parameter]]></ac:plain-text-body></ac:structured-macro><p /><h2>Validation failed for schema</h2><p><code>Validation failed for configs/Cart-Service/application-dyn-dev.yml against schema application-schema.yml</code></p><p /><h2>Error in cloning the repo</h2><p>If the error occurs in a <code>Multibranch Pipeline</code> while cloning the repo. Please follow below steps:</p><ul><li><p>Ensure that the credential being used here are of Github App. Refer Screenshot below</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"923\" ac:original-width=\"1776\" ac:width=\"1700\"><ri:attachment ri:filename=\"image-20230921-082942.png\" ri:version-at-save=\"1\" /></ac:image><p /></li></ul><p>If that is correct, reindex the repo by clicking on <code>Scan Repository now</code> at Job page</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"524\" ac:original-width=\"1777\" ac:width=\"1700\"><ri:attachment ri:filename=\"image-20230921-083118.png\" ri:version-at-save=\"1\" /></ac:image><p /><h2>Github 422 error (commit not found for SHA)</h2><p>This happens when your PR is behind the branch you are trying to merge to, and it cannot be fast-forwarded. Jenkins tries to rebase, creating a new commit which is not pushed to github.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"437ae1f1-d4bb-4c04-a33e-e0f309b893a1\"><ac:plain-text-body><![CDATA[Error during Quality Gate check: \nRequestError [HttpError]: No commit found for SHA: 7c50b5e31d844899e4af7353df5ce1fa0c71b2a1 \n  at /usr/src/app/node_modules/@octokit/request/dist-node/index.js\n  at processTicksAndRejections (node:internal/process/task_queues:96:5) { \n    status: 422,\n    ...\n    }]]></ac:plain-text-body></ac:structured-macro><h3>Solution:</h3><p>Pull the base branch, merge manually, push to github, and retrigger jenkins.</p><h2>Github 4xx error (not 422) while PR creation and merge</h2><p>This is the most reoccurring known error in pipeline. Looks like following</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"444\" ac:original-width=\"1433\" ac:width=\"1700\"><ri:attachment ri:filename=\"image-20230921-083354.png\" ri:version-at-save=\"1\" /></ac:image><h3>Solution</h3><ul><li><p>Get the branch name from the console output for which PR creation/merge is failed</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"894\" ac:original-width=\"1444\" ac:width=\"1700\"><ri:attachment ri:filename=\"image-20230921-083528.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>Go to devops-helm-chart repo in github</p></li><li><p>Click on branches</p></li><li><p>Select the branch copied from console output</p></li><li><p>Now there are two way to resolve the error</p></li><li><p>Delete the branch and any PR associate with it, and run the CICD job again</p></li><li><p>Create the PR if not created, merge and sync manually from ArgoCD</p></li><li><p>OR, merge the PR if created, and sync manually form ArgoCD</p></li><li><p>Both of the above options will resolve the above error</p></li></ul><p /><p />",
        "representation": "storage",
        "word_count": 326
      },
      "version": {
        "number": 3,
        "when": "2025-02-21T08:00:45.324Z",
        "by": "Arnav Andrew Jose"
      },
      "labels": []
    },
    {
      "id": "3835527291",
      "title": "Affiliate On-Call Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3835527291",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "",
        "representation": "storage",
        "word_count": 0
      },
      "version": {
        "number": 1,
        "when": "2025-01-09T06:59:39.379Z",
        "by": "Amit Kumar"
      },
      "labels": []
    },
    {
      "id": "3815374910",
      "title": "Toolchain RunBook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DV1/pages/3815374910",
      "space": {
        "key": "DV1",
        "name": "Dev-Productivity"
      },
      "content": {
        "body": "<h3>Purpose</h3><p>The purpose of this page is to document all the toolchain related issues, their solve and backlog tasks. </p><hr /><h3>Issues</h3><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"ea233fe9-705a-454b-9b59-95411f1458d4\"><colgroup><col style=\"width: 435.0px;\" /><col style=\"width: 208.0px;\" /><col style=\"width: 412.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"#e6fcff\"><p><strong>Issue</strong></p></th><th data-highlight-colour=\"#e6fcff\"><p><strong>Impact</strong></p></th><th data-highlight-colour=\"#e6fcff\"><p><strong>Resolution Steps</strong></p></th></tr><tr><td><p>Port mismatch in registry and service deployment files</p></td><td><p>Service doesn&rsquo;t come up. </p></td><td><p>Ask the developer to correct the port.</p></td></tr><tr><td><p>Service account not getting attached to the application.</p></td><td><p /></td><td><p>Owners to check this if policy binding got exceeded. </p></td></tr><tr><td><p>Incorrect configs picked up for services. </p></td><td><p>connectivity issues with diff services. </p></td><td><p>check the config.yaml file</p></td></tr><tr><td><p>OOM killed </p></td><td><p>not enough resource is assigned for the pod to come up.</p></td><td><p>increase the resources.</p></td></tr></tbody></table><hr /><h3>Backlog Tasks</h3><ol start=\"1\"><li><p>Clean up for pods and service accounts after env getting destroyed/terminated on toolchain.</p><ol start=\"1\"><li><p>Currently we have set a time of 24hours, if service does not come up in 24hours, we stop showing them the deployment links, but the pod created for it does not get destroyed, occupying the resources, hence we need to do 2 changes here: </p><ol start=\"1\"><li><p>&nbsp;Reduce the time from 24 hours to 20mins. </p></li><li><p>Destroy the pods for those services after 20mins. </p></li></ol></li><li><p>clean up the policy binding of the toolchain service account for these services. </p></li></ol></li><li><p>Generalise the profiles in toolchain, currently we have set &lsquo;stg&rsquo; for CAC changes, instead we can store all these profiles in an enum or class and can use that for calling, this will help make changes easily in future if any profile modification is required. </p></li><li><p><strong>Fetch Default Config</strong> functionality breaking.  </p></li></ol><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"884\" ac:original-width=\"1317\" ac:custom-width=\"true\" ac:width=\"478\"><ri:attachment ri:filename=\"Screenshot 2024-12-22 at 3.40.27 PM.png\" ri:version-at-save=\"1\" /></ac:image><p /><ol start=\"4\"><li><p>while adding a stateful service, we need to wait for a few seconds before clicking on add stateful button, this needs to be fixed. more context: <a href=\"https://meesho.slack.com/archives/C07AB4GL1L0/p1733910075135409\" data-card-appearance=\"inline\">https://meesho.slack.com/archives/C07AB4GL1L0/p1733910075135409</a> </p></li><li><p>Currently the default profile when we create a env plan and launch it is &lsquo;dev&rsquo; , hence users cannot launch their envs with &lsquo;stg&rsquo; profile, for that they have to use the direct launch feature. this needs to be fixed. </p></li><li><p>Remove the policy binding of service account when envs are destroyed, need to automate this process, currently we have to do manual deletion here. </p></li><li><p>Make parity in the registry names present in toolchain. <br /><br /><br /><br />Toolchain support channel: <u>#toolchain-support</u></p></li></ol><p />",
        "representation": "storage",
        "word_count": 364
      },
      "version": {
        "number": 4,
        "when": "2025-01-13T18:46:11.349Z",
        "by": "Shristi Sethiya"
      },
      "labels": []
    },
    {
      "id": "3623419914",
      "title": "Sale Runbook for loyalty",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3623419914",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>Grafana sale dashboard - <a href=\"https://grafana-prd.meeshogcp.in/d/KOpRhUi4z/loyalty?orgId=1\">https://grafana-prd.meeshogcp.in/d/KOpRhUi4z/loyalty?orgId=1</a></p><p>Cloudsql - <a href=\"https://console.cloud.google.com/sql/instances/msql-dmnd-grwth-loyalty-earn-engine-prd-ase1/overview?project=meesho-demand-prd-0622\">https://console.cloud.google.com/sql/instances/msql-dmnd-grwth-loyalty-earn-engine-prd-ase1/overview?project=meesho-demand-prd-0622</a></p><p>Redis - <a href=\"https://app.redislabs.com/#/databases/12503719/subscription/2220916/view-bdb/configuration\" data-card-appearance=\"inline\">https://app.redislabs.com/#/databases/12503719/subscription/2220916/view-bdb/configuration</a> </p><p>ArgoCD - <a href=\"https://argocd-demand-prd.meeshogcp.in/applications/argocd-demand-prd/prd-loyalty-earn-engine-eks?view=tree&amp;resource=\">https://argocd-demand-prd.meeshogcp.in/applications/argocd-demand-prd/prd-loyalty-earn-engine-eks?view=tree&amp;resource=</a></p><p>Major serving api is /productConfig , we are calling wallet, AB and redis in parallel for this api. In case wallet or AB goes down, we have circuit breakers and default response that we are sending. </p><p>Major failure point can be redis spikes although for redis also command timeout is set at 70ms, but we don&rsquo;t have circuit breaker currently in redis. We have set our tomcat threads based on 100ms p99.9, so even if redis fails we should be okay. Major caller for loyalty is Price aggregator which already has CB for loyalty call, so just check if the CB is open in PA if this happens. These graphs are present in loyalty sale dashboard(link above). </p><p>We currently don&rsquo;t have retries or metrics for wallet call in SubOrderNonReturnableConsumer, so keep a check on slack alerts #loyalty-service-alerts slack channel for wallet failures. </p><p /><p /><p /><p> </p>",
        "representation": "storage",
        "word_count": 163
      },
      "version": {
        "number": 1,
        "when": "2024-09-26T10:55:23.236Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3620438051",
      "title": "Search Sale Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3620438051",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<ol start=\"1\"><li><p>Rx-fa Circuit opening and closing - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Updating CB (Circuit Breaker) State from Zookeeper Configurations\" ri:version-at-save=\"1\" /><ac:link-body>Updating CB (Circuit Breaker) State from Zookeeper Configurations</ac:link-body></ac:link>  </p></li><li><p>Search orchestrator guide - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"search orch sale run book [MBS 24]\" ri:version-at-save=\"4\" /><ac:link-body>search orch sale run book [MBS 24]</ac:link-body></ac:link> </p></li><li><p>Traffic migration to different dag on sale day - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Traffic migration to new dag\" ri:version-at-save=\"2\" /><ac:link-body>Traffic migration to new dag</ac:link-body></ac:link> </p></li><li><p>Stop, reduce concurrency and start ES indexers - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Search Indexer MBS Sale 2024 Run Book\" ri:version-at-save=\"1\" /><ac:link-body>Search Indexer MBS Sale 2024 Run Book</ac:link-body></ac:link> </p></li><li><p>Traffic migration to different ES cluster - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Traffic diversion for ES based CGs\" ri:version-at-save=\"1\" /><ac:link-body>Traffic diversion for ES based CGs</ac:link-body></ac:link> </p></li><li><p>List of monitoring dashboards - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Search Monitoring Dashboards\" ri:version-at-save=\"7\" /><ac:link-body>Search Monitoring Dashboards</ac:link-body></ac:link> </p></li></ol><p />",
        "representation": "storage",
        "word_count": 132
      },
      "version": {
        "number": 1,
        "when": "2024-09-24T17:27:56.183Z",
        "by": "Shubham Gupta"
      },
      "labels": []
    },
    {
      "id": "3617128527",
      "title": "Product Amplifyr Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3617128527",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"6c1b3264-2953-4357-88bf-b95a7720af20\" ac:macro-id=\"ee700116-39fc-47c1-8701-63a151797ecb\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">6</ac:parameter><ac:parameter ac:name=\"outline\">false</ac:parameter><ac:parameter ac:name=\"style\">default</ac:parameter><ac:parameter ac:name=\"type\">list</ac:parameter><ac:parameter ac:name=\"printable\">true</ac:parameter></ac:structured-macro><h2>Overview</h2><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"2f888c81-f484-4fe3-9c1b-4b9d06562767\"><colgroup><col style=\"width: 171.0px;\" /><col style=\"width: 587.0px;\" /></colgroup><tbody><tr><td><p>Repository</p></td><td><p><a href=\"https://github.com/Meesho/product-amplifyr\" data-card-appearance=\"inline\">https://github.com/Meesho/product-amplifyr</a> </p></td></tr><tr><td><p>Clusters</p></td><td><ul><li><p>product-amplifyr</p></li><li><p>product-amplifyr-pdp</p></li><li><p>product-amplifyr-secondary</p></li><li><p>product-amplifyr-cart</p></li><li><p>product-amplifyr-internal</p></li></ul></td></tr><tr><td><p>Jenkins</p></td><td><p><a href=\"https://jenkins-prd.meeshogcp.in/job/product-amplifyr-cicd/\">https://jenkins-prd.meeshogcp.in/job/product-amplifyr-cicd/</a></p></td></tr><tr><td><p>Vault</p></td><td><p>&nbsp;<a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/list/prd/dmnd/splat/\">https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/list/prd/dmnd/splat/</a></p></td></tr><tr><td><p>ArgoCD</p></td><td><p>&nbsp;<a href=\"https://argocd-demand-prd.meeshogcp.in/applications/argocd-demand-prd/prd-product-amplifyr?view=tree&amp;resource=\">https://argocd-demand-prd.meeshogcp.in/applications/argocd-demand-prd/prd-product-amplifyr?view=tree&amp;resource=</a></p></td></tr></tbody></table><h2>Deployments</h2><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"8eda7144-39fc-4038-9e3b-32717f306225\"><tbody><tr><th><p><strong>Cluster</strong></p></th><th><p><strong>Usecases</strong></p></th></tr><tr><td><p>product-amplifyr</p></td><td><p>Serves information for all the feed pages</p></td></tr><tr><td><p>product-amplifyr-pdp</p></td><td><p>Serves information on pdp page</p></td></tr><tr><td><p>product-amplifyr-cart</p></td><td><p>Serves information on cart page</p></td></tr><tr><td><p>product-amplifyr-secondary</p></td><td><p>Serves information for widget usecase primarily FIF</p></td></tr><tr><td><p>product-amplifyr-internal</p></td><td><p>Serves information for internal usecases and also serve as a fallback for other clusters</p></td></tr></tbody></table><h2>Redis Infrastructure</h2><p><strong><u>Cluster Details</u></strong></p><table data-table-width=\"760\" data-layout=\"center\" ac:local-id=\"4cbf27a5-3f39-48d5-8d82-4f2f7d189bc3\"><colgroup><col style=\"width: 257.0px;\" /><col style=\"width: 129.0px;\" /><col style=\"width: 122.0px;\" /><col style=\"width: 125.0px;\" /><col style=\"width: 127.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Redis Name</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Role</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Ops/sec</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Memory</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>HA Enabled</strong></p></th></tr><tr><td><p><code>redis-dmnd-splat-amplifyr-1-prd-ase1-new</code> (<a href=\"https://grafana-prd.meeshogcp.in/d/neSOwn7Vk/redis-bdb-dashboard-dbe-team?orgId=1&amp;var-cluster=c29181.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12104609&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation&amp;from=now-24h&amp;to=now\">link</a>)</p></td><td><p>Main Redis 1</p></td><td><p>1000k</p></td><td><p>200 GB</p></td><td><p>Yes</p></td></tr><tr><td><p><code>redis-dmnd-splat-prd-amplifyr-1-prd-ase1</code> (<a href=\"https://grafana-prd.meeshogcp.in/d/neSOwn7Vk/redis-bdb-dashboard-dbe-team?orgId=1&amp;var-cluster=c28855.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12070292&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation&amp;from=now-24h&amp;to=now\">link</a>)</p></td><td><p>Main Redis 2</p></td><td><p>500k</p></td><td><p>100 GB</p></td><td><p>Yes</p></td></tr><tr><td><p><code>redis-dmnd-splat-amplifyr-2-new-prd-ase1</code> (<a href=\"https://grafana-prd.meeshogcp.in/d/neSOwn7Vk/redis-bdb-dashboard-dbe-team?orgId=1&amp;var-cluster=c33639.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12535067&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation&amp;from=now-24h&amp;to=now\">link</a>)</p></td><td><p>Fallback Redis</p></td><td><p>1000k</p></td><td><p>200 GB</p></td><td><p>Yes</p></td></tr></tbody></table><p><strong><u>Configuration</u></strong></p><table data-table-width=\"760\" data-layout=\"center\" ac:local-id=\"575370bc-d712-4a74-b6ce-e433a1e1b81d\"><tbody><tr><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Product Amplifyr Cluster</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Primary Redis (% traffic)</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Secondary Redis (% traffic)</strong></p></th></tr><tr><td><p>product-amplifyr</p></td><td><p>Main Redis 1 (50%)</p></td><td><p>Fallback Redis (50%)</p></td></tr><tr><td><p>product-amplifyr-pdp</p></td><td><p>Main Redis 1 (50%)</p></td><td><p>Fallback Redis (50%)</p></td></tr><tr><td><p>product-amplifyr-cart</p></td><td><p>None</p></td><td><p>None</p></td></tr><tr><td><p>product-amplifyr-internal</p></td><td><p>Main Redis 2</p></td><td><p>Fallback Redis</p></td></tr><tr><td><p>product-amplifyr-secondary</p></td><td><p>Main Redis 2</p></td><td><p>Fallback Redis</p></td></tr></tbody></table><h2>Disaster Recovery Levers</h2><p>DR controls that help maintaining resiliency of the systems.</p><h3>Cache </h3><ol start=\"1\"><li><p>Zk Node (<a href=\"https://zk-web.meeshogcp.in/node?path=%2Fconfig%2Fproduct-amplifyr%2Fconstants%2FpercentageMigrationToSecondaryRedis\">percentageMigrationToSecondaryRedis</a>) - Currently using 2 redis cache with 50% traffic on each. In case of failure of one cache, we can divert complete traffic on the other redis by changing value of the zk Node</p></li><li><p>GetEx - If the call to downstream is failing resulting in cb open for either (taxonomy and taxonomy-fallback) or( supplier-store and supplier-store-fallback) we will make getex calls to redis which will increase timeouts for existing redis keys while making get call<br />Zk Node for disabling getex - <a href=\"https://zk-web.meeshogcp.in/node?path=%2Fconfig%2Fproduct-amplifyr%2Fconstants%2FincreaseRedisTTLWhenCBOpen\"><u>link</u></a><br />Zk Node for setting ttl - <a href=\"https://zk-web.meeshogcp.in/node?path=%2Fconfig%2Fproduct-amplifyr%2Fconstants%2FredisTTLInSecondsWhenCBOpen\">link</a><br />Grafana dashboard for getEx metrics- <a href=\"https://grafana-prd.meeshogcp.in/d/2I_RCyM7ksdsd/product-amplifyr-all-metrics?orgId=1&amp;from=1727181933487&amp;to=1727189760419&amp;tab=query&amp;var-service=product-amplifyr&amp;var-environment=prod&amp;var-pod=All&amp;var-node=gke-k8s-demand-prd-a-np-dmnd-xdmnd-su-cd6eefd7-qzzg&amp;viewPanel=58\"><u>grafana</u></a><br /></p></li></ol><h3>CB Overrides</h3><p>FORCE_OPEN_CB -&gt; open CB for given service </p><p>FORCE_CLOSE_CB -&gt; disable CB for given service </p><p>RESET_CB -&gt; Returns the circuit breaker to its original closed state, losing statistics.</p><p>Zk Node - <a href=\"https://zk-web.meeshogcp.in/node?path=%2Fconfig%2Fproduct-amplifyr%2Fcb-overrides%2FserviceCBStateOverrideMap%2F\">link</a></p><p>Set service name ( <code>taxonomy-service</code>, <code>taxonomy-fallback-service</code> etc.. you can find service name in application-r4j.yml) as node name and values as FORCE_OPEN_CB, FORCE_CLOSE_CB, RESET_CB to open, close and reset cb</p><p>To force close cb you have to first set value as RESET_CB and then set value as  FORCE_CLOSE_CB.</p><p>To check cb logic, can refer to the following file in r4j <code>CBAdminControls</code></p><h3>Taxonomy DR</h3><p>We have implemented partial feed feature where the taxonomy call for some of the products in the feed fails, we will return response for rest of the products rather than failing the entire request.</p><p>Zk Node - <a href=\"https://zk-web.meeshogcp.in/node?path=%2Fconfig%2Fproduct-amplifyr%2Fconstants%2FpartialResponseCompletenessThreshold\">link</a>  We can set minimum threshold percentage of product calls that should be successful above which partial feed call will succeed</p><h3>Supplier-store DR</h3><p>We are sending default values  for the following feeds (catalog_recommendation,text_search,for_you,catalog_listing_page,collection, pdp) in case of failures from supplier-store <br />Zk Node: <a href=\"https://zk-web.meeshogcp.in/node?path=%2Fconfig%2Fproduct-amplifyr%2Fconstants%2FsupplierStoreNonMandatoryContexts\">link</a></p><p>Default values: <a href=\"https://zk-web.meeshogcp.in/node?path=%2Fconfig%2Fproduct-amplifyr%2Fconstants%2FsupplierStoreDefaultValues\">link</a></p><h3>Price Aggregator DR</h3><p>Inventory calls on price aggregator is made non mandatory and the zk flag on storefront is set to true.<br />When this flag is true we get inventory details from taxonomy and save it in redis as a failsafe. In case the variation response from price-aggregator comes as null we fetch that data from redis</p><p>Zk Node of the flag : <a href=\"https://zk-web.meeshogcp.in/node?path=%2Fconfig%2Fstorefront%2Fconstants%2Fcommon%2FfetchVariationNameFromTaxonomy\">link</a></p><p>Grafana dashboard to represent the time we override variation data from taxonomy<br /><a href=\"https://grafana-prd.meeshogcp.in/d/2I_RCyM7ksdsd/product-amplifyr-all-metrics?orgId=1&amp;from=now-7d&amp;to=now&amp;tab=query&amp;var-service=product-amplifyr-pdp&amp;var-environment=prod&amp;var-pod=All&amp;var-node=gke-k8s-demand-prd-a-np-dmnd-xdmnd-su-cd6eefd7-m288&amp;viewPanel=93\">grafana dashboard link</a></p><h2>Appendix</h2><h3>Downstream Service Details</h3><p>Support Slack channel - <code>#product-amplifyr-downstreams</code></p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"7ed3634e-240a-43ef-9b60-e992f6257adc\"><colgroup><col style=\"width: 206.0px;\" /><col style=\"width: 115.0px;\" /><col style=\"width: 168.0px;\" /><col style=\"width: 267.0px;\" /></colgroup><tbody><tr><th><p><strong>Downstream Service</strong></p></th><th><p><strong>Mandatory</strong></p></th><th><p><strong>PIC</strong></p></th><th><p><strong>Team (oncall handle)</strong></p></th></tr><tr><td><p>Price Aggregator</p></td><td><p>Yes</p></td><td><p>Rohit Raj</p></td><td><p>@supplier-services-oncall </p></td></tr><tr><td><p>Taxonomy</p></td><td><p>Yes</p></td><td><p>Sachin Kumar</p><p>Rohith Balaji</p></td><td><p>Cataloging (@sg-oncall)</p></td></tr><tr><td><p>Supplier Store</p></td><td><p>Yes</p></td><td><p /></td><td><p>@sg-oncall</p></td></tr><tr><td><p>User Profile Service</p></td><td><p>No</p></td><td><p /></td><td><p /></td></tr><tr><td><p>CMS</p></td><td><p>No</p></td><td><p>Saksham Agarwal</p></td><td><p>DTB (@dtb-oncall)</p></td></tr><tr><td><p>Review Service</p></td><td><p>No</p></td><td><p>Manan Bajaj</p></td><td><p>DTB (@dtb-oncall)</p></td></tr><tr><td><p>Reseller Tracking</p></td><td><p>No</p></td><td><p /></td><td><p>Discovery (@product-feed-oncall)</p></td></tr><tr><td><p>Feed Aggregator</p></td><td><p>No</p></td><td><p>Narayan Soni</p></td><td><p>Discovery (@product-feed-oncall)</p></td></tr><tr><td><p>Follow Service</p></td><td><p>No</p></td><td><p>Sanu Gupta</p></td><td><p>Cataloging</p></td></tr><tr><td><p>Route Aggregator Service</p></td><td><p>No</p></td><td><p /></td><td><p /></td></tr></tbody></table><p><strong>Grafana Metrics </strong></p><p><strong>Custom Metrics - </strong><a href=\"https://grafana-prd.meeshogcp.in/d/2I_RCyM7ksdsd/product-amplifyr-all-metrics?orgId=1&amp;var-service=product-amplifyr&amp;var-environment=prod&amp;var-pod=All&amp;var-node=gke-k8s-demand-prd-a-np-dmnd-xdmnd-su-cd6eefd7-p7rz&amp;from=now-15m&amp;to=now&amp;refresh=1m\">https://grafana-prd.meeshogcp.in/d/2I_RCyM7ksdsd/product-amplifyr-all-metrics?orgId=1&amp;var-service=product-amplifyr&amp;var-environment=prod&amp;var-pod=All&amp;var-node=gke-k8s-demand-prd-a-np-dmnd-xdmnd-su-cd6eefd7-p7rz&amp;from=now-15m&amp;to=now&amp;refresh=1m</a><br /><strong>Redis</strong> - <br /><strong>Product-amplifyr , Product-amplifyr-pdp, Product-amplifyr-cart</strong><br />Primary redis - <a href=\"https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-splat-amplifyr-1-prd-ase1-new&amp;var-cluster=c29181.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12104609&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m\">https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-splat-amplifyr-1-prd-ase1-new&amp;var-cluster=c29181.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12104609&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m</a><br />Secondary Redis - <a href=\"https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-splat-amplifyr-2-new-prd-ase1&amp;var-cluster=c33639.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12535067&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m\">https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-splat-amplifyr-2-new-prd-ase1&amp;var-cluster=c33639.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12535067&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m</a><br /><strong>Product-amplifyr-internal, Product-amplifyr-secondary</strong><br />Primary Redis - <a href=\"https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-splat-prd-amplifyr-1-prd-ase1&amp;var-cluster=c28855.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12070292&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m\">https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-splat-prd-amplifyr-1-prd-ase1&amp;var-cluster=c28855.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12070292&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m</a><br />Secondary Redis -  <a href=\"https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-splat-amplifyr-2-new-prd-ase1&amp;var-cluster=c33639.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12535067&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m\">https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-splat-amplifyr-2-new-prd-ase1&amp;var-cluster=c33639.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12535067&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m</a></p><p><strong>Envoy metrics</strong></p><p>Product-amplifyr - <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr\">https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr</a></p><p>Product-amplifyr-internal - <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr-internal&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr-internal&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr-internal\">https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr-internal&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr-internal&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr-internal</a></p><p>Product-amplifyr-cart - <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr-cart&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr-cart&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr-cart\">https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr-cart&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr-cart&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr-cart</a></p><p>Product-amplifyr-pdp - <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr-pdp&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr-pdp&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr-pdp\">https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr-pdp&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr-pdp&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr-pdp</a></p><p>Product-amplifyr-secondary - <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr-secondary&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr-secondary&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr-secondary\">https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;refresh=60s&amp;var-service=prd-product-amplifyr-secondary&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-product-amplifyr-secondary&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-product-amplifyr-secondary</a></p><p />",
        "representation": "storage",
        "word_count": 608
      },
      "version": {
        "number": 10,
        "when": "2024-09-25T10:17:51.957Z",
        "by": "Saksham Vijay Agarwal"
      },
      "labels": []
    },
    {
      "id": "3616833603",
      "title": "Meta Feed Sale Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3616833603",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h2><u>Grafana Links</u></h2><p><strong>Product Feed</strong></p><p><em>All Services Dashboard - </em><a href=\"https://grafana-prd.meeshogcp.in/dashboards/f/tFJq1BFMk/discovery-metrics\"><em>link</em></a></p><ol start=\"1\"><li><p>CMS -<br /><strong>Web</strong> - <a href=\"https://grafana-prd.meeshogcp.in/d/2I_RCyM7k/telegraf-services-dashboard-2?orgId=1&amp;var-env=prod&amp;var-service=cms-web\">https://grafana-prd.meeshogcp.in/d/2I_RCyM7k/telegraf-services-dashboard-2?orgId=1&amp;var-env=prod&amp;var-service=cms-web</a><br /><strong>Consumer</strong> - <a href=\"https://grafana-prd.meeshogcp.in/d/2I_RCyM7k/telegraf-services-dashboard-2?orgId=1&amp;var-env=prod&amp;var-service=cms-consumer&amp;from=now-15m&amp;to=now\">https://grafana-prd.meeshogcp.in/d/2I_RCyM7k/telegraf-services-dashboard-2?orgId=1&amp;var-env=prod&amp;var-service=cms-consumer&amp;from=now-15m&amp;to=now</a><br /><strong>Redis</strong> - <a href=\"https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-pmeta-cms-new-prd-ase1&amp;var-cluster=c28825.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12065917&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m\">https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=redis-dmnd-pmeta-cms-new-prd-ase1&amp;var-cluster=c28825.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12065917&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m</a><br /><strong>BigTable</strong> - <a href=\"https://console.cloud.google.com/bigtable/instances/bt-dmnd-pmeta-cmsmer-prd-ase1/overview?authuser=1&amp;project=meesho-demand-prd-0622\">https://console.cloud.google.com/bigtable/instances/bt-dmnd-pmeta-cmsmer-prd-ase1/overview?authuser=1&amp;project=meesho-demand-prd-0622</a><br /></p></li><li><p>Review -<br /><strong>Web</strong> - <a href=\"https://grafana-prd.meeshogcp.in/d/2I_RCyM7k/telegraf-services-dashboard-2?orgId=1&amp;var-env=prod&amp;var-service=review&amp;from=now-15m&amp;to=now\">https://grafana-prd.meeshogcp.in/d/2I_RCyM7k/telegraf-services-dashboard-2?orgId=1&amp;var-env=prod&amp;var-service=review&amp;from=now-15m&amp;to=now</a><br /><strong>Consumer </strong>- <a href=\"https://grafana-prd.meeshogcp.in/d/2I_RCyM7k/telegraf-services-dashboard-2?orgId=1&amp;var-env=prod&amp;var-service=review-consumer&amp;from=now-15m&amp;to=now\">https://grafana-prd.meeshogcp.in/d/2I_RCyM7k/telegraf-services-dashboard-2?orgId=1&amp;var-env=prod&amp;var-service=review-consumer&amp;from=now-15m&amp;to=now</a><br /><strong>DAO Layer Dashboard</strong> - <a href=\"https://grafana-prd.meeshogcp.in/d/gwMMqzf7z/review-service-custom?orgId=1\">https://grafana-prd.meeshogcp.in/d/gwMMqzf7z/review-service-custom?orgId=1</a><br /><strong>Redis</strong> - <a href=\"https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=prd-dmnd-pmeta-review&amp;var-cluster=c28736.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12060658&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m\">https://grafana-prd.meeshogcp.in/d/X_ax1wHnk/discovery-redis?orgId=1&amp;var-bdb_name=prd-dmnd-pmeta-review&amp;var-cluster=c28736.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12060658&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m</a><a href=\"https://grafana.meesho.com/d/X_ax1wHnk/discovery-redis?orgId=1&amp;refresh=5s&amp;var-bdb_name=prd-dmnd-pmeta-review-new&amp;var-cluster=c20145.ap-seast-1-mz.ec2.cloud.rlrcp.com&amp;var-bdb=11093887&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=1m\">gregation=1m</a><br /><strong>RDS</strong> - <a href=\"https://console.cloud.google.com/sql/instances/msql-dmnd-pmeta-review-prd-ase1/overview?authuser=1&amp;project=meesho-demand-prd-0622\">https://console.cloud.google.com/sql/instances/msql-dmnd-pmeta-review-prd-ase1/overview?authuser=1&amp;project=meesho-demand-prd-0622</a><br /><strong>BigTable</strong> - <a href=\"https://console.cloud.google.com/bigtable/instances/bt-demand-dtb-review-prd-ase1/overview?authuser=1&amp;project=meesho-demand-prd-0622\">https://console.cloud.google.com/bigtable/instances/bt-demand-dtb-review-prd-ase1/overview?authuser=1&amp;project=meesho-demand-prd-0622</a><a href=\"http://ip-172-31-17-130.ap-southeast-1.compute.internal:16010/master-status#baseStats\">aseStats</a><br /></p></li><li><p>Product Amplifyr - <br /><strong>Custom Metrics - </strong><a href=\"https://grafana-prd.meeshogcp.in/d/2I_RCyM7ksdsd/product-amplifyr-all-metrics?orgId=1&amp;var-service=product-amplifyr&amp;var-environment=prod&amp;var-pod=All&amp;var-node=gke-k8s-demand-prd-a-np-dmnd-xdmnd-su-cd6eefd7-p7rz&amp;from=now-15m&amp;to=now&amp;refresh=1m\">https://grafana-prd.meeshogcp.in/d/2I_RCyM7ksdsd/product-amplifyr-all-metrics?orgId=1&amp;var-service=product-amplifyr&amp;var-environment=prod&amp;var-pod=All&amp;var-node=gke-k8s-demand-prd-a-np-dmnd-xdmnd-su-cd6eefd7-p7rz&amp;from=now-15m&amp;to=now&amp;refresh=1m</a><br />Redis - <br /></p></li><li><p><strong>MQ Producers</strong></p></li></ol><hr /><h2>Run Book</h2><p>&nbsp;</p><ol start=\"1\"><li><p><strong><span style=\"color: rgb(191,38,0);\">Add Kill Switch for all services. Eg - How to turn off review/rating if required?</span></strong></p></li><li><p><strong><span style=\"color: rgb(191,38,0);\">Add monitoring of fallback responses. Eg - How to check if widget fallback response is correct? How to update?</span></strong></p></li></ol><p>We have fallback setup at 2 places one at Edge Proxy and one at CLoudflare CDN. the Cloudflare fallback will only be used if there some issue with EP and the EP fallback also doesn&rsquo;t work</p><h4>EdgeProxy :</h4><p>When the call from EP to widgetAggregator fails, we have configured a fallback response in zookeeper which is served, and no 5xx are thrown to upstream.</p><p>Grafana For checking if traffic is being served from fallback : <a href=\"https://grafana.meesho.com/d/bo4La1SVk/edge-proxy-business-metrics?orgId=1&amp;from=1696312053544&amp;to=1696334129315&amp;viewPanel=104\">FallBack-Metrics</a></p><h4>CDN :</h4><p>When due to some reason we are getting 5xx from EP side , we have a fallback confgured at CDN level which works for users above app Version 507. This CDN cache is refreshed in every 4 hours using and api in WidgetAggregator <code>/api/fallback/1.0/home-page/fetch</code> If the cache has expired or the data is not found in CDN, Cloudflare will call the same downstream WA api .</p><p>Monitoring Link &rarr; To Be added</p><p /><ol start=\"3\"><li><p><strong><span style=\"color: rgb(191,38,0);\">How to debug Collection Tagging/Detagging issue?</span></strong></p><ol start=\"1\"><li><p>check the lag of catalog collection event in marvel</p></li><li><p>then check the lag between marvel to ofs indexer</p></li><li><p>if there is no lag then check in es with query</p></li><li><p>and if the result is not satisfied the ask them to tag it again.</p></li><li><p>If we are getting urgent requirement to tag detag then we will use our python script to do this opetation (overall it takes max 10 min to tag and detag in ES)</p></li><li><p>And if we are getting urgent requirement to detag catalog from the collection on urgent basis then you can use this command ( this is temporary but quick, directly in ES)<br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"93ebf6b2-23c3-46a1-b1ba-987799165755\"><ac:plain-text-body><![CDATA[POST <index>/_update_by_query\n{\n  \"query\": {\n    \"terms\": {\n      \"collection_ids\": [72509]\n    }\n  },\n  \"script\" : {\n             \"source\": \"ctx._source.collection_ids.removeAll(Collections.singleton(params.coll_ids))\",\n             \"lang\": \"painless\",\n             \"params\" : {\n                 \"coll_ids\" : 72509\n             }\n         }\n} \n]]></ac:plain-text-body></ac:structured-macro></li></ol></li><li><p><strong><span style=\"color: rgb(191,38,0);\">How to debug Widget not showing issue?</span></strong></p><ol start=\"1\"><li><p>We have a tool built on admin panel, and here&rsquo;s the handbook for it - <a href=\"https://docs.google.com/document/d/10gDTTdN5AnbGyvLTZjfgXF_CqVj__tjSJPEAlAWvBR8\">Debug Your Widget - Handbook</a></p></li></ol></li><li><p><strong><span style=\"color: rgb(191,38,0);\">How to check and act on lag on OFS indexer?</span></strong></p><ol start=\"1\"><li><p>If you see any lag on indexer side then ideally you can add more pods but we also need to take care of indexing rate. so first monitor the indexing rate and then slowly add the pods</p></li></ol></li><li><p><strong><span style=\"color: rgb(191,38,0);\">When and how to increase TTL for OFS?</span></strong></p><ol start=\"1\"><li><p>When ES is in trouble (Increased Search rate , ES cpu is high like more then 70% ) then you need to change the zk value of stale-interval-in-sec, you need to change this in both web and consumer. current value is 400 sec. you can increase this upto the 60 min ( depends upon how much time you want to buy to fix the underlying ES issue. ) you also need to check the OOS graph</p></li></ol></li><li><p><strong><span style=\"color: rgb(191,38,0);\">When and how to use es-saviour-mode in OFS?</span></strong></p><ol start=\"1\"><li><p>When ES is in trouble and <strong>Above Point 6a</strong> is not able to save you then you need to enable this, this will cut the ES calls from P1 api and ofs consumer. ZK flag for this is es-saviour-mode</p></li></ol></li><li><p><strong><span style=\"color: rgb(191,38,0);\">When and how to use Fallback ES in OFS?</span></strong></p><ol start=\"1\"><li><p>When <strong>Above point 6a and 7a </strong>is not working then we need to switch our ES and for this there is one config - organic-feed-web-&gt;elastic-search-&gt;cluster, you need to change this value primary to secondary in both web and consumer.</p></li><li><p>once ES is recovered then change this back to primary.</p></li></ol></li><li><p><strong><span style=\"color: rgb(191,38,0);\">When and how to remove redis call (in case of redis failure) from wishlist aggregate API in reseller tracking?</span></strong></p><ol start=\"1\"><li><p>zk flag (use-cache-in-aggregate-api-product-call)</p></li><li><p>In case of redis failure in reseller tracking service, our hbase is capable to handle the load so we need to turn off the above flag.</p></li></ol></li><li><p><strong><span style=\"color: rgb(191,38,0);\">Review Service degradation levers &amp; fallback</span></strong> - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Review Service - Disaster Recovery\" ri:version-at-save=\"1\" /><ac:link-body>Review Service - Disaster Recovery</ac:link-body></ac:link></p></li></ol>",
        "representation": "storage",
        "word_count": 731
      },
      "version": {
        "number": 2,
        "when": "2024-09-24T07:59:00.184Z",
        "by": "Saksham Vijay Agarwal"
      },
      "labels": []
    },
    {
      "id": "3616735238",
      "title": "Faulty Nodes runbook and a go through",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/SRE/pages/3616735238",
      "space": {
        "key": "SRE",
        "name": "SRE"
      },
      "content": {
        "body": "<p>This document tries to define what is considered as a faulty node. It also outlines the several cases of faulty nodes we have encountered since GCP migration, including the efforts made to reduce the frequency and impact of those issues.</p><h2>What is a faulty node?</h2><p>The building blocks of all clouds are ultimately a rack of computers, and with the help of virtualisation these are then distributed into several virtual machines. Each and every service offered by any cloud provider is in some way running on these virtual machines.</p><p>In Kubernetes, a node is also a virtual machine (VM). This makes any node to be prone to issues originating either from hardware or the platform issues of the cloud provider. In our experience of running Kubernetes at scale since November 2022 now, we have seen that both AWS and GCP are susceptible to these hardware/platform issues. The frequency of these issues however varies.</p><p>Faulty nodes don&rsquo;t always show one single behaviour, in fact there are several issues which can result in considering a node as faulty.</p><p>We define a node X to be faulty in cases when:</p><ul><li><p>Application pods assigned to node X remain in pending state due to node&rsquo;s network sandbox entering the not-ready state.</p></li><li><p>All application pods fail to become healthy or intermittently become healthy on node X, given that the pods have enough compute and become healthy on other nodes)</p></li><li><p>All replicas running on node X observe higher network ingress/egress latencies as compared to the replicas running on some other nodes (given that all warmup periods have ended).</p></li><li><p>Node X gets removed from the Node Pool before one or more application pods could gracefully terminate.</p></li><li><p>All pods running on the node X observe partial or complete network packet loss.</p></li></ul><p>We will now discuss each case in detail, their current status and plans to reduce the impact.</p><h2><u>Case 1: Pods remain in pending state</u></h2><p>In this scenario the pods which are nominated to be scheduled in the given node remain in pending state and.</p><p><strong>When does this happen: </strong>This only happens for a new node and a new pod. This does not impact already running pods/nodes.</p><p><strong>How to Identify</strong>: Any pod which remains in a pending state for more than 5 minutes and has the most recent event which says something similar to:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"1a751a70-f5fb-4c3e-b04f-afe4e7629d9e\"><ac:plain-text-body><![CDATA[Failed to schedule pod, node network sandbox not ready]]></ac:plain-text-body></ac:structured-macro><p><strong>Current State </strong><ac:emoticon ac:name=\"tick\" ac:emoji-shortname=\":check_mark:\" ac:emoji-id=\"atlassian-check_mark\" ac:emoji-fallback=\":check_mark:\" /> : We have not seen even a single occurrence of this issue in last one June and July. This was reported to GCP at a stage when it was severely impacting the autoscaling of several applications and was having more than 2 occurrences each day.</p><h2><u>Case 2: Pods fail to become healthy or enter a restart loop</u></h2><p>In this case, the pods which are scheduled on the node enter the running phase but either fail to become ready or intermittently become ready and the service continues to face restarts, resulting in the pod entering a crash loop.</p><p><strong>When does this happen: </strong>This has only be seen to happen for a new node and a new pod. However this can also happen for a already running pod/node in cases when there are abrupt hardware failures, although we have not seen those occurrences yet</p><p><strong>How to Identify</strong>: Debugging this kind of failures requires a series of debugging:</p><ul><li><p>In ArgoCD, group the pods by nodes and see whether the pods which are restarting belong to a single node. If this is not the case or all the pods are restarting then it is most likely a case of service issue or the serving going OOM/OutOfCPU/DiskPressure, and will not be a faulty node issue.</p></li><li><p>Observe the events of the pods which are restarting, for things like OOM/OutOfCPU/DiskPressure. If any of these is present then it is not the case of a faulty node.</p></li><li><p>Once above two have been things have been verified then it is quite possible that this could be the case of a faulty node. The following things can further strengthen the claim:</p><ul><li><p>All pods on the node are restarting, including those of a different service</p></li><li><p>The node is relatively new<br /><br /></p></li></ul></li></ul><ul><li><p><strong>Steps taken</strong>: We have setup an alert which immediately alarms the oncalls in cases when 50% or more pods on a given node are restarting. This is a reactive measure, as these nodes do not have any particular anomaly which could be used to identify such nodes proactively.</p></li></ul><p><br /><strong>How to mitigate/runbook: </strong><br /><br />1. <strong>Node Identification and check</strong> - This can be done in 3 ways &rarr; <br />  <strong>i) via vmselect -</strong> execute the below query in <a href=\"https://vmselect-infra-prd.meeshogcp.in/select/100/vmui/#/\">https://vmselect-infra-prd.meeshogcp.in/select/100/vmui/#/</a> after finding the particular pods that are failing/restarting/terminating with a loop.<br /><code>kube_pod_info{pod=&lt;pod-name&gt;}</code><br /><br />From this we could get the node name and other details such as host-ip, pod-ip etc.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"829\" ac:original-width=\"878\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-09-23 at 2.07.00 PM.png\" ri:version-at-save=\"1\" /></ac:image><p> <br /><strong>ii) via kubectl command - </strong><br />After knowing the respective pod and namespace name, as the devops team has access to all clusters with improved access, execute the below command to know the result as nodename.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"53b90b5f-102f-4db7-81ac-d77da1896a63\"><ac:plain-text-body><![CDATA[kubectl get pod <pod-name> -n <namespace> -o jsonpath='{.spec.nodeName}']]></ac:plain-text-body></ac:structured-macro><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"106\" ac:original-width=\"1088\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-09-23 at 2.12.09 PM.png\" ri:version-at-save=\"1\" /></ac:image><p><strong> 2. Identification whether its a faulty node or not - </strong><br /> i) Firstly, observe the graphs from the node monitor, observe that is only one pod/one service is behaving like this or multiple pods from diff service are also behaving. <br /><a href=\"https://grafana-prd.meeshogcp.in/d/a3aeec01-a917-476e-a0b7-d256d4c42bc7/gke-node-monitor-by-name?orgId=1\">https://grafana-prd.meeshogcp.in/d/a3aeec01-a917-476e-a0b7-d256d4c42bc7/gke-node-monitor-by-name?orgId=1</a><br />ii) You would see spiky CPU and unstable CPU spikes in those pods, which shows the unstable nature of a node which becomes faulty.<br />iii) Before concluding it as a faulty node, make sure that there are no other issues from infra or the application end which could be resolved in that pod by changing necessary conditions.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"1117\" ac:original-width=\"1728\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-09-23 at 4.20.55 PM.png\" ri:version-at-save=\"1\" /></ac:image><p> <br /><strong>3. Things to be performed after identification of faulty node - </strong><br />We need all the below mentioned reports before we raise the GCP ticket for faulty node issue - <br /><br />Firstly, go the the respective google project and from the compute engine/VM instances link, go to the node and exec it or copy the ssh command. Example - <br /><code>gcloud compute ssh --zone &quot;asia-southeast1-b&quot; &quot;gke-k8s-admin-prd-as-np-admin-default-3e1551bb-zub8&quot; --tunnel-through-iap --project &quot;meesho-admin-prd-0622&quot;</code> <br /><br />After doing exec, the following steps needs to be done/ran on the node to collect the details<br /> <strong>i) Generate sos-report </strong><br /><code>sudo sos report --all-logs --batch --tmp-dir=/var</code><br /><strong>ii) Tcp dump command</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"4d244feb-cbdc-4776-9ae8-5eb5ca78af52\"><ac:plain-text-body><![CDATA[sudo toolbox bash \napt install -y tcpdump \ntimeout 60 tcpdump -i eth0 -w capture.pcap \n#on host the file would be in /var/lib/toolbox/<containername>]]></ac:plain-text-body></ac:structured-macro><p><strong>iii) top command</strong><br /><code>top -b -n1&gt; top.dump</code><br /><strong>iv) ps aux command</strong><br /><code>ps aux --sort=-%mem | head &gt; ps.dump</code><br /><strong>v) thread dump/java process dump on node</strong><br /><code>ps -To pcpu,tid,pid,cmd -C java</code><br /><strong>vi) Heap and thread dump on pod (if needed from GCP) - </strong></p><p>pod/container heap dump</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"93798462-c33a-4513-8e14-0a94921d7d41\"><ac:plain-text-body><![CDATA[kubectl exec -it <pod-name> -- /bin/bash -c jmap -dump:format=b,file=<pod-name>_heap_267 1]]></ac:plain-text-body></ac:structured-macro><p>pod/container thread dump</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"73cb1611-d94c-4449-ade7-0893901444ac\"><ac:plain-text-body><![CDATA[kubectl exec -it <pod-name> -- /bin/bash -c jstack 1 > <pod-name>_thread_265]]></ac:plain-text-body></ac:structured-macro><p>use <code>kubectl cp</code> to download from there after exec and running the above codes.<br /><br /><br /><strong>Raise </strong><a href=\"https://console.cloud.google.com/support/cases?project=meesho-admin-prd-0622\"><strong>GCP support ticket</strong></a><strong> with this relation and mention it with critical case P1 thereby also mentioning it in </strong><span style=\"color: rgb(76,154,255);\">#meesho-google </span><strong>slack channel for active response and tracking.</strong><br /><br />[ Ref ] ticket &ndash; <a href=\"https://console.cloud.google.com/support/cases/detail/v2/53629165?project=meesho-demand-prd-0622\">https://console.cloud.google.com/support/cases/detail/v2/53629165?project=meesho-demand-prd-0622</a><br /><br />[ Ref ] explanation from GCP regarding this with recent timeframes - there was some degree of hot-spotting to a given physical host. Our backline teams found that many of the identified GKE nodes where the issue was observed were found to be hosted on the same physical machine. Although we did not discover any problematic symptoms to indicate a host-level issue, we have proactively marked it unschedulable to schedule any GCE instance on that host as a &nbsp;precautionary measure for now.<br /> <br />After that, the node needs to be drained and cordoned by us and that node would be marked as unschedulable by GCP team. <strong>kubectl drain</strong> evicts existing pods from a node and marks it schedulable, ensuring a graceful transition of workloads to other nodes. <strong>kubectl cordon or the cordon button </strong>marks a node as schedulable, preventing new pods from being scheduled but allowing existing pods to continue running until further action is taken. Its upon us how we want to run these.<br /><br />For <strong>cordoning the node</strong>, go to the respective node and mark it as cordon from the option specified.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"1117\" ac:original-width=\"1728\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-09-23 at 3.42.51 PM.png\" ri:version-at-save=\"1\" /></ac:image><p> <br /><br /><br /><br /></p><p /><p />",
        "representation": "storage",
        "word_count": 1424
      },
      "version": {
        "number": 3,
        "when": "2024-09-25T10:41:46.812Z",
        "by": "Roshan Kumar Patro V"
      },
      "labels": []
    },
    {
      "id": "3612246023",
      "title": "Dragonfly Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3612246023",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<p>Dragonfly is an in-memory datastore, fully compatible with Redis and Memcached APIs. Dragonfly implements novel algorithms and data structures on top of a multi-threaded, shared-nothing architecture. <br />As a result, Dragonfly reaches 25X performance compared to Redis and supports millions of QPS on a single instance.</p><p>Dragonfly is fully compatible with the Redis ecosystem and requires no code changes to implement.<br /><br />We have below instances in prod : <br />In supply project - ctqct - mcache-supl-ctlng-ct-qct-v2 - 3nodes <br />In datascience project - feature-v4-iop - 3 nodes<br />In datascience project -  feature-v4-mp- 6 nodes <br />In datascience project -  feature-exp- 2 nodes<br />In datascience project -  feature-vss- 2 nodes  <br /><br /><strong>Grafana dashboards</strong> <br />Includes mcache level metrics like read and write p99 latency, and overall RPS, with their respective thresholds.</p><p><a href=\"https://grafana-prd.meeshogcp.in/d/d91f7248-129b-4ed8-9bf9-d11bd48046f3/mcache-basic?orgId=1\">https://grafana-prd.meeshogcp.in/d/d91f7248-129b-4ed8-9bf9-d11bd48046f3/mcache-basic?orgId=1</a></p><p><strong>Dragonfly grafana for each nodes:</strong></p><p><a href=\"https://grafana-prd.meeshogcp.in/d/dragonfly_final/dragonfly-final?orgId=1&amp;refresh=5s\">https://grafana-prd.meeshogcp.in/d/dragonfly_final/dragonfly-final?orgId=1&amp;refresh=5s</a> <br /></p><p />",
        "representation": "storage",
        "word_count": 140
      },
      "version": {
        "number": 1,
        "when": "2024-09-20T13:20:33.607Z",
        "by": "Vinuthana PM"
      },
      "labels": []
    },
    {
      "id": "3607232623",
      "title": "Supplier Growth Frontend Sale Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h2>Overview</h2><ul><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#Architecture-Reviews\">Architecture Reviews</a></p></li><li><p>Monitoring</p><ul><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#Grafana-Monitoring\">Grafana</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#DB-Monitoring\">DB</a></p></li></ul></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#Logging\">Logging</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#CDN-Layer\">CDN Layer</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#Analytics\">Analytics</a></p></li><li><p>Alerts</p><ul><li><p><a href=\"https://docs.google.com/spreadsheets/d/1GTKmtTT76PGnqeHGfpcD9SY0uR9DWzU3BZxchM8-FZc/edit?usp=sharing\">on-call roster</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#Pulse\">Pulse</a></p></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#Pagerduty\">Pagerduty</a></p></li></ul></li><li><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3607232623/Supplier+Growth+Frontend+Sale+Runbook#Scaling\">Scaling</a></p></li></ul><h2>Architecture Reviews</h2><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"5122620d-2974-4d52-a4eb-6e848a1d8f05\"><tbody><tr><th><p><strong>service</strong></p></th><th><p><strong>review links</strong></p></th></tr><tr><td><p>supplier_platform_container</p></td><td><p><ac:link><ri:page ri:content-title=\"Supplier Platform Container Architecture Review\" ri:version-at-save=\"4\" /><ac:link-body>link</ac:link-body></ac:link></p></td></tr><tr><td><p>supplier_platform_cataloging</p></td><td><p><ac:link><ri:page ri:content-title=\"[Sale readiness] Supplier Platform Cataloging\" ri:version-at-save=\"2\" /><ac:link-body>link</ac:link-body></ac:link></p></td></tr><tr><td><p>supplier_platform_growth</p></td><td><p><a href=\"https://docs.google.com/document/d/1X_JZ9wljFJo824YI0WbVg8RQosnlh_n-7143JlrCb80/edit?usp=sharing\">link</a></p></td></tr><tr><td><p>supplier_platform_insights</p></td><td><p><ac:link><ri:page ri:content-title=\"[Sale Rediness] Supplier Platform Insights\" ri:version-at-save=\"1\" /><ac:link-body>link</ac:link-body></ac:link></p></td></tr></tbody></table><h2>Grafana Monitoring</h2><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"6cfed5e3-3d26-4d03-997a-de142ad2cea3\"><tbody><tr><th><p><strong>Service</strong></p></th><th><p><strong>Telegraf</strong></p></th><th><p><strong>Envoy</strong></p></th><th><p><strong>Infra Monitoring</strong></p></th><th><p><strong>Uptime</strong></p></th><th><p><strong>Jenkins CD Job</strong></p></th><th><p><strong>Vault</strong></p></th><th><p><strong>npmrc</strong></p></th></tr><tr><td><p>supplier_platform_container</p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/m992dA14z3/telegraf-services-dashboard-gke?orgId=1&amp;var-service=supplier-platform-container&amp;var-datasource=Prometheus&amp;from=now-7d&amp;to=now\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-supplier-platform-container_prd-supplier-platform-container_80&amp;var-hosts=All&amp;from=now-7d&amp;to=now&amp;refresh=60s&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.90&amp;var-latency_percentile_period=1m&amp;var-latency_percentile_threshold=0.90&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;var-service=prd-supplier-platform-container&amp;var-cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-namespace=prd-supplier-platform-container&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-copy_of_namespace=prd-supplier-platform-container&amp;from=now-7d&amp;to=now&amp;refresh=60s\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/w72igLl4k/system-uptime-stateless?orgId=1&amp;var-telegraf_service=supplier-platform-container&amp;var-cluster=prd-supplier-platform-container_prd-supplier-platform-container_80&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.99&amp;var-latency_percentile_period=5m&amp;var-latency_percentile_threshold=0.999&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d&amp;var-uri=All&amp;from=now-7d&amp;to=now\">link</a></p></td><td><p><a href=\"https://jenkins-prd.meeshogcp.in/job/supplier_platform_container-cicd/\">link</a></p></td><td><p><a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/xsupl/supplier_platform_container\">link</a></p></td><td><p><a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/xsupl/supplier_platform_container-npmrc-prd\">link</a></p></td></tr><tr><td><p>supplier_platform_growth</p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/m992dA14z3/telegraf-services-dashboard-gke?orgId=1&amp;var-service=supplier-platform-growth&amp;var-datasource=Prometheus&amp;from=now-7d&amp;to=now\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-supplier-platform-growth_prd-supplier-platform-growth_80&amp;var-hosts=All&amp;from=now-7d&amp;to=now&amp;refresh=60s&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.90&amp;var-latency_percentile_period=1m&amp;var-latency_percentile_threshold=0.90&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;var-service=prd-supplier-platform-growth&amp;var-cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-namespace=prd-supplier-platform-growth&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-copy_of_namespace=prd-supplier-platform-growth&amp;from=now-7d&amp;to=now&amp;refresh=60s\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/w72igLl4k/system-uptime-stateless?orgId=1&amp;var-telegraf_service=supplier-platform-growth&amp;var-cluster=prd-supplier-platform-growth_prd-supplier-platform-growth_80&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.99&amp;var-latency_percentile_period=5m&amp;var-latency_percentile_threshold=0.999&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d&amp;var-uri=All&amp;from=now-7d&amp;to=now\">link</a></p></td><td><p><a href=\"https://jenkins-prd.meeshogcp.in/job/supplier_platform_growth-cicd/\">link</a></p></td><td><p><a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/fctlg/supplier_platform_growth\">link</a></p></td><td><p><a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/fctlg/supplier_platform_growth-npmrc-prd\">link</a></p></td></tr><tr><td><p>supplier_platform_cataloging</p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/m992dA14z3/telegraf-services-dashboard-gke?orgId=1&amp;var-service=supplier-platform-cataloging&amp;var-datasource=Prometheus&amp;from=now-7d&amp;to=now\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-supplier-platform-cataloging_prd-supplier-platform-cataloging_80&amp;var-hosts=All&amp;from=now-7d&amp;to=now&amp;refresh=60s&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.90&amp;var-latency_percentile_period=1m&amp;var-latency_percentile_threshold=0.90&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;var-service=prd-supplier-platform-cataloging&amp;var-cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-namespace=prd-supplier-platform-cataloging&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-copy_of_namespace=prd-supplier-platform-cataloging&amp;from=now-7d&amp;to=now&amp;refresh=60s\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/w72igLl4k/system-uptime-stateless?orgId=1&amp;var-telegraf_service=supplier-platform-cataloging&amp;var-cluster=prd-supplier-platform-cataloging_prd-supplier-platform-cataloging_80&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.99&amp;var-latency_percentile_period=5m&amp;var-latency_percentile_threshold=0.999&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d&amp;var-uri=All&amp;from=now-7d&amp;to=now\">link</a></p></td><td><p><a href=\"https://jenkins-prd.meeshogcp.in/job/supplier_platform_cataloging-cicd/\">link</a></p></td><td><p><a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/fctlg/supplier_platform_cataloging\">link</a></p></td><td><p><a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/fctlg/supplier_platform_cataloging-npmrc-prd\">link</a></p></td></tr><tr><td><p>supplier_platform_insights</p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/m992dA14z3/telegraf-services-dashboard-gke?orgId=1&amp;var-service=supplier-platform-insights&amp;var-datasource=Prometheus&amp;from=now-7d&amp;to=now\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-supplier-platform-insights_prd-supplier-platform-insights_80&amp;var-hosts=All&amp;from=now-7d&amp;to=now&amp;refresh=60s&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.90&amp;var-latency_percentile_period=1m&amp;var-latency_percentile_threshold=0.90&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;var-service=prd-supplier-platform-insights&amp;var-cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-namespace=prd-supplier-platform-insights&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-supply-prd-ase1&amp;var-copy_of_namespace=prd-supplier-platform-insights&amp;from=now-7d&amp;to=now&amp;refresh=60s\">link</a></p></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/w72igLl4k/system-uptime-stateless?orgId=1&amp;var-telegraf_service=supplier-platform-insights&amp;var-cluster=prd-supplier-platform-insights_prd-supplier-platform-insights_80&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.99&amp;var-latency_percentile_period=5m&amp;var-latency_percentile_threshold=0.999&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d&amp;var-uri=All&amp;from=now-7d&amp;to=now\">link</a></p></td><td><p><a href=\"https://jenkins-prd.meeshogcp.in/job/supplier_platform_insights-cicd/\">link</a></p></td><td><p><a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/fctlg/supplier_platform_insights\">link</a></p></td><td><p><a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/fctlg/supplier_platform_insights-npmrc-prd\">link</a></p></td></tr></tbody></table><h2>DB Monitoring</h2><p>Supplier Panel Redis Cluster Monitoring Dashboard ( BDB Dashboard ) [ <a href=\"https://grafana-prd.meeshogcp.in/d/jZWX54Nnk/bdb-dashboard?orgId=1&amp;var-cluster=c28819.asia-seast1-mz.gcp.cloud.rlrcp.com&amp;var-bdb=12065762&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation&amp;from=now-7d&amp;to=now\">link</a> ]</p><p>Shared MySQL instance ( msql-web-adminpanel-master.prd.meesho.int ), following are the monitoring references:</p><ul><li><p>CloudSQL</p><ul><li><p>Grafana [ <a href=\"https://grafana-prd.meeshogcp.in/d/c451fd5d-1390-4116-af7a-482cc0325681/cloudsql?orgId=1&amp;var-bu=demand&amp;var-database=meesho-demand-prd-0622:msql-dmnd-web-adminpanel-prd-ase1&amp;from=now-7d&amp;to=now\">link</a> ]</p></li><li><p>GCP [ <a href=\"https://console.cloud.google.com/sql/instances/msql-dmnd-web-adminpanel-prd-ase1/overview?project=meesho-demand-prd-0622\">link</a> ]</p></li></ul></li></ul><h2>Logging</h2><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"3953cca5-440e-4bb0-a2b0-56cb6ef80a04\"><tbody><tr><th><p><strong>Service</strong></p></th><th><p><strong>ElastiSearch Logs</strong></p></th><th><p><strong>GCS Bucket Infra Logs</strong></p></th></tr><tr><td><p>supplier_platform_container</p></td><td><p><a href=\"https://0b6ea1325493466faf2f9be526283f5a.psc.asia-southeast1.gcp.elastic-cloud.com/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-1w,to:now))&amp;_a=(columns:!(),filters:!(),index:'5e4fda77-086c-4a1c-8e38-69e060c35f7b',interval:auto,query:(language:kuery,query:'application_name%20:%20%22prd-supplier-platform-container%22'),sort:!(!('@timestamp',asc)))\">link</a></p></td><td><p><a href=\"https://console.cloud.google.com/storage/browser/gcs-infr-dvps-meesho-logs-prd/supply/prd-supplier-platform-container?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&amp;project=meesho-admin-prd-0622&amp;prefix=&amp;forceOnObjectsSortingFiltering=false\">link</a></p></td></tr><tr><td><p>supplier_platform_growth</p></td><td><p><a href=\"https://0b6ea1325493466faf2f9be526283f5a.psc.asia-southeast1.gcp.elastic-cloud.com/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-1w,to:now))&amp;_a=(columns:!(),filters:!(),index:'5e4fda77-086c-4a1c-8e38-69e060c35f7b',interval:auto,query:(language:kuery,query:'application_name%20:%20%22prd-supplier-platform-growth%22'),sort:!(!('@timestamp',asc)))\">link</a></p></td><td><p><a href=\"https://console.cloud.google.com/storage/browser/gcs-infr-dvps-meesho-logs-prd/supply/prd-supplier-platform-growth?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&amp;project=meesho-admin-prd-0622&amp;prefix=&amp;forceOnObjectsSortingFiltering=false\">link</a></p></td></tr><tr><td><p>supplier_platform_cataloging</p></td><td><p><a href=\"https://0b6ea1325493466faf2f9be526283f5a.psc.asia-southeast1.gcp.elastic-cloud.com/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-3d,to:now))&amp;_a=(columns:!(),filters:!(),index:'5e4fda77-086c-4a1c-8e38-69e060c35f7b',interval:auto,query:(language:kuery,query:'application_name%20:%20%22prd-supplier-platform-cataloging%22'),sort:!(!('@timestamp',asc)))\">link</a></p></td><td><p><a href=\"https://console.cloud.google.com/storage/browser/gcs-infr-dvps-meesho-logs-prd/supply/prd-supplier-platform-cataloging?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&amp;project=meesho-admin-prd-0622&amp;prefix=&amp;forceOnObjectsSortingFiltering=false\">link</a></p></td></tr><tr><td><p>supplier_platform_insights</p></td><td><p><a href=\"https://0b6ea1325493466faf2f9be526283f5a.psc.asia-southeast1.gcp.elastic-cloud.com/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-3d,to:now))&amp;_a=(columns:!(),filters:!(),index:'5e4fda77-086c-4a1c-8e38-69e060c35f7b',interval:auto,query:(language:kuery,query:'application_name%20:%20%22prd-supplier-platform-insights%22'),sort:!(!('@timestamp',asc)))\">link</a></p></td><td><p><a href=\"https://console.cloud.google.com/storage/browser/gcs-infr-dvps-meesho-logs-prd/supply/prd-supplier-platform-insights?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&amp;project=meesho-admin-prd-0622&amp;prefix=&amp;forceOnObjectsSortingFiltering=false\">link</a></p></td></tr></tbody></table><h2>CDN Layer</h2><ul><li><p>Akamai [ <a href=\"https://control.akamai.com/apps/home-page/#/home\">link</a> ]</p></li><li><p>Cloudflare [ <a href=\"https://dash.cloudflare.com/5d7b32945c76c32f7b670624dc216fb8/meesho.com\">link</a> ]</p></li></ul><h2>Analytics</h2><ul><li><p>mixpanel</p></li><li><p>metabase:</p><ul><li><p>SG Metabase Tables [ <ac:link><ri:page ri:content-title=\"SG Metabase Tables\" ri:version-at-save=\"3\" /><ac:link-body>link</ac:link-body></ac:link> ]</p></li></ul></li></ul><h2>Alerts</h2><ul><li><p><a href=\"https://docs.google.com/spreadsheets/d/1GTKmtTT76PGnqeHGfpcD9SY0uR9DWzU3BZxchM8-FZc/edit?usp=sharing\">on-call roster</a></p></li></ul><h4>Pulse</h4><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"1deb5983-c90b-4598-8d40-76f0dd97e697\"><tbody><tr><th><p><strong>service</strong></p></th><th><p><strong>alerts</strong></p></th></tr><tr><td><p>supplier_platform_container</p></td><td><p><a href=\"https://pulse.meeshogcp.in/alert/alerts/supplier-platform-container\">link</a></p></td></tr><tr><td><p>supplier_platform_growth</p></td><td><p><a href=\"https://pulse.meeshogcp.in/alert/alerts/supplier-platform-growth\">link</a></p></td></tr><tr><td><p>supplier_platform_cataloging</p></td><td><p><a href=\"https://pulse.meeshogcp.in/alert/alerts/supplier-platform-cataloging\">link</a></p></td></tr><tr><td><p>supplier_platform_insights</p></td><td><p><a href=\"https://pulse.meeshogcp.in/alert/alerts/supplier-platform-insights\">link</a></p></td></tr></tbody></table><h4>Pagerduty</h4><ul><li><p>insights report</p><ul><li><p><a href=\"https://meesho.pagerduty.com/analytics/insights/incident-activity-report/tGwVyLYakFz5MEd7luWiMw\">container</a></p></li><li><p><a href=\"https://meesho.pagerduty.com/analytics/insights/incident-activity-report/_k_h1EQ3clfqz7U3q7Q6WQ\">cataloging</a></p></li><li><p><a href=\"https://meesho.pagerduty.com/analytics/insights/incident-activity-report/lSfKbKRkHk-Y4iE62D9O3A\">growth</a></p></li><li><p><a href=\"https://meesho.pagerduty.com/analytics/insights/incident-activity-report/ojdzk75NUBWEdbOLcT6FTQ\">insights</a></p></li></ul></li><li><p>on-call schedules</p></li></ul><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"6e170c36-8394-453d-b75a-594c95fcf876\"><tbody><tr><th><p><strong>service</strong></p></th><th><p><strong>Primary</strong></p></th><th><p><strong>Seconday</strong></p></th></tr><tr><td><p>cataloging + growth + insights</p></td><td><p><a href=\"https://meesho.pagerduty.com/schedules-new/PIXDFMW\">link</a></p></td><td><p><a href=\"https://meesho.pagerduty.com/schedules/PELSFN6\">link</a></p></td></tr><tr><td><p>container</p></td><td><p><a href=\"https://meesho.pagerduty.com/schedules/PUZUFZ2\">link</a></p></td><td><p><a href=\"https://meesho.pagerduty.com/schedules/PR06EJC\">link</a></p></td></tr></tbody></table><h2>Scaling</h2><p>Ringmaster references</p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"0e013186-9aa2-415b-8485-d475adbb9da4\"><tbody><tr><th><p><strong>service</strong></p></th><th><p><strong>link</strong></p></th></tr><tr><td><p>supplier_platform_container</p></td><td><p><a href=\"https://ringmaster.meeshogcp.in/applications/prd-supplier-platform-container\">link</a></p></td></tr><tr><td><p>supplier_platform_cataloging</p></td><td><p><a href=\"https://ringmaster.meeshogcp.in/applications/prd-supplier-platform-cataloging\">link</a></p></td></tr><tr><td><p>supplier_platform_growth</p></td><td><p><a href=\"https://ringmaster.meeshogcp.in/applications/prd-supplier-platform-growth\">link</a></p></td></tr><tr><td><p>supplier_platform_insights</p></td><td><p><a href=\"https://ringmaster.meeshogcp.in/applications/prd-supplier-platform-insights\">link</a></p></td></tr></tbody></table><p /><p />",
        "representation": "storage",
        "word_count": 178
      },
      "version": {
        "number": 8,
        "when": "2024-09-24T13:05:11.929Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3606282242",
      "title": "MLP Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3606282242",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<p><strong>Issue: Models not able to be fetched from gcs bucket </strong></p><p>we have to bind the sa of gcp with kubernetes service account  </p><p>sample:<br />gcloud iam service-accounts add-iam-policy-binding&nbsp; <a href=\"mailto:sa-dsci-model-inference-servic@meesho-datascience-prd-0622.iam.gserviceaccount.com\">sa-dsci-model-inference-servic@meesho-datascience-prd-0622.iam.gserviceaccount.com</a> \\</p><p>&nbsp; &nbsp; --role roles/iam.workloadIdentityUser \\ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p>&nbsp; &nbsp; --member &quot;serviceAccount:meesho-datascience-prd-0622.svc.id.goog[prd-model-inference-service-search-ad-gpu/prd-model-inference-service-search-ad-gpu]&quot;<br /><br /></p><p><strong>Issue:</strong> <strong>Mis services onboarding can not be onboarded  via  ringmaster .</strong></p><p><strong>resolution: </strong>have created a job to onboard the svc quickly<br />just in case new deployment is required at the sale time use this job to onboard the service for model inference : <br /><a href=\"https://jenkins-prd.meeshogcp.in/job/mis-onboard/\">https://jenkins-prd.meeshogcp.in/job/mis-onboard/</a><br /><br /><br /></p>",
        "representation": "storage",
        "word_count": 95
      },
      "version": {
        "number": 1,
        "when": "2024-09-18T11:18:43.226Z",
        "by": "Shivam Gupta1"
      },
      "labels": []
    },
    {
      "id": "3605561355",
      "title": "KEDA Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605561355",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<hr /><h4><strong>1. Issue: Service Not Scaling Up Despite Threshold Breach</strong></h4><p><strong>Issue Reported:</strong></p><ul><li><p>The service crosses the scaling threshold but does not scale up immediately.</p></li><li><p>Metrics fluctuate frequently, causing KEDA to not have enough time to trigger a scaling action.</p></li></ul><p><strong>Root Cause:</strong></p><ul><li><p>Metrics fall back to normal levels too quickly, not allowing KEDA to initiate the scale-up process.</p></li></ul><p><strong>Resolution:</strong></p><ul><li><p>Adjust the metric collection window or threshold to smooth out spikes in the data. This can help reduce false positives or negatives in the scaling mechanism.</p></li><li><p>Add a delay to the downscale threshold to avoid rapid changes.<br />Example:</p><ul><li><p>Modify the <code>cooldownPeriod</code> in the <code>ScaledObject</code> definition to give KEDA more time to stabilize metrics before deciding to scale.<br /><code>cooldownPeriod: 300</code></p></li></ul></li></ul><p /><hr /><p /><h4><strong>2. Issue: 503 Errors in Canary Due to Delayed Scaling</strong></h4><p><strong>Issue Reported:</strong></p><ul><li><p>503 errors are observed during a canary deployment phase.</p></li><li><p>The HPA is delayed in bringing up new pods, causing the service to be temporarily unavailable.</p></li></ul><p><strong>Root Cause:</strong></p><ul><li><p>The Horizontal Pod Autoscaler takes time to respond and spin up new pods. The canary analysis interval is too short for the HPA to effectively adjust.</p></li></ul><p /><p /><p><strong>Resolution:</strong></p><ul><li><p>Increase the canary analysis interval to allow enough time for the autoscaler to react to the changing load.<br />Example:</p><ul><li><p>Set the analysis interval to 120 seconds to provide sufficient time for the new pods to spin up before the analysis evaluates success.</p></li></ul></li></ul><p /><p><code>canaryAnalysis:</code></p><p><code>&nbsp;&nbsp;interval: 120s</code></p><p /><hr /><p /><p /><p /><p /><p />",
        "representation": "storage",
        "word_count": 221
      },
      "version": {
        "number": 2,
        "when": "2024-09-18T07:40:48.791Z",
        "by": "Yash Nandwana"
      },
      "labels": []
    },
    {
      "id": "3605430289",
      "title": "NodePool runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605430289",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<p><strong>Issue: GCE Out of Resources</strong></p><p>While pods are being scheduled on new nodes, we occasionally encounter the error message <code>ZONE_RESOURCE_POOL_EXHAUSTED</code>. This indicates that the resource pool in the current zone is depleted.</p><p><strong>Steps to Resolve the Issue:</strong></p><ol start=\"1\"><li><p><strong>Check the nodeSelector</strong>:</p><ul><li><p>Inspect the <code>nodeSelector</code> on the deployment/pod to see which node pool the service is using.</p></li></ul></li><li><p><strong>Enable Additional Zones</strong>:</p><ul><li><p>Open the GCP console / terraform-gcp-infra  repo and navigate to the node pool .</p></li><li><p>If only Zone A is enabled, enable additional zones such as Zone B and Zone C to increase capacity.</p></li></ul></li><li><p><strong>Create a New Node Pool</strong> (if capacity is exhausted in all zones):</p><ul><li><p>If capacity is unavailable across all enabled zones, create a new node pool with the same taints and labels as the existing one.</p></li><li><p>Consider adding a different machine type in the new node pool to better meet the resource demands.</p></li></ul></li><li><p><strong>Rescue-Nodepool: </strong><br />we are having one rescue node pool with taints ( <strong>sale-rescue</strong> ) which will spin up<strong> c3-standard-22</strong> instance types. We can move to this node pool incase any specific instance type not available in any of the zones.<br />This node pool is added in <strong>Demand</strong> , <strong>Supply</strong> , <strong>DataEngg</strong> and <strong>Datascience</strong> and <strong>Central</strong> BUs<br /></p></li></ol><h3>Issue: Optimizing Node Allocation to a Single Zone for Cost Savings (this activity should be done in night time)</h3><p>To minimize network costs, the goal is to consolidate nodes into a single zone once the capacity in <strong>Zone A</strong> has been updated. The following steps outline the process:</p><ol start=\"1\"><li><p><strong>List Nodes in Other Zones</strong>:</p><ul><li><p>Identify and list all nodes that are currently part of <strong>Zones B and C</strong>.</p></li></ul></li><li><p><strong>Pre-scale Nodes in Zone A</strong>:</p><ul><li><p>Increase the node capacity in <strong>Zone A</strong> by pre-scaling the number of nodes as needed.</p></li><li><p>Disable the autoscaling in the node pool.</p></li></ul></li><li><p><strong>Cordon the Nodes in Other Zones</strong>:</p><ul><li><p>Mark the nodes in <strong>Zones B and C</strong> as unschedulable by cordoning them. This ensures that no new pods are scheduled on these nodes.</p></li></ul></li><li><p><strong>Drain the Nodes</strong>:</p><ul><li><p>Safely drain the cordoned nodes to migrate running workloads to nodes in <strong>Zone A</strong>. This process will ensure that workloads are rescheduled without disruption.</p></li></ul></li><li><p><strong>Disable Zones B and C</strong>:</p><ul><li><p>Once all nodes in <strong>Zones B and C</strong> have been successfully drained, we can disable these zones to prevent any further resource allocation, thus achieving the goal of reducing network costs.</p></li></ul></li></ol><p />",
        "representation": "storage",
        "word_count": 366
      },
      "version": {
        "number": 5,
        "when": "2024-09-23T10:38:45.837Z",
        "by": "Pavan Kumar Reddy M"
      },
      "labels": []
    },
    {
      "id": "3605266537",
      "title": "GKE Multi-Cluster Services (MCS) Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605266537",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<p /><h4><strong>1. Issue: Multiple Connection Timeouts Across Services</strong></h4><p><strong>Symptoms:</strong></p><ul><li><p>Multiple services are experiencing connection timeouts.</p></li><li><p>These timeouts are occurring across almost every service within the GKE MCS architecture.</p></li></ul><p><strong>Root Cause:</strong></p><ul><li><p>A race condition happens when a pod's IP changes. MCS processes these IP changes to update the service routing across clusters.</p></li><li><p>In the event of an error during this process, MCS temporarily removes all retrieved IPs from the system, causing a network blip as it tries to fetch and update the IPs again.</p></li></ul><p><strong>Resolution:</strong></p><ul><li><p>Reach out to Google Support team. Keep mcs-importer logs handy.</p></li><li><p>We can find MCS logs over <a href=\"https://0b6ea1325493466faf2f9be526283f5a.psc.asia-southeast1.gcp.elastic-cloud.com/s/devopssre/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-15m,to:now))&amp;_a=(columns:!(),filters:!(),hideChart:!f,index:a97e1a9e-72ea-455b-924e-a9a58f1c3c50,interval:auto,query:(language:kuery,query:'application_name%20:%22gke-mcs%22%20and%20host:%20!'supply!'%20'),sort:!(!('@timestamp',desc)))\">ES</a>.</p></li><li><p>GCP team may ask for a faulty node sos report in some cases, follow the <a href=\"https://cloud.google.com/container-optimized-os/docs/how-to/sosreport\">steps</a> to get the same.</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"wide\" ac:original-height=\"853\" ac:original-width=\"1600\" ac:custom-width=\"true\" ac:width=\"862\" ac:src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe0Wcv8egSGK65w4XxJg_lQ2VhFOZSglH8H54d_lCrDC1IjcvPm3NVjqSv_ofPGV7WvOwdNuI-dRfVFq9ZSkoH7rKJhf7F2vDiP-I4fFblNdO2DjzK-gXq6oCaBAKaBQDCcSq0dnWuyfT_-c-nc-KbQCYk?key=3dBnc4c-tCPhmV5gU7ardw\"><ri:url ri:value=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe0Wcv8egSGK65w4XxJg_lQ2VhFOZSglH8H54d_lCrDC1IjcvPm3NVjqSv_ofPGV7WvOwdNuI-dRfVFq9ZSkoH7rKJhf7F2vDiP-I4fFblNdO2DjzK-gXq6oCaBAKaBQDCcSq0dnWuyfT_-c-nc-KbQCYk?key=3dBnc4c-tCPhmV5gU7ardw\" /></ac:image>",
        "representation": "storage",
        "word_count": 122
      },
      "version": {
        "number": 2,
        "when": "2024-09-18T08:20:29.069Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3605102604",
      "title": "RingMaster - Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3605102604",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<p><strong>Downscaling Freeze</strong> - This will first find the highest number of desired pods for the service in last 48 hours and then will update the minimum count to the same. </p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"279\" ac:original-width=\"1434\" ac:custom-width=\"true\" ac:alt=\"image-20240918-053315.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240918-053315.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Bulk Min Max Update</strong> - One of the most used feature for sale. It can help update min/max of multiple applications in one go. After things are updated here: as_min and as_max in deployment yaml will go down in precedence, and control will go to values_properties in devops-helm-charts repo.</p><p>(<span style=\"color: rgb(255,153,31);\">This action is supported only for superadmin users</span>).</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"365\" ac:original-width=\"1454\" ac:custom-width=\"true\" ac:alt=\"image-20240918-053632.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240918-053632.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Min Max Update -</strong> For one particular applications ( opposite of the above feature ), this api can be used. This is update min/max of desired applications and control will go to values.yaml and values_properties.yaml from depolyment.yaml</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"211\" ac:original-width=\"715\" ac:custom-width=\"true\" ac:alt=\"image-20240918-073100.png\" ac:width=\"715\"><ri:attachment ri:filename=\"image-20240918-073100.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Fetch secrets and Restart canary- </strong>This will fetch secrets and restart the canary object. The new pods of  canary will be created from the new secrets fetched from vault. Depending in the analysis result, it will be proceed accordingly.</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"111\" ac:original-width=\"746\" ac:custom-width=\"true\" ac:alt=\"image-20240918-073523.png\" ac:width=\"746\"><ri:attachment ri:filename=\"image-20240918-073523.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Zookeeper Dynamic configs: </strong>This allows you to view and compare differences between the GitHub configurations and the current Zookeeper settings.(this will work if config-as-code is onboarded)</p><p>path: repo_name/configs/module_name/application-dyn-env.yml</p><p>Other features: CPU based scaling change, Scheduled Autoscaling which can also be used as per need. </p>",
        "representation": "storage",
        "word_count": 248
      },
      "version": {
        "number": 1,
        "when": "2024-09-18T07:53:44.194Z",
        "by": "Rahul Kumar"
      },
      "labels": []
    },
    {
      "id": "3584196660",
      "title": "JIT Oncall Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3584196660",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p /><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"2e10f6b7-e4c4-41e9-aa1f-454341697f17\" ac:macro-id=\"9c4c38d5-aebf-4c26-8421-fd1b4bcb7f65\"><ac:parameter ac:name=\"style\">none</ac:parameter></ac:structured-macro><h2><strong><span style=\"color: rgb(191,38,0);\">Steps to be followed for launching any new listing in JIT</span></strong></h2><p><strong>Step 1: Mark Existing Listing Batches as Invalid</strong></p><ul><li><p><strong>Time to Execute:</strong> 4:50 PM</p></li></ul><p><strong>Action:</strong> Update the <code>product_listing_batches</code> table to mark the existing batches as invalid. Use the following query and replace the sample <code>product_listing_id</code> values with the IDs of the listings to be launched.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"bd81a39e-325b-4d38-9f47-9eb3d1294dd9\"><ac:plain-text-body><![CDATA[update product_listing_batches\nset valid = 0\nwhere product_listing_id in (40252, 40253 , ...)]]></ac:plain-text-body></ac:structured-macro><hr /><p><strong><ac:inline-comment-marker ac:ref=\"d57fd7ed-6330-47b7-a220-0847c3b9e111\">Step 2: Pre-Picklist Generation Check for </ac:inline-comment-marker></strong><code><ac:inline-comment-marker ac:ref=\"d57fd7ed-6330-47b7-a220-0847c3b9e111\">is_jit</ac:inline-comment-marker></code><ac:inline-comment-marker ac:ref=\"d57fd7ed-6330-47b7-a220-0847c3b9e111\"> Flag</ac:inline-comment-marker></p><p><strong>Important:</strong> Before picklist generation, ensure that all listings to be launched have the <code>is_jit</code> flag set to <code>false</code>.</p><hr /><p><strong><ac:inline-comment-marker ac:ref=\"115e3b55-5d87-41f5-9732-98d2351f31b9\">Step 3: Post-Picklist Generation - Mark </ac:inline-comment-marker></strong><code><ac:inline-comment-marker ac:ref=\"115e3b55-5d87-41f5-9732-98d2351f31b9\">is_jit</ac:inline-comment-marker></code><ac:inline-comment-marker ac:ref=\"115e3b55-5d87-41f5-9732-98d2351f31b9\"> Flag as </ac:inline-comment-marker><code><ac:inline-comment-marker ac:ref=\"115e3b55-5d87-41f5-9732-98d2351f31b9\">true</ac:inline-comment-marker></code></p><p><strong>Trigger Event:</strong> Upon receiving the &quot;Good To Go&quot; message in the <code>#grocery-python-picklist-alert</code> channel (typically around 5:45 PM, but could be delayed), execute the following actions:</p><hr /><p><strong>Action:</strong> Once picklist generation has occurred, update the <code>customer_order_details</code> table to mark sub-orders associated with the launched listings as <code>is_jit = true</code>.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d4f4247c-24bc-41c9-b746-02fb99a2f95d\"><ac:plain-text-body><![CDATA[update customer_order_details\nset is_jit = 1\nwhere product_listing_id in (24879, ***) \nand created_at > '2024-09-05 17:00:00' and is_jit = 0]]></ac:plain-text-body></ac:structured-macro><p /><h2><strong><span style=\"color: rgb(191,38,0);\">JIT Crons</span></strong></h2><p>The code for all these crons is present under <code>com/farmiso/crons/jit/JitCron.java</code> . </p><ol start=\"1\"><li><p><code>rtvRTOExpectancyCron</code></p><ol start=\"1\"><li><p>This cron creates rtv expectancy for any rto received against jit listings. <br />Replace date in the curl with the date we want to create expectancy for.<br />Curl : </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"b88faa63-78f0-494a-b423-1fdc6f3a0743\"><ac:plain-text-body><![CDATA[curl --location 'farmiso-backend.prd.meesho.int/farmiso/api/1.0/cron/trigger/rtv-rto-expectancy' \\\n--header 'Authorization: Token q2O74ruF5DdKLOhsBVtO' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"date\":\"2024-09-06\"\n}']]></ac:plain-text-body></ac:structured-macro></li></ol></li><li><p><code>rtvGRNDiscardExpectancyCron</code></p><ol start=\"1\"><li><p>This cron create rtv expectancy for anythings that falls under unresolved grn_issues hence considered as grn discard.</p></li><li><p>curl <br />Replace date in the curl with the date we want to create expectancy for. </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"e10ece80-f050-43e7-b3e3-a96c1f52f9de\"><ac:plain-text-body><![CDATA[curl --location 'localhost:8094/farmiso/api/1.0/cron/trigger/rtv-grn-discard-expectancy' \\\n--header 'Authorization: Token Test123' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"date\":\"2024-08-27\"\n}']]></ac:plain-text-body></ac:structured-macro></li></ol></li><li><p><code>grnExpectancyCron</code></p><ol start=\"1\"><li><p>This cron runs at 5:35pm daily to populate data in jit_grn_expectancy present in appsheet_nagpur db.</p></li><li><p>curl<br />Replace date with today&rsquo;s date.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"a944c065-3ba1-494e-b034-4cab1e21c180\"><ac:plain-text-body><![CDATA[curl --location 'http://farmiso-backend.prd.meesho.int/farmiso/api/1.0/cron/trigger/grn-expectancy' \\\n--header 'Authorization: Token q2O74ruF5DdKLOhsBVtO' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"date\":\"2024-09-07\"\n}']]></ac:plain-text-body></ac:structured-macro></li></ol></li><li><p><code>updateFMLPickUpQuantity</code></p><ol start=\"1\"><li><p>This cron runs at 5:30pm daily .</p></li><li><p>For jit listings, it calculates the quantity to be picked from sellers on a particular day. This data flows into the FML app for <code>FWD_PICKUP_DRIVER</code></p></li><li><p>curl : </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"ccbbe6f3-0bd4-4d24-92b3-0d5ae3171dbe\"><ac:plain-text-body><![CDATA[curl --location 'http://farmiso-backend.prd.meesho.int//farmiso/api/1.0//cron/trigger/fml-pick-up-quantity-update' \\\n--header 'Authorization: Token q2O74ruF5DdKLOhsBVtO' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"date\":\"2024-08-31\"\n}']]></ac:plain-text-body></ac:structured-macro></li></ol></li><li><p><code>updateBatchesForPurchaseRequests</code></p><ol start=\"1\"><li><p>This cron runs at 5pm daily.</p></li><li><p>This is used to deactivate the old batches and activate new ones.</p></li><li><p>curl  </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"232024e1-c58f-4b37-9de4-17accfd069f8\"><ac:plain-text-body><![CDATA[curl --location 'http://farmiso-backend.prd.meesho.int/farmiso/api/1.0/cron/trigger/jit//listing/batches' \\\n--header 'Authorization: Token q2O74ruF5DdKLOhsBVtO' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"date\":\"2024-08-31\"\n}']]></ac:plain-text-body></ac:structured-macro></li></ol></li></ol><h2><strong><span style=\"color: rgb(191,38,0);\">Re-run picklist code in case of failures</span></strong></h2><p>Involve <ac:link><ri:user ri:account-id=\"62c2a9082c528400c9b56cd5\" ri:local-id=\"94e1281f-23c8-44dc-aa37-9a0d725eae71\" /></ac:link> or <ac:link><ri:user ri:account-id=\"5da9a99ca539480c3b2ecf4c\" ri:local-id=\"43976b2f-95d5-4ab0-ab7b-343ff46519bb\" /></ac:link> the first time you do it.</p><p>Step 1:  [VVIMP] Ask @ratul.saha to stop so and ap uploads</p><p>Step 2 : [VVIMP]  Ask the warehouse team (Darshan hy, Harshal Vaidya, Sumegh) to stop picking in warehouse.</p><p>Step 3: Delete data against the given <span style=\"color: rgb(255,86,48);\">warehouse-processing-date</span> from the below tables.<br />             </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"7a00439b-0d3a-4be0-91b9-88d1d6fccdeb\"><ac:plain-text-body><![CDATA[picklist_data\npicklists\nchief_picking\nchief_picking_child\nchief_picklist_customer_consolidated_data\nchief_picklist_customer_wave_wise_data\nchief_to_be_dispatched\njit_chief_sku_drop_list_details\nchief_receiveing_crates]]></ac:plain-text-body></ac:structured-macro><p>Step 4 : Fix things</p><p>Step 5 : Rerun schedulers</p><h1><strong><span style=\"color: rgb(255,86,48);\">Create Driver seller mapping for Today</span></strong></h1><p /><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"5f635bcc-1bac-4629-b1fc-a62dea68cb8c\"><ac:plain-text-body><![CDATA[curl --location 'farmiso-backend-worker.prd.meesho.int/farmiso/admin/api/1.0/jit/driver-seller-mapping/bulk-create' \\\n--header 'Authorization: Token rdezq0UbwB' \\\n--header 'facility_id: 42' \\\n--form 'file=@\"/Users/lakshitaagarwal/Downloads/driver_seller_mapping_CSV\"']]></ac:plain-text-body></ac:structured-macro><p>In this upload the csv file with driver seller mapping for a day, refer this format</p><p class=\"media-group\"><ac:structured-macro ac:name=\"view-file\" ac:schema-version=\"1\" ac:macro-id=\"6c1b3d6b-488b-4c54-beb8-56f0a93eb738\"><ac:parameter ac:name=\"name\"><ri:attachment ri:filename=\"driver_seller_mapping_CSV\" ri:version-at-save=\"1\" /></ac:parameter></ac:structured-macro></p><p><strong><span style=\"color: rgb(255,86,48);\">Steps to verify the driver seller mapping and get the credentials for a day</span></strong></p><ul><li><p>Check <strong>driver_seller_mapping</strong> Table and check for date and flow type what is the mapping created.</p></li><li><p>Take the <strong>driver_id</strong> from <strong>driver_seller_mapping</strong> table and search this id in ops_executives table to verify the role added for driver</p></li><li><p>With the same <strong>driver_id</strong> check the <strong>ops_executive_login_credentials</strong> table and get the username and password</p></li><li><p>Note : If the warehouse_executive is added already in DB and there is no change then dont add this role in CSV as we are not creating any driver-seller mapping for this role type</p></li><li><p>Note : Ensure that while uploading CSV there are no spaces in phone_number or name.</p></li></ul><p /><h1><strong><span style=\"color: rgb(255,86,48);\">Purchase request creation</span></strong></h1><ul><li><p>Upload the purchase request using admin panel with this format</p><p><ac:structured-macro ac:name=\"view-file\" ac:schema-version=\"1\" ac:macro-id=\"ea10d8c6-f7a9-401b-a1f3-6cabc9fb7e37\"><ac:parameter ac:name=\"name\"><ri:attachment ri:filename=\"purchase_request_csv\" ri:version-at-save=\"1\" /></ac:parameter></ac:structured-macro> </p></li><li><p>For jit we are automatically creating the batches with valid as false so no need to upload any batches for jit purchase request.</p></li><li><p>To verify that batches are created , check the product_listing_batches table and verify with the purchase_request_id in the table</p></li><li><p>Cron runs at 5:00 pm to make these batches valid and the older batches false</p></li></ul>",
        "representation": "storage",
        "word_count": 747
      },
      "version": {
        "number": 8,
        "when": "2024-09-11T07:15:57.963Z",
        "by": "Lakshita Agarwal"
      },
      "labels": []
    },
    {
      "id": "3583049734",
      "title": "Quality Gate [SonarQube] Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3583049734",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>Currently we have a mandatory Quality Gate stage in our CI pipeline for maven. The condition for Quality gate to pass is that the Code Coverage should be &gt; 60%.<br />If this condition is not met, the Quality Gate stage will fail and the PR is set to block unless it is success. This rule on the PRs is set for most of the Java services.<br /><br />We are leveraging the autoCI implemented in ringmaster to make sure that a build is triggered every time a PR is raised against main/master/develop branches or whenever a commit is pushed to these PRs.<br /><br />Sometimes the github webhook might fail and the CI may not trigger, in this case we can re trigger it by <code>converting to draft</code> and reverting it using <code>Ready for review</code></p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"223\" ac:original-width=\"372\" ac:custom-width=\"true\" ac:width=\"322\"><ri:attachment ri:filename=\"Screenshot 2024-09-16 at 10.15.32 AM.png\" ri:version-at-save=\"2\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"482\" ac:original-width=\"923\" ac:custom-width=\"true\" ac:width=\"462\"><ri:attachment ri:filename=\"Screenshot 2024-09-16 at 10.15.19 AM.png\" ri:version-at-save=\"2\" /></ac:image><p>If the Quality Gate is getting timed out then it would be due to long queue of running tasks in sonarqube, as of now we have increased the timeout to 10 mins[Do not increase this further].<br />We can also check the cpu/mem stats of sonar if we are seeing any slowness and scale vertically if required. And also verify the pvc utilisation of the sonarqube and it&rsquo;s postgresql db.<br /><br />We can get to the jenkins build logs by clicking on the <strong>Details </strong>next to the status check in the PR.<br /></p><p><strong>Limitations</strong> we are facing currently are because we are using the OSS version of Sonarqube and we cannot increase the number of workers and also cannot enable parallel processing as these are features of enterprise edition.</p>",
        "representation": "storage",
        "word_count": 286
      },
      "version": {
        "number": 1,
        "when": "2024-09-18T07:47:34.401Z",
        "by": "Yeleswaram Venkata Sai Teja"
      },
      "labels": []
    },
    {
      "id": "3581116446",
      "title": "Rollout Service - Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3581116446",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<p><strong>Hosting details:</strong></p><ul><li><p>Clusters: Demand, Supply &amp; Farmiso</p></li><li><p>Namespace: rollout-service</p></li><li><p>No. of replicas: 1</p></li><li><p>Rollout Strategy: rollingUpdate</p></li><li><p>Repo: <a href=\"https://github.com/Meesho/rollout-service\" data-card-appearance=\"inline\">https://github.com/Meesho/rollout-service</a> </p></li></ul><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"c415e146-78a5-4603-93e5-f850a583fbf4\"><ac:rich-text-body><p>First check if the <code>rollourServiceEnabled</code> flag is set to true in the canary object <code>status</code></p><p>If yes, it is using rollout-service else it is using flagger</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"380\" ac:original-width=\"587\" ac:custom-width=\"true\" ac:alt=\"image-20240909-103352.png\" ac:width=\"587\"><ri:attachment ri:filename=\"image-20240909-103352.png\" ri:version-at-save=\"1\" /></ac:image></ac:rich-text-body></ac:structured-macro><h2><strong>Common Issues/use cases and their fixes:</strong></h2><ol start=\"1\"><li><p>What is analysis approval?</p><ol start=\"1\"><li><p>In rollout-service in addition to promotion approval there is a new step where you need to give approval for analysis to start.</p></li><li><p>Unless this approval is given, canary analysis won&rsquo;t start.</p></li></ol></li><li><p>Not able to approve analysis/promotion with canary-bot</p><ol start=\"1\"><li><p>Rollout-service uses a different set of apis for approval: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Approval via API\" ri:version-at-save=\"8\" /><ac:link-body>Rollout Service - Manual Approval</ac:link-body></ac:link> </p></li></ol></li><li><p>If the above curl command return an error saying <code>Workflow not found</code></p><ol start=\"1\"><li><p> Check the canary object and verify if it has reached this approval stage (Should be in WaitingAnalysis/WaitingPromotion).</p></li><li><p>Unless it has reached approval stage, analysis approval curl command won&rsquo;t work.</p></li></ol></li><li><p>How to do a <strong>Soft Rollback</strong>?</p><ol start=\"1\"><li><p>Sending false in any of the above approvals commands will result in soft rollback.</p></li></ol></li><li><p>Update to new version when analysis is happening</p><ol start=\"1\"><li><p> Just deploy the new version. Within 120 seconds it will start rolling back the previously deployed version and start rolling out the new version.</p></li></ol></li><li><p>How to do a <strong>Immediate Rollback?</strong></p><ol start=\"1\"><li><p>First set <code>skipAnalysis</code> to <strong>true </strong>in canary object and then update the canary deployment spec.</p></li></ol></li></ol><ac:structured-macro ac:name=\"note\" ac:schema-version=\"1\" ac:macro-id=\"4fbe8a71-3911-474d-9aca-b0db121f5403\"><ac:rich-text-body><p><strong>Do not do this unless absolutely necessary!!!!</strong><br />If the old version(image) is not backwards compatible with the rest of the spec, it could break things.</p></ac:rich-text-body></ac:structured-macro><ol start=\"7\"><li><p>To offboard a service from rolout-service and back to flagger</p><ol start=\"1\"><li><p>Go the the argo service page of that particular BU&rsquo;s flagger</p></li><li><p>There should be a config map called <code>rollout-service</code></p></li><li><p>There should be a config.json file with the list of service names</p></li><li><p>You can either set it to false or remove it entirely</p></li><li><p>Then restart flagger. </p></li><li><p>Once restart wait for 2-3 mins, check if the <code>rollourServiceEnabled</code> flag is set to <strong>false</strong> in the canary object <code>status</code></p></li></ol></li></ol>",
        "representation": "storage",
        "word_count": 328
      },
      "version": {
        "number": 1,
        "when": "2024-09-09T11:07:24.144Z",
        "by": "Vignesh Ganesan"
      },
      "labels": []
    },
    {
      "id": "3580690467",
      "title": "Flagger - Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3580690467",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<p><strong>Hosting details:</strong></p><ul><li><p>Clusters : All clusters</p></li><li><p>Namespace : flagger-&lt;bu&gt;-prd</p></li><li><p>No. of replicas : 1 </p></li><li><p>Rollout-strategy : Recreate</p></li><li><p>Repo: <a href=\"https://github.com/Meesho/devops-flagger\" data-card-appearance=\"inline\">https://github.com/Meesho/devops-flagger</a> </p></li></ul><p><strong>Common Issues,usecases and their fixes:</strong></p><p><u>Not able to promote or approve canary:</u></p><ul><li><p>Usually in slack the canary-bot times out after 10mins, so it&rsquo;s possible to approve it via slack, so follow the manual approval process, Referral doc: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Canary manual promotion - How our bot works\" ri:version-at-save=\"10\" /><ac:link-body>Canary manual promotion - How our bot works</ac:link-body></ac:link> </p></li><li><p>Once after you approve via slack or api call, it can take atmost 120sec to proceed further.</p></li></ul><p><u>503&rsquo;s In canary pods:</u></p><ul><li><p>This behaviour is usually observed in cases where analysis interval is set to 60sec, This happens due to the reason that Keda is taking more than 60sec to create the HPA and scale the canary deployment,<br /><strong>To Resolve</strong> : Increasing the analysis interval to 120sec will fix this. (giving enough time for keda to create HPA and scale the canary deployment before it starts serving the live traffic).<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"99\" ac:original-width=\"301\" ac:custom-width=\"true\" ac:width=\"301\"><ri:attachment ri:filename=\"Screenshot 2024-09-09 at 3.33.06 PM.png\" ri:version-at-save=\"1\" /></ac:image><p> </p></li></ul><p><u>Triggering Canary Manually:</u></p><ul><li><p>For any reason if we want to start the canary process manually, Restart of the canary deployment will retrigger the canary process again.</p></li></ul><p><u>Degraded App State due to canary: (Not an issue)</u></p><ul><li><p>Due to canary the app health can show <strong>degraded</strong>.</p></li><li><p>This can occur due to 2 reasons:</p><ul><li><p>The analysis failed for the canary version (or)</p></li><li><p>The developer aborts the canary while it&rsquo;s asking for approval</p></li></ul></li><li><p>In this case the canary version is not promoted to primary</p></li><li><p>If we want to retrigger the canary for the same deployment then follow the above steps</p></li></ul><p><u>Rollback of Canary deployment:</u> </p><ul><li><p>If we want to rollback canary deployment in case of any issues, </p><ul><li><p>The first way we can do this, is by aborting the canary process</p></li><li><p>But if u have already promoted then it&rsquo;s not possible to directly rollback, in this case we need to update the spec in the canary deployment to the older version and then usual canary process starts, and promotes it to prod</p></li><li><p> <strong>If we want Immediate rollback </strong>, First we need to modify skip-analysis as <strong>true </strong> in canary CRD, and then update the deployment to the intended version.<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"79\" ac:original-width=\"246\" ac:custom-width=\"true\" ac:width=\"246\"><ri:attachment ri:filename=\"Screenshot 2024-09-09 at 3.57.02 PM.png\" ri:version-at-save=\"1\" /></ac:image></li></ul></li></ul><ac:structured-macro ac:name=\"note\" ac:schema-version=\"1\" ac:macro-id=\"4394a77c-1c06-4dfc-89c4-fe0964e9afb6\"><ac:rich-text-body><p><strong>Do not do this unless absolutely necessary!!!!</strong><br />If the old version(image) is not backwards compatible with the rest of the spec, it could break things.</p></ac:rich-text-body></ac:structured-macro><p /><p /><p /><p /><p />",
        "representation": "storage",
        "word_count": 401
      },
      "version": {
        "number": 2,
        "when": "2024-09-09T11:13:31.031Z",
        "by": "Vignesh Ganesan"
      },
      "labels": []
    },
    {
      "id": "3578331150",
      "title": "CDN fallback runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3578331150",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h2>Objective -</h2><p>To fallback to fallback CDN infrastructure for <a href=\"http://www.meesho.com\">www.meesho.com</a>, <a href=\"http://suppliER.meesho.com\">supplier.meesho.com</a> and <a href=\"http://prod.meeshoapi.com\">prod.meeshoapi.com</a> </p><h2>CDN Infra details -</h2><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"bd139047-3392-4efd-8573-f90364bfa9ca\"><tbody><tr><th><p><strong>Hostname</strong></p></th><th><p><strong>Primary CDN(edge DNS)</strong></p></th><th><p><strong>Fallback CDN(edge DNS)</strong></p></th></tr><tr><td><p>www.meesho.com</p></td><td><p>Akamai(<a href=\"http://www.meesho.com.edgekey.net\">www.meesho.com.edgekey.net</a>)</p></td><td><p>Cloudflare(<a href=\"http://www.meesho.com.cdn.cloudflare.net\">www.meesho.com.cdn.cloudflare.net</a>)</p></td></tr><tr><td><p>supplier.meesho.com</p></td><td><p>Cloudflare(<a href=\"http://supplier.meesho.com.cdn.cloudflare.net\">supplier.meesho.com.cdn.cloudflare.net</a>)</p></td><td><p>Akamai(<a href=\"http://supplier.meesho.com.edgekey.net\">supplier.meesho.com.edgekey.net</a>)</p></td></tr><tr><td><p>prod.meeshoapi.com</p></td><td><p>Akamai(<a href=\"http://prod.meeshoapi.com.edgesuite.net\">prod.meeshoapi.com.edgesuite.net</a>)</p></td><td><p>GCP(<a href=\"http://ext-lb-cntr-cndvs-cdn-prd.meeshogcp.in\">ext-lb-cntr-cndvs-cdn-prd.meeshogcp.in</a>)</p></td></tr></tbody></table><p>1% traffic is always running on the fallback CDN to keep the cache warm (except for <a href=\"http://www.meesho.com\">www.meesho.com</a>)</p><h2>How to Fallback -</h2><p>In case of issues with primary CDN&rsquo;s, we need to update the Weighted round robin policy DNS entry to move 100% weight to the fallback CDN&rsquo;s edge hostname.</p><p /><p>For <a href=\"http://www.meesho.com\">www.meesho.com</a>, we should migrate the traffic with weighted routing only after disabling the Account protection, not doing so will result in a lot of false positives and blocks</p><p />",
        "representation": "storage",
        "word_count": 110
      },
      "version": {
        "number": 3,
        "when": "2024-09-26T10:20:42.501Z",
        "by": "Amul P Kesrani"
      },
      "labels": []
    },
    {
      "id": "3562864670",
      "title": "Runbook: Cost Dashboard - V2.0",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3562864670",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<h3>Filters:</h3><ac:adf-extension><ac:adf-node type=\"panel\"><ac:adf-attribute key=\"panel-type\">note</ac:adf-attribute><ac:adf-content><p>Applying these filters specified below will enhance the entire report, so it's recommended to use them for broader filtering.</p></ac:adf-content></ac:adf-node><ac:adf-fallback><div class=\"panel conf-macro output-block\" style=\"background-color: rgb(234,230,255);border-color: rgb(153,141,217);border-width: 1.0px;\"><div class=\"panelContent\" style=\"background-color: rgb(234,230,255);\">\n<p>Applying these filters specified below will enhance the entire report, so it's recommended to use them for broader filtering.</p>\n</div></div></ac:adf-fallback></ac:adf-extension><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"56\" ac:original-width=\"1172\" ac:custom-width=\"true\" ac:alt=\"image-20240829-161026.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240829-161026.png\" ri:version-at-save=\"1\" /></ac:image><p>The dashboard offers a range of filters designed to enable users to refine data to a granular level. These filters helps to select specific criteria, such as directors, teams, date ranges, projects, services, SKUs, and Kubernetes namespaces, allowing for precise control over the displayed data.</p><h3><strong>Leaders Cost:</strong></h3><ul><li><p>By default the cost here will be shown for last 7 days. We can modify the date range filter to specifc dates if needed. </p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"416\" ac:original-width=\"1174\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2024-08-29 at 9.50.19 PM.png\" ri:version-at-save=\"1\" /></ac:image><h3><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":thinking:\" ac:emoji-id=\"1f914\" ac:emoji-fallback=\"🤔\" /> <strong>Thinking How can we drill down and find the granular cost splits:</strong></h3><p>We are glad, that now we have a solution to do it. As pointed above in the image, Once we click on the + icon we can drill down the costs to granular level.</p><p><strong>Hierarchy Followed:</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"3ebe1148-1118-4437-8ab7-8610a332586f\"><ac:plain-text-body><![CDATA[DOE --> GCP Project --> GCP Service --> GCP SKU --> Team --> K8S Namespace]]></ac:plain-text-body></ac:structured-macro><p /><p><u>PFB Sample Tabular View for Reference: </u></p><ac:structured-macro ac:name=\"panel\" ac:schema-version=\"1\" ac:macro-id=\"d850a5a1-9922-425c-801b-a24e307f737c\"><ac:parameter ac:name=\"panelIcon\">:rainbow:</ac:parameter><ac:parameter ac:name=\"panelIconId\">1f308</ac:parameter><ac:parameter ac:name=\"panelIconText\">🌈</ac:parameter><ac:parameter ac:name=\"bgColor\">#E6FCFF</ac:parameter><ac:rich-text-body><p>You&rsquo;ll also get SubTotal and GrandTotal for each day in the tabular view</p></ac:rich-text-body></ac:structured-macro><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"297\" ac:original-width=\"1146\" ac:custom-width=\"true\" ac:alt=\"image-20240829-163928.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240829-163928.png\" ri:version-at-save=\"1\" /></ac:image><h3><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":hugging:\" ac:emoji-id=\"1f917\" ac:emoji-fallback=\"🤗\" /> Graphical View with Drill Down Option:</h3><p>We now also have another way to visualize the costs and drill down to granular level. <strong>The same hierarchy which is mentioned above is followed across all the panels in the dashboard.</strong></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"604\" ac:original-width=\"500\" ac:custom-width=\"true\" ac:width=\"280\"><ri:attachment ri:filename=\"Screenshot 2024-08-29 at 11.47.42 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>By Applying filters and Clicking the drill down button it&rsquo;s possible to visualize cost at all granular levels, <span style=\"color: rgb(7,71,166);\">Example Usecase:</span> If i am <strong>XYZ</strong> Doe and in the above chart if wanted to view alone my granular costs, I need to first apply filters at Report level and then drill down, applying filters at report level also helps me in correlation between different panels.</p><h3>Comparison between Dates:</h3><ac:structured-macro ac:name=\"note\" ac:schema-version=\"1\" ac:macro-id=\"d24acf06-9413-4da1-a13a-e74cf7ab2f51\"><ac:rich-text-body><p>By default looker studio doesn&rsquo;t support comparison, so comparison view between 2 different custom dates in a single panel is not possible</p></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"tip\" ac:schema-version=\"1\" ac:macro-id=\"65a5d3ce-3b7d-44c8-a0a7-092a6862c70f\"><ac:rich-text-body><p>But we made this comparison possible with two different panels by selecting 2 different dates for each panels which helps in easily identifying the difference between benchamarked date vs current date&rsquo;s diff.</p></ac:rich-text-body></ac:structured-macro><p>You can now select two different dates 1. Global level filter and 2. Panel Specific, which is marked by arrow below. PFB the comparison charts between Aug 22nd vs Aug 1st cost.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"648\" ac:original-width=\"1020\" ac:custom-width=\"true\" ac:width=\"746\"><ri:attachment ri:filename=\"Screenshot 2024-08-30 at 12.05.53 AM.png\" ri:version-at-save=\"1\" /></ac:image><p />",
        "representation": "storage",
        "word_count": 472
      },
      "version": {
        "number": 1,
        "when": "2024-08-29T18:42:32.272Z",
        "by": "Naveen Vellingiri"
      },
      "labels": []
    },
    {
      "id": "3485106236",
      "title": "Mcache: Runbooks",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3485106236",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "",
        "representation": "storage",
        "word_count": 0
      },
      "version": {
        "number": 1,
        "when": "2024-07-23T07:46:27.149Z",
        "by": "Bhawani Singh"
      },
      "labels": []
    },
    {
      "id": "3405152257",
      "title": "PDP Brands Widget RunBook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3405152257",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p /><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"ae48e8dd-844a-4999-a52a-e7e76adde43e\" ac:macro-id=\"a49b3fbe-c9a3-4923-8fb9-211c4b121f89\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">6</ac:parameter><ac:parameter ac:name=\"outline\">false</ac:parameter><ac:parameter ac:name=\"style\">none</ac:parameter><ac:parameter ac:name=\"type\">list</ac:parameter><ac:parameter ac:name=\"printable\">true</ac:parameter></ac:structured-macro><h2>Important Links -</h2><p>vault <a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/dplay/display-ads-brands?namespace=&amp;redirect_to=&amp;tab=&amp;type=&amp;version=&amp;wrapped_token=\">LINK</a></p><p>observability <a href=\"https://0b6ea1325493466faf2f9be526283f5a.psc.asia-southeast1.gcp.elastic-cloud.com/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-6h,to:now))&amp;_a=(columns:!(),filters:!(),index:'5e4fda77-086c-4a1c-8e38-69e060c35f7b',interval:auto,query:(language:kuery,query:'service_name%20:%20%22display-ads-server%22%20and%20not%20text%20:%20%22%2Factuator%2Fprometheus%22%20and%20text%20:%20%22java.lang.NullPointerException%20java.util.concurrent.CompletionException%22'),sort:!(!('@timestamp',desc)))\">LINK</a></p><p>Grafana Dashboard <a href=\"https://grafana-prd.meeshogcp.in/d/dfda2599-3af8-45ab-845b-9b3004fd0bb0/flow-wise-dashboard-2?orgId=1\">LINK</a></p><h2><br /><strong>Serving flow -</strong></h2><p>1.Check if valid sscat_id<br />2.Fetch pdpBrandsCatalogsResponse from cla external service<br />3.Fetch catalogAdvertiserCache, add AdvertiserId in the view, group by catalogs by advertiserId.<br />4.Fetch campaignAdvertiserCache, filter inactive catalogs.<br />5. Calculate ranking score of each Advertiser , rank the advertiser and get the top ranked advertiser.<br />6. Build response view by adding adsmetadata, brandDetails and feedFilters.<br /></p><h2><strong>Major Issues and resolution steps [Evolving list]:</strong></h2><h3><strong>Ads not getting served for a user</strong></h3><ul><li><p>Check if we are getting expected request on ad-server<br />check if sscat is whitelisted in bigtable</p></li><li><p>check if any issue in downstream cla service. SAMPLE CURL<br /></p></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"aa943074-1a27-48e0-8d6a-ab9a77767f6e\"><ac:plain-text-body><![CDATA[curl --location 'advertisement-pdp-widget.prd.meesho.int/api/v1/ads/brands/pdp/fetch' \\\n--header 'Authorization: hduza7tbhg' \\\n--header 'MEESHO-USER-CONTEXT: logged_in' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'MEESHO-USER-ID: 34166353' \\\n--header 'APP-VERSION-CODE: 478' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"session_id\": \"ad_session\",\n    \"position\": 0,\n    \"catalog_id\": 74426547,\n    \"metadata\": {\n        \"sscat_id\": 5558,\n        \"product_id\": 125154786\n    }\n}' \ncheck if any failure redis/mysql for catalogAdvertiserCache and campaignAdvertiserCache]]></ac:plain-text-body></ac:structured-macro><ul><li><p>check if not able to connect redis/mysql for catalogAdvertiserCache , campaignAdvertiserCache cache data.<br />check if MLP  api is failing. SAMPLE CURL<br /></p></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"ba2827f3-0864-49d4-b094-23c6b81e5284\"><ac:plain-text-body><![CDATA[curl --location 'pdp-iop-web.prd.meesho.int/api/v1/entities' \\\n--header 'Authorization: HrRRsCqQ203i1PVnL1TfNY0Tt9QW9SXzTRE1mEI6B4LhzSdVa2tFqyoDVvnyZV1i' \\\n--header 'MEESHO-USER-ID: 600522' \\\n--header 'MEESHO-USER-CONTEXT: logged_in' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"data\": {\n        \"parent_entity_type\": \"catalog\",\n        \"parent_entity_id\": 47474671,\n        \"tenants\": [\n            {\n                \"cursor\": null,\n                \"limit\": 10,\n                \"tenant_context\": \"ad\",\n                \"meta\": {\"ad_feed_type\":\"brands_widget\"}\n                \n            }\n        ]\n    }\n}'\nCheck if we have enough/required widgets after all filtering steps like dedupe, view cap.]]></ac:plain-text-body></ac:structured-macro><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"e03aa116-d112-4e69-bc3e-b0875108247a\"><ac:plain-text-body><![CDATA[curl --location 'pdp-iacg-web.prd.meesho.int/api/v1/similar-entities' \\\n--header 'MEESHO-USER-ID: 251078202' \\\n--header 'MEESHO-TENANT-CONTEXT: ad' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: qgU4zbxbN1YkvX41xBiK3Iba6ANOb3xVM9sJwg8xG7YOFf8TkwUHCs60mn3ihqO9' \\\n--data '{\n    \"data\": {\n        \"parent_entity_id\": 33613665,\n        \"parent_entity_type\": \"catalog\",\n        \"cgs\": [\n            {\n                \"cg_name\": \"entity-similarity-iacg\",\n                \"cg_version\": 2,\n                \"limit\": 100\n            }\n        ]\n    }\n}']]></ac:plain-text-body></ac:structured-macro><h3><strong>Potential 5xxs causing reasons</strong></h3><ul><li><p>Application configs missing/not up-to-date across different infra environments <a href=\"https://vault-prd.meeshogcp.in/ui/vault/secrets/meesho/show/prd/supl/dplay/display-ads-brands?namespace=&amp;redirect_to=&amp;tab=&amp;type=&amp;version=&amp;wrapped_token=\">LINK</a></p></li><li><p><br />Any bugs introduced in latest release causing exceptions like NPEs or IndexOutOfBound etc. Search for logs here <a href=\"https://0b6ea1325493466faf2f9be526283f5a.psc.asia-southeast1.gcp.elastic-cloud.com/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-6h,to:now))&amp;_a=(columns:!(),filters:!(),index:'5e4fda77-086c-4a1c-8e38-69e060c35f7b',interval:auto,query:(language:kuery,query:'service_name%20:%20%22display-ads-server%22%20and%20not%20text%20:%20%22%2Factuator%2Fprometheus%22%20and%20text%20:%20%22java.lang.NullPointerException%20java.util.concurrent.CompletionException%22'),sort:!(!('@timestamp',desc)))\">LINK</a></p></li></ul><p /><h3><strong>Potential circuit open (from WA) reasons</strong></h3><p>Increase in serving APIs latency leading to timeouts<br />Increase in non 2xxs count<br /></p>",
        "representation": "storage",
        "word_count": 344
      },
      "version": {
        "number": 1,
        "when": "2024-06-11T21:26:34.356Z",
        "by": "Paras Mehndiratta"
      },
      "labels": []
    },
    {
      "id": "3402236031",
      "title": "Update on Incidents and RCA Followup runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/SRE/pages/3402236031",
      "space": {
        "key": "SRE",
        "name": "SRE"
      },
      "content": {
        "body": "<p /><p>As part of your on-call duties, please closely monitor the #tech-incident-management channel to understand all incidents that occur during your on-call period. For each incident, gather detailed information to fully understand the issue, including:</p><ul><li><p>The incident owner</p></li><li><p>Affected services</p></li><li><p>Whether it involves a third-party issue</p></li></ul><p>By Monday morning, create a record for each incident in the <a href=\"https://docs.google.com/spreadsheets/d/16JC6dZUGo0vYvzadbp9EqpsnnSpOfXMFBy8DijPXm6o/edit?gid=0#gid=0\">RCA Backlogs</a> sheet. Include the owner&rsquo;s name, ETA for publishing the RCA, and a reason if the RCA does not need to be discussed centrally.</p><p>Additionally, ensure you:</p><ul><li><p>Add your name under the SRE POC column in the RCA Backlogs sheet for all incidents you are following up.</p></li><li><p>Follow up on each RCA to ensure it is published using Pulse and approved by the reviewer.</p></li></ul><p>If you do not receive a timely response from the incident owner, create a separate channel with the Manager and DOE. Follow up at least three times in this separate channel.</p><p>Continue to follow up based on the ETA provided by the owners to ensure we drive each RCA to closure.<br /><br />Along with above Oncalls should provide the detailed insight on all the incidents happened during there oncall tenure to entire team on Monday team standup.</p>",
        "representation": "storage",
        "word_count": 191
      },
      "version": {
        "number": 3,
        "when": "2024-06-17T08:23:20.073Z",
        "by": "Aditya Madvesh Nadig (Unlicensed)"
      },
      "labels": []
    },
    {
      "id": "3381068360",
      "title": "Runbook for  Onboarding Opstech on Shipsy:",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3381068360",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>Dev Changes<strong> : </strong><br /><strong>        </strong>ValmoLogisticsService:  Add new opstech in service : <a href=\"https://github.com/Meesho/ValmoLogisticsService/pull/148\">Sample PR</a> </p><p style=\"margin-left: 30.0px;\">Manifest Supply , Manifest Worker :</p><p style=\"margin-left: 60.0px;\">Add in configs:</p><p style=\"margin-left: 60.0px;\">Example :&nbsp;</p><p style=\"margin-left: 90.0px;\"><code>manifest_meesho_shipsy_carrier_map_shadowfax : shadowfax warehouse</code></p><p style=\"margin-left: 90.0px;\"><code>manifest_shipsy_carrier_account_map_shadowfax_domestic_surface :SHADOWFAX WAREHOUSE</code></p><h3>Create Hubs and Links in AutoDml : </h3><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"a39d5e5b-8122-451e-a0d6-a0ab3fae7543\"><ac:parameter ac:name=\"title\">Create Hubs</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"164f2b8c-df46-46c0-a80a-9306421901e0\"><ac:plain-text-body><![CDATA[curl --location 'http://localhost:8888/v1/upsert/hub' \\\n--header 'CLIENT_ID: route-aggregator' \\\n--header 'SECRET_ID: T25XQ64JkCM5LTa' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"requests\": [\n        {\n            \"request_id\": 1,\n            \"row_number\": 1,\n            \"code\": \"PKA\",\n            \"ops_tech\": {\n                \"carrier_name\": \"elasticrun\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"PKA\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"PKA\"\n        },\n        {\n            \"request_id\": 2,\n            \"row_number\": 2,\n            \"code\": \"LRR\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"LRR\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"LRR\"\n        },\n        {\n            \"request_id\": 3,\n            \"row_number\": 3,\n            \"code\": \"KLU\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"KLU\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"KLU\"\n        },\n        {\n            \"request_id\": 4,\n            \"row_number\": 4,\n            \"code\": \"PRJ\",\n            \"ops_tech\": {\n                \"carrier_name\": \"elasticrun\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"PRJ\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"PRJ\"\n        },\n        {\n            \"request_id\": 5,\n            \"row_number\": 5,\n            \"code\": \"SPS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"elasticrun\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"W1/SPS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"SPS\"\n        },\n        {\n            \"request_id\": 6,\n            \"row_number\": 6,\n            \"code\": \"FRS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"N1/FRS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"FRS\"\n        },\n        {\n            \"request_id\": 7,\n            \"row_number\": 7,\n            \"code\": \"LUS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"N1/LUS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"LUS\"\n        },\n        {\n            \"request_id\": 8,\n            \"row_number\": 8,\n            \"code\": \"AHS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"elasticrun\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"W1/AHS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"AHS\"\n        },\n        {\n            \"request_id\": 9,\n            \"row_number\": 9,\n            \"code\": \"FRS_CD\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"FRS\",\n            \"property\": \"CD\",\n            \"sunday_working\": true,\n            \"code_override\": \"FRS\"\n        },\n        {\n            \"request_id\": 10,\n            \"row_number\": 10,\n            \"code\": \"SHS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"elasticrun\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"SHS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"SHS\"\n        },\n        {\n            \"request_id\": 11,\n            \"row_number\": 11,\n            \"code\": \"NBJ\",\n            \"ops_tech\": {\n                \"carrier_name\": \"elasticrun\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"NBJ\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"NBJ\"\n        },\n        {\n            \"request_id\": 12,\n            \"row_number\": 12,\n            \"code\": \"BLR_Domlur\",\n            \"ops_tech\": {\n                \"carrier_name\": \"shipsy\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"BLR_Domlur\",\n            \"property\": \"LMDC\",\n            \"sunday_working\": true,\n            \"code_override\": \"BLR_Domlur\"\n        },\n        {\n            \"request_id\": 13,\n            \"row_number\": 13,\n            \"code\": \"SPS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"elasticrun\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"SPS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"SPS\"\n        },\n        {\n            \"request_id\": 14,\n            \"row_number\": 14,\n            \"code\": \"BLS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"fareye\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"BLS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"BLS\"\n        },\n        {\n            \"request_id\": 15,\n            \"row_number\": 15,\n            \"code\": \"HAR\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"HAR\",\n            \"property\": \"LMDC\",\n            \"sunday_working\": true,\n            \"code_override\": \"HAR\"\n        },\n        {\n            \"request_id\": 16,\n            \"row_number\": 16,\n            \"code\": \"PCSA\",\n            \"ops_tech\": {\n                \"carrier_name\": \"elasticrun\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"PCSA\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"PCSA\"\n        },\n        {\n            \"request_id\": 17,\n            \"row_number\": 17,\n            \"code\": \"FRR\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"FRR\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"FRR\"\n        },\n        {\n            \"request_id\": 17,\n            \"row_number\": 17,\n            \"code\": \"KLU\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"KLU\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"KLU\"\n        },\n        {\n            \"request_id\": 18,\n            \"row_number\": 18,\n            \"code\": \"VAS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"fareye\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"VAS\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"VAS\"\n        },\n        {\n            \"request_id\": 19,\n            \"row_number\": 19,\n            \"code\": \"BWS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"fareye\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"BWS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"BWS\"\n        },\n        {\n            \"request_id\": 20,\n            \"row_number\": 20,\n            \"code\": \"BLS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"fareye\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"BLS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"BLS\"\n        },\n        {\n            \"request_id\": 21,\n            \"row_number\": 21,\n            \"code\": \"NDN1\",\n            \"ops_tech\": {\n                \"carrier_name\": \"shipsy\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"NDN1\",\n            \"property\": \"LMDC\",\n            \"sunday_working\": true,\n            \"code_override\": \"NDN1\"\n        },\n        {\n            \"request_id\": 22,\n            \"row_number\": 22,\n            \"code\": \"CCD\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"CCD\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"CCD\"\n        },\n        {\n            \"request_id\": 23,\n            \"row_number\": 23,\n            \"code\": \"DSS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"DSS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"DSS\"\n        },\n        {\n            \"request_id\": 24,\n            \"row_number\": 24,\n            \"code\": \"GHS\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"GHS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"GHS\"\n        },\n        {\n            \"request_id\": 25,\n            \"row_number\": 25,\n            \"code\": \"TSK\",\n            \"ops_tech\": {\n                \"carrier_name\": \"loadshare\"\n            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"TSK\",\n            \"property\": \"LMDC\",\n            \"sunday_working\": true,\n            \"code_override\": \"TSK\"\n        },\n        {\n            \"request_id\": 26,\n            \"row_number\": 26,\n            \"code\": \"FSK\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"loadshare\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"FSK\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"FSK\"\n        },\n        {\n            \"request_id\": 27,\n            \"row_number\": 27,\n            \"code\": \"HGS\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"loadshare\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"HGS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"HGS\"\n        },\n        {\n            \"request_id\": 28,\n            \"row_number\": 28,\n            \"code\": \"LKS\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"elasticrun\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"LKS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"LKS\"\n        },\n        {\n            \"request_id\": 29,\n            \"row_number\": 29,\n            \"code\": \"SFXDLU\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"shadowfax\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"SFXDLU\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"SFXDLU\"\n        },\n        {\n            \"request_id\": 30,\n            \"row_number\": 30,\n            \"code\": \"GD_GONDA\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"shadowfax\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"GD_GONDA\",\n            \"property\": \"LMDC\",\n            \"sunday_working\": true,\n            \"code_override\": \"GD_GONDA\"\n        },\n        {\n            \"request_id\": 31,\n            \"row_number\": 31,\n            \"code\": \"BYD\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"loadshare\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"BYD\",\n            \"property\": \"FMH\",\n            \"sunday_working\": true,\n            \"code_override\": \"BYD\"\n        },\n        {\n            \"request_id\": 32,\n            \"row_number\": 32,\n            \"code\": \"BWS\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"fareye\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"BWS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"BWS\"\n        },\n        {\n            \"request_id\": 33,\n            \"row_number\": 33,\n            \"code\": \"KOS\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"loadshare\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"KOS\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"KOS\"\n        },\n        {\n            \"request_id\": 34,\n            \"row_number\": 34,\n            \"code\": \"SFXKOL\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"shadowfax\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"SFXKOL\",\n            \"property\": \"SC\",\n            \"sunday_working\": true,\n            \"code_override\": \"SFXKOL\"\n        },\n        {\n            \"request_id\": 35,\n            \"row_number\": 35,\n            \"code\": \"CCU_Belur\",\n            \"ops_tech\": {\n                           \"ops_tech_name\":\"shadowfax\",\n                           \"ops_tech_type\":\"carrier\",\n                           \"is_active\":true\n                            },\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": 1000,\n            \"active\": true,\n            \"label_code\": \"CCU_Belur\",\n            \"property\": \"LMDC\",\n            \"sunday_working\": true,\n            \"code_override\": \"CCU_Belur\"\n        }\n    ]\n}'\n\n]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"240ed162-e557-45b2-a286-051d4efdb454\"><ac:parameter ac:name=\"title\">Pincode Lm Mapping</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"602a52cc-0d6f-4e81-b0d9-1a5eab982c82\"><ac:plain-text-body><![CDATA[curl --location 'http://localhost:8888/v1/upsert/pincode-lm-mapping' \\\n--header 'CLIENT_ID: route-aggregator' \\\n--header 'SECRET_ID: T25XQ64JkCM5LTa' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"requests\": [\n        {\n            \"request_id\": 0,\n            \"row_number\": 1,\n            \"pincode\": \"271123\",\n            \"lm_hubs\":[\"GD_GONDA\"],\n            \"lm_pincode_capacities\":[1000]\n        }\n    ]\n}'\n\ncurl --location 'http://localhost:8888/v1/upsert/pincode-lm-mapping' \\\n--header 'CLIENT_ID: route-aggregator' \\\n--header 'SECRET_ID: T25XQ64JkCM5LTa' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"requests\": [\n        {\n            \"request_id\": 0,\n            \"row_number\": 1,\n            \"pincode\": \"711201\",\n            \"lm_hubs\":[\"CCU_Belur\"],\n            \"lm_pincode_capacities\":[1000]\n        }\n    ]\n}'\n]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"2d9e7f87-073f-40d1-a30b-50877d5e5524\"><ac:parameter ac:name=\"title\">Upsert LInk</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"8d7f3fef-26cf-4ff2-922a-2bf4215d7cb6\"><ac:plain-text-body><![CDATA[curl --location 'http://localhost:8888/v1/upsert/link' \\\n--header 'CLIENT_ID: route-aggregator' \\\n--header 'SECRET_ID: T25XQ64JkCM5LTa' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"requests\": [\n        {\n            \"request_id\": 1,\n            \"row_number\": 1,\n            \"source_hub_code\": \"PKA\",\n            \"dest_hub_code\": \"SPS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 2,\n            \"row_number\": 2,\n            \"source_hub_code\": \"LRR\",\n            \"dest_hub_code\": \"FRS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 3,\n            \"row_number\": 3,\n            \"source_hub_code\": \"KLU\",\n            \"dest_hub_code\": \"LUS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 4,\n            \"row_number\": 4,\n            \"source_hub_code\": \"PRJ\",\n            \"dest_hub_code\": \"AHS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 5,\n            \"row_number\": 5,\n            \"source_hub_code\": \"SPS\",\n            \"dest_hub_code\": \"BAN\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 6,\n            \"row_number\": 6,\n            \"source_hub_code\": \"FRS\",\n            \"dest_hub_code\": \"BAN\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 7,\n            \"row_number\": 7,\n            \"source_hub_code\": \"LUS\",\n            \"dest_hub_code\": \"FRS_CD\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 8,\n            \"row_number\": 8,\n            \"source_hub_code\": \"AHS\",\n            \"dest_hub_code\": \"SHS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 9,\n            \"row_number\": 9,\n            \"source_hub_code\": \"FRS_CD\",\n            \"dest_hub_code\": \"BAN\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 10,\n            \"row_number\": 10,\n            \"source_hub_code\": \"SHS\",\n            \"dest_hub_code\": \"BAN\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 11,\n            \"row_number\": 11,\n            \"source_hub_code\": \"BAN\",\n            \"dest_hub_code\": \"BLR_Domlur\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 12,\n            \"row_number\": 12,\n            \"source_hub_code\": \"PKA\",\n            \"dest_hub_code\": \"SPS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 13,\n            \"row_number\": 13,\n            \"source_hub_code\": \"SPS\",\n            \"dest_hub_code\": \"BLS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 14,\n            \"row_number\": 14,\n            \"source_hub_code\": \"BLS\",\n            \"dest_hub_code\": \"HAR\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n         {\n            \"request_id\": 15,\n            \"row_number\": 15,\n            \"source_hub_code\": \"PCSA\",\n            \"dest_hub_code\": \"SHS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 16,\n            \"row_number\": 16,\n            \"source_hub_code\": \"SHS\",\n            \"dest_hub_code\": \"HAR\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 17,\n            \"row_number\": 17,\n            \"source_hub_code\": \"LRR\",\n            \"dest_hub_code\": \"FRS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n          {\n            \"request_id\": 18,\n            \"row_number\": 18,\n            \"source_hub_code\": \"FRS\",\n            \"dest_hub_code\": \"BAN\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 19,\n            \"row_number\": 19,\n            \"source_hub_code\": \"PRJ\",\n            \"dest_hub_code\": \"SHS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 20,\n            \"row_number\": 20,\n            \"source_hub_code\": \"SHS\",\n            \"dest_hub_code\": \"NBJ\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        }\n        ,\n        {\n            \"request_id\": 21,\n            \"row_number\": 21,\n            \"source_hub_code\": \"NBJ\",\n            \"dest_hub_code\": \"BLR_Domlur\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 22,\n            \"row_number\": 22,\n            \"source_hub_code\": \"VAS\",\n            \"dest_hub_code\": \"BWS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 23,\n            \"row_number\": 23,\n            \"source_hub_code\": \"BWS\",\n            \"dest_hub_code\": \"BLS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 24,\n            \"row_number\": 24,\n            \"source_hub_code\": \"BLS\",\n            \"dest_hub_code\": \"NDN1\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 25,\n            \"row_number\": 25,\n            \"source_hub_code\": \"CCD\",\n            \"dest_hub_code\": \"DSS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 26,\n            \"row_number\": 26,\n            \"source_hub_code\": \"DSS\",\n            \"dest_hub_code\": \"GHS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 27,\n            \"row_number\": 27,\n            \"source_hub_code\": \"GHS\",\n            \"dest_hub_code\": \"TSK\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 28,\n            \"row_number\": 28,\n            \"source_hub_code\": \"FSK\",\n            \"dest_hub_code\": \"HGS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 29,\n            \"row_number\": 29,\n            \"source_hub_code\": \"HGS\",\n            \"dest_hub_code\": \"LKS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 30,\n            \"row_number\": 30,\n            \"source_hub_code\": \"LKS\",\n            \"dest_hub_code\": \"SFXDLU\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 31,\n            \"row_number\": 31,\n            \"source_hub_code\": \"SFXDLU\",\n            \"dest_hub_code\": \"GD_GONDA\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 32,\n            \"row_number\": 32,\n            \"source_hub_code\": \"BYD\",\n            \"dest_hub_code\": \"BWS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 33,\n            \"row_number\": 33,\n            \"source_hub_code\": \"BWS\",\n            \"dest_hub_code\": \"KOS\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 34,\n            \"row_number\": 34,\n            \"source_hub_code\": \"KOS\",\n            \"dest_hub_code\": \"SFXKOL\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        },\n        {\n            \"request_id\": 35,\n            \"row_number\": 35,\n            \"source_hub_code\": \"SFXKOL\",\n            \"dest_hub_code\": \"CCU_Belur\",\n            \"price\": 2,\n            \"tat\": 2,\n            \"capacity\": -1,\n            \"active\": true\n        }\n    ]\n}'\n]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"6bccb4e5-e774-4683-88ca-b38efdac1284\"><ac:parameter ac:name=\"title\">Create mm_mm_paths </ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"507bcc8e-b866-4103-b2a8-86690c9bb40c\"><ac:plain-text-body><![CDATA[FSK -> HGS -> LKS -> SFXDLU -> GD_GONDA\n\nFSK::HGS -> {\"id\":\"FSK::HGS\",\"srcMm\":\"FSK\",\"destMm\":\"HGS\",\"paths\":[{\"edges\":[\"FSK::HGS\"],\"nodes\":[\"FSK\",\"HGS\"],\"tat\":4.5}]}\n\n\nHGS::LKS ->{\"id\":\"HGS::LKS\",\"srcMm\":\"HGS\",\"destMm\":\"LKS\",\"paths\":[{\"edges\":[\"HGS::LKS\"],\"nodes\":[\"HGS\",\"LKS\"],\"tat\":4.5}]}\n\n\nLKS::SFXDLU ->{\"id\":\"LKS::SFXDLU\",\"srcMm\":\"LKS\",\"destMm\":\"SFXDLU\",\"paths\":[{\"edges\":[\"LKS::SFXDLU\"],\"nodes\":[\"LKS\",\"SFXDLU\"],\"tat\":4.5}]}\n\n\nSFXDLU::GD_GONDA ->{\"id\":\"SFXDLU::GD_GONDA\",\"srcMm\":\"SFXDLU\",\"destMm\":\"GD_GONDA\",\"paths\":[{\"edges\":[\"SFXDLU::GD_GONDA\"],\"nodes\":[\"SFXDLU\",\"GD_GONDA\"],\"tat\":4.5}]}\n\n\nHGS::SFXDLU -> {\"id\":\"HGS::SFXDLU\",\"srcMm\":\"HGS\",\"destMm\":\"SFXDLU\",\"paths\":[{\"edges\":[\"HGS::LKS\",\"LKS::SFXDLU\"],\"nodes\":[\"HGS\",\"LKS\",\"SFXDLU\"],\"tat\":4.5}]}\n\n\n\n\n\n\nBYD -> BWS -> KOS -> SFXKOL -> CCU_Belur\n\n\n\n\nBYD::BWS -> {\"id\":\"BYD::BWS\",\"srcMm\":\"BYD\",\"destMm\":\"BWS\",\"paths\":[{\"edges\":[\"BYD::BWS\"],\"nodes\":[\"BYD\",\"BWS\"],\"tat\":4.5}]}\n\n\nBWS::KOS ->{\"id\":\"BWS::KOS\",\"srcMm\":\"BWS\",\"destMm\":\"KOS\",\"paths\":[{\"edges\":[\"BWS::KOS\"],\"nodes\":[\"BWS\",\"KOS\"],\"tat\":4.5}]}\n\n\nKOS::SFXKOL ->{\"id\":\"KOS::SFXKOL\",\"srcMm\":\"KOS\",\"destMm\":\"SFXKOL\",\"paths\":[{\"edges\":[\"KOS::SFXKOL\"],\"nodes\":[\"KOS\",\"SFXKOL\"],\"tat\":4.5}]}\n\n\nSFXKOL::CCU_Belur ->{\"id\":\"SFXKOL::CCU_Belur\",\"srcMm\":\"SFXKOL\",\"destMm\":\"CCU_Belur\",\"paths\":[{\"edges\":[\"SFXKOL::CCU_Belur\"],\"nodes\":[\"SFXKOL\",\"CCU_Belur\"],\"tat\":4.5}]}\n\n\nBWS::SFXKOL -> {\"id\":\"BWS::SFXKOL\",\"srcMm\":\"BWS\",\"destMm\":\"SFXKOL\",\"paths\":[{\"edges\":[\"BWS::KOS\",\"KOS::SFXKOL\"],\"nodes\":[\"BWS\",\"KOS\",\"SFXKOL\"],\"tat\":4.5}]}\n\n]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"84c93fdd-0997-45a4-817b-8c9f8baf5913\"><ac:parameter ac:name=\"title\">Check Serviceability </ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"095376c9-0fbf-4382-b5ac-61b41c02653e\"><ac:plain-text-body><![CDATA[curl --location 'http://localhost:8888/v1/serviceability-info/manifestation' \\\n--header 'Authorization: kaleyra' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'CLIENT_ID: route-aggregator' \\\n--header 'SECRET_ID: T25XQ64JkCM5LTa' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"requests\": [\n        {\n            \"requestId\": \"156548888869_1_yky\",\n            \"srcNodes\": [\n                \"FSK\"\n            ],\n            \"destPincode\": \"271123\",\n            \"destState\": \"Jammu & Kashmir\",\n            \"dispatchDate\": \"2024-03-28\",\n            \"address\": {\n                \"id\": 81219302,\n                \"name\": \"Anjali\",\n                \"landmark\": \"\",\n                \"city\": \"jourian\",\n                \"district\": \"\",\n                \"state\": \"Jammu & Kashmir\",\n                \"pin\": \"271123\",\n                \"address_line_1\": \"bandwal colony\",\n                \"address_line_2\": \"bandwal colony\"\n            }\n        }\n    ]\n}'\n]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><p>Postman Collection Link : <a href=\"https://meesho.postman.co/workspace/Manifest~8e5f7b49-956a-47ce-b254-877d75584d87/folder/21455471-b585136d-b6bb-44d7-90e1-34d2629c1814?ctx=documentation\">https://meesho.postman.co/workspace/Manifest~8e5f7b49-956a-47ce-b254-877d75584d87/folder/21455471-b585136d-b6bb-44d7-90e1-34d2629c1814?ctx=documentation</a></p><h3>Sample Test Case Sheet : </h3><p style=\"margin-left: 30.0px;\"><a href=\"https://docs.google.com/spreadsheets/d/1UpaRhDpXKvXAGDL0OMSRkAzmol3e6Gk9hyXZEaBi9w0/edit#gid=133991873\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1UpaRhDpXKvXAGDL0OMSRkAzmol3e6Gk9hyXZEaBi9w0/edit#gid=133991873</a> </p><p /><p>Release Steps  : </p><p> 1. ValmoLogistics Service Deployment </p><ol start=\"2\"><li><p>Manifest Supply and Manifest Worker release for configs sync </p></li><li><p>Get ops team to Create AutoDml entities via file uploads </p></li><li><p>Prod Sanity of new opstech lane . </p></li></ol>",
        "representation": "storage",
        "word_count": 2062
      },
      "version": {
        "number": 7,
        "when": "2025-02-20T11:22:56.846Z",
        "by": "Akanksha Malhotra"
      },
      "labels": []
    },
    {
      "id": "3326967819",
      "title": "ML On-Call RunBook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3326967819",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h2>On-Call Responsibilities</h2><ol start=\"1\"><li><p>To Fix any issue in the on-call week (If you don&rsquo;t know then pull in service owner and get this debugged and deployed)</p></li><li><p>For any HOTFIX in service, the on-call should take responsibility of the development, Approval and deployment</p></li><li><p>Create own Queue for solving Issues (P0, P1, P2)</p></li><li><p>Cost Calculation, Experiment Launch, Orion Migration, Skye Model Onboarding (Any onboarding requests)</p></li><li><p>Sale Readiness Planning if you are lying before sale weekend</p></li><li><p>RCA to be written by on-call in case of any major issue happens</p></li><li><p>Drive Standup</p></li><li><p>Kaizen Sheet Running, Drive on-call Handover and fill Kaizen summary as well</p></li><li><p>Alert Noise Fix (Fix Alert Thresholds)</p></li><li><p>Add to Kaizen Summary in case of Uptime degradation</p></li></ol><h2><strong>Service Owner Responsibility</strong></h2><ol start=\"1\"><li><p>To Review PR of the owned service</p></li><li><p>Any Scale-up Request should be handled by Service Owner</p></li><li><p>Any change related to Feature Releases in the service</p></li></ol><p /><p>On-Call Sheet</p><p><a href=\"https://docs.google.com/spreadsheets/d/1n2Gi-HsMlwit8OHxe-OPWISiH8wHPcx_CleyBABXR3o/edit?gid=119304247#gid=119304247\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1n2Gi-HsMlwit8OHxe-OPWISiH8wHPcx_CleyBABXR3o/edit?gid=119304247#gid=119304247</a> </p><p>Kaizen Summary Sheet</p><p><a href=\"https://docs.google.com/spreadsheets/d/1CeSn6rrukFLroaF0JCVDMyNz-c3U1ycSaZqYUS_4br0/edit?gid=1806700779#gid=1806700779\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1CeSn6rrukFLroaF0JCVDMyNz-c3U1ycSaZqYUS_4br0/edit?gid=1806700779#gid=1806700779</a> </p>",
        "representation": "storage",
        "word_count": 140
      },
      "version": {
        "number": 4,
        "when": "2025-03-24T10:20:10.429Z",
        "by": "Aditya Kumar Garg"
      },
      "labels": []
    },
    {
      "id": "3303342081",
      "title": "SonarQube Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3303342081",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h2><strong><u>Change Default Branch of Projects</u></strong></h2><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"1072\" ac:original-width=\"2516\" ac:custom-width=\"true\" ac:alt=\"image (7).png\" ac:width=\"760\"><ri:attachment ri:filename=\"image (7).png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"1436\" ac:original-width=\"2526\" ac:custom-width=\"true\" ac:alt=\"image (8).png\" ac:width=\"760\"><ri:attachment ri:filename=\"image (8).png\" ri:version-at-save=\"1\" /></ac:image><p /><p />",
        "representation": "storage",
        "word_count": 31
      },
      "version": {
        "number": 1,
        "when": "2024-04-19T12:50:00.674Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3292758050",
      "title": "RunBook : Onboarding a new Opstech Partner on IOL[WIP]",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3292758050",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>This runbook provides the template to be followed while onboarding a new Opstech.</p><h3>API Contracts</h3><p>Opstech partners should share the API contracts along with staging credentials for:</p><ul><li><p><strong>Create</strong>: This should be idempotent.</p></li><li><p><strong>Cancel</strong></p></li><li><p><strong>Update</strong></p></li></ul><p>Additional Features if need to  implement on opstech . </p><ul><li><p>Secondary Qc and Packet Id </p></li><li><p>Image capture <br />   </p></li></ul><h5>To be Shared by Us:</h5><ul><li><p><strong>Tracking Webhook</strong>: </p><ul><li><p>Webhook where Opstech will send tracking events. </p></li><li><p /></li></ul></li><li><p><strong>Seller OTP</strong>: </p><ul><li><p>For sending OTP  code to the supplier panel during RTO deliveries.</p></li><li><p><a href=\"https://docs.google.com/document/d/1fMuzl6m7fhevJcVLE6dMrh55j5EdlaeU6waDA3UuR-E/edit\" data-card-appearance=\"inline\">https://docs.google.com/document/d/1fMuzl6m7fhevJcVLE6dMrh55j5EdlaeU6waDA3UuR-E/edit</a> </p></li></ul></li><li><p>Trip API : </p><ul><li><p>For bulk update for shipments to create pendency among opstech : </p></li><li><p><a href=\"https://docs.google.com/document/d/11cPYYhDrserrcwP5UcwddVZuoZkvmiASTkxIG7E9iWc/edit\" data-card-appearance=\"inline\">https://docs.google.com/document/d/11cPYYhDrserrcwP5UcwddVZuoZkvmiASTkxIG7E9iWc/edit</a> </p></li></ul></li></ul><h3>Tracking Status Mappings</h3><p style=\"margin-left: 30.0px;\">Opstech should share scan mappings that need to be validated by the Product POC.</p><h5 style=\"margin-left: 30.0px;\">Validation Pointers:</h5><p style=\"margin-left: 60.0px;\">Ensure mappings are accurate and comprehensive.</p><h3>Additional Requirements</h3><ul><li><p><strong>Dashboard</strong>: Provide dashboards for p99 and p95 metrics for manifestation and cancellation from OTP.</p></li></ul><h3>Development Changes</h3><ul><li><p><strong>ValmoLogisticsService</strong>: Add Opstech name to the enum.</p></li><li><p><strong>Nexus</strong>: Add opstech in Enum </p></li><li><p><strong>SubNexus</strong>: New Topic for opstech along with producer and Consumer</p></li><li><p><strong>Upload SubNexus &lt;&gt; Opstech Scan Mappings</strong> for FWD and RTO.</p></li></ul><h3>Testing</h3><ul><li><p><strong>Sample Test Sheet</strong>: Prepare and share.</p></li></ul><h3>Pre-Release Checks</h3><ul><li><p><strong>IP Whitelisting</strong>: Whitelist Opstech IP on Valmo.</p></li></ul><h3>Post-Release Checks</h3><p>Conduct production sanity checks for both alpha and beta lanes covering the following cases:</p><ul><li><p>Forward delivered</p></li><li><p>RTO delivered</p></li><li><p>Cancelled</p></li><li><p>Seller OTP</p></li></ul><h5>Additional Steps for Beta Lane:</h5><ul><li><p><strong>Trip API for Handover</strong>: Verify functionality.</p></li></ul>",
        "representation": "storage",
        "word_count": 202
      },
      "version": {
        "number": 5,
        "when": "2024-11-07T05:46:38.521Z",
        "by": "Akanksha Malhotra"
      },
      "labels": []
    },
    {
      "id": "3258155009",
      "title": "Ranking & MLP On-call Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3258155009",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"14c5c523-5c21-41af-8ec1-ad0570d824d8\"><colgroup><col style=\"width: 176.0px;\" /><col style=\"width: 583.0px;\" /></colgroup><tbody><tr><th><p><strong>Runbook name</strong></p></th><td><p>Ranking &amp; MLP</p></td></tr><tr><th data-highlight-colour=\"#f4f5f7\"><p><strong>Runbook description</strong></p></th><td><p>Oncall Runbook</p></td></tr><tr><th data-highlight-colour=\"#f4f5f7\"><p><strong>Owner</strong></p></th><td><p>Ranking &amp; MLP team (e-mail: <a href=\"mailto:discovery-ranking@meesho.com\">discovery-ranking@meesho.com</a>)</p></td></tr><tr><th data-highlight-colour=\"#f4f5f7\"><p><strong>Service</strong></p></th><td><p>Ranking:</p><ol start=\"1\"><li><p>vision</p></li><li><p>relevance</p></li><li><p>clp-iop</p></li><li><p>pdp-iop</p></li><li><p>feed-aggregator: feed-aggregator and feed-aggregator-secondary</p></li><li><p>rx-fa-collections</p></li><li><p>offline-cg</p></li><li><p>catalog-visibility-filtering</p></li><li><p>fy-iop</p></li><li><p>cg-algo-starter</p></li></ol><p>MLP:</p><ol start=\"1\"><li><p>Model Proxy</p></li><li><p>Feature Store</p></li><li><p>Model Inference</p></li><li><p>GPMC</p></li></ol></td></tr><tr><th data-highlight-colour=\"#f4f5f7\"><p><strong>Version</strong></p></th><td><p>1.0</p></td></tr><tr><th data-highlight-colour=\"#f4f5f7\"><p><strong>Version date</strong></p></th><td><p><time datetime=\"2024-03-25\" /> </p></td></tr><tr><th data-highlight-colour=\"#f4f5f7\"><p><strong><ac:inline-comment-marker ac:ref=\"bad75166-f80a-4afe-be70-3f24240436d0\">On this page</ac:inline-comment-marker></strong></p></th><td><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"a4ade54e-b2df-41c5-84a3-a982d0d2ed5b\" ac:macro-id=\"a7093733-9c42-4812-8814-b0c1948bd1a9\"><ac:parameter ac:name=\"minLevel\">2</ac:parameter></ac:structured-macro></td></tr></tbody></table><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":joystick:\" ac:emoji-id=\"1f579\" ac:emoji-fallback=\"🕹\" />&nbsp;Application monitoring</h2><table data-table-width=\"1011\" data-layout=\"default\" ac:local-id=\"28276656-48bd-4b21-92c7-80622aceacc8\"><colgroup><col /><col style=\"width: 137.0px;\" /><col style=\"width: 314.0px;\" /><col style=\"width: 126.0px;\" /><col style=\"width: 392.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>Application</strong></p></th><th><p><strong>Function</strong></p></th><th><p><strong>Downstreams</strong></p></th><th><p><strong>Dashboards</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p>Relevance</p></td><td><p>Generates the For-You Organic Feed</p></td><td><p>2</p></td><td><p>Infra: <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;from=now-2d&amp;to=now&amp;var-service=prd-relevance-web&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-relevance-web&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-relevance-web\">Relevance Infra</a><br />ArgoCD: <a href=\"https://argocd-demand-prd.meeshogcp.in/applications/argocd-demand-prd/prd-relevance-web?view=tree&amp;resource=\">Relevance ArgoCD</a><br />Application: <a href=\"https://grafana-prd.meeshogcp.in/d/c25e1350-8328-451c-b6ad-88800803ff68/all-about-relevance?orgId=1&amp;var-demand_cluster=p-demand-cluster&amp;var-namespace=prd-relevance-web&amp;var-r_service=relevance-web&amp;var-kube_job=.%2Akube-state-metrics-p-demand-cluster&amp;var-deployment=%28prd-relevance-web%7Cprd-relevance-web-primary%29&amp;var-cluster=All&amp;var-pod=All&amp;var-instance=All&amp;var-node=All&amp;var-infra_pod=All&amp;var-app_instance=All&amp;from=now-2d&amp;to=now&amp;refresh=10s\">All about Relevance</a></p></td></tr><tr><td class=\"numberingColumn\">2</td><td><p>Model Proxy</p></td><td><p>Provides scores to input catalogs</p></td><td><p>3, 4, 5</p></td><td><p>Infra: <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;from=now-2d&amp;to=now&amp;var-service=prd-model-proxy-service-pdp-ad&amp;var-cluster=kube-state-metrics-k8s-datascience-prd-ase1&amp;var-namespace=prd-model-proxy-service-pdp-ad&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-datascience-prd-ase1&amp;var-copy_of_namespace=prd-model-proxy-service-pdp-ad\">MP Infra</a><br />Application: <a href=\"https://grafana-prd.meeshogcp.in/d/dQ1Gux-Vk/model-proxy-service?orgId=1&amp;from=now-2d&amp;to=now&amp;var-service=model-proxy-service-pdp-ad&amp;refresh=5s\">MP Application</a><br />ArgoCD: <a href=\"https://argocd-datascience-prd.meeshogcp.in/applications/prd-model-proxy-service-fy-organic?resource=\">MP ArgoCD</a></p></td></tr><tr><td class=\"numberingColumn\">3</td><td><p>Feature Store</p></td><td><p>Provides values for input catalog/user features</p></td><td><p /></td><td><p>Infra: <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;from=now-2d&amp;to=now&amp;var-service=prd-online-feature-store-read-v4-mp&amp;var-cluster=kube-state-metrics-k8s-datascience-prd-ase1&amp;var-namespace=prd-online-feature-store-read-v4-mp&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-datascience-prd-ase1&amp;var-copy_of_namespace=prd-online-feature-store-read-v4-mp\">FS v4 MP Infra</a><br />Envoy: <a href=\"https://grafana-prd.meeshogcp.in/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-online-feature-store-read-v4_prd-online-feature-store-read-v4-primary_80&amp;var-hosts=All&amp;refresh=10s&amp;from=now-2d&amp;to=now\">FS Envoy</a><br />Application: <a href=\"https://grafana-prd.meeshogcp.in/d/jA-iyAY4O/feature-store-v4?orgId=1&amp;refresh=30s&amp;from=now-2d&amp;to=now\">FS v4 Application</a></p></td></tr><tr><td class=\"numberingColumn\">4</td><td><p>Model Inference</p></td><td><p>Orchestrator for executing the Models for getting the scores</p></td><td><p /></td><td><p>Infra: <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;from=now-2d&amp;to=now&amp;var-service=prd-model-inference-service-fy-organic&amp;var-cluster=kube-state-metrics-k8s-datascience-prd-ase1&amp;var-namespace=prd-model-inference-service-fy-organic&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-datascience-prd-ase1&amp;var-copy_of_namespace=prd-model-inference-service-fy-organic\">MI Infra</a><br />Application: <a href=\"https://grafana-prd.meeshogcp.in/d/a2605923-52c4-4834-bdae-97570966b765/model-inference-service?orgId=1&amp;refresh=5s&amp;from=now-24h&amp;to=now&amp;var-service=model-inference-service-pdp-ad\">MI Application</a></p></td></tr><tr><td class=\"numberingColumn\">5</td><td><p>GPMC</p></td><td><p>Provides the final score to input catalogs</p></td><td><p /></td><td><p>Infra: <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;from=now-2d&amp;to=now&amp;var-service=prd-gpmc-service&amp;var-cluster=kube-state-metrics-k8s-datascience-prd-ase1&amp;var-namespace=prd-gpmc-service&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-datascience-prd-ase1&amp;var-copy_of_namespace=prd-gpmc-service\">GPMC Infra</a><br />Application: <a href=\"https://grafana-prd.meeshogcp.in/d/dQ1Gux-Vz/gpmc-service?orgId=1&amp;from=now-24h&amp;to=now\">GPMC</a></p></td></tr><tr><td class=\"numberingColumn\">6</td><td><p>catalog-visibility-filtering</p></td><td><p>Filters bad catalogs at global level as well as at user level.</p></td><td><p>AB, segemnt-store</p></td><td><p>Infra: <a href=\"https://grafana-prd.meeshogcp.in/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;from=now-2d&amp;to=now&amp;var-service=prd-catalog-visibility-filtering-web&amp;var-cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-namespace=prd-catalog-visibility-filtering-web&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-k8s-demand-prd-ase1&amp;var-copy_of_namespace=prd-catalog-visibility-filtering-web\">CVF Infra</a><br />Application: <a href=\"https://grafana-prd.meeshogcp.in/d/c208fff3-1a5e-43a0-a83e-7d2f0a04faa0/catalog-visibility-filtering?orgId=1&amp;from=now-50h&amp;to=now&amp;refresh=5s\">CVF Application</a></p></td></tr><tr><td class=\"numberingColumn\">7</td><td><p>feed-aggregator</p></td><td><p>Aggregator of different feeds for the user. This is primary based on the priority of APIs that it serves like the CLP Feed API.</p></td><td><p /></td><td><p>Telegraf: <a href=\"https://grafana-prd.meeshogcp.in/d/4GVPlNL4z/telegraf-services-dashboard?orgId=1&amp;var-service=feed-aggregator&amp;var-instance=All&amp;refresh=5s\">feed-aggregator Telegraf</a><br />Envoy: <a href=\"https://grafana-prd.meeshogcp.in/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-feed-aggregator_prd-feed-aggregator-primary_80&amp;var-hosts=All&amp;refresh=10s&amp;from=now-1h&amp;to=now\">feed-aggregator Envoy</a><br />ArgoCD: <a href=\"https://argocd-demand-prd.meeshogcp.in/applications/prd-feed-aggregator?resource=\">feed-aggregator ArgoCD</a><br />Application: Does not exist</p></td></tr><tr><td class=\"numberingColumn\">8</td><td><p>feed-aggregator-secondary</p></td><td><p>Aggregator of different feeds for the user. This is secondary based on the priority of APIs that it serves like it doesn&rsquo;t serve the CLP Feed API.</p></td><td><p /></td><td><p>Telegraf: <a href=\"https://grafana-prd.meeshogcp.in/d/4GVPlNL4z/telegraf-services-dashboard?orgId=1&amp;var-service=feed-aggregator-secondary&amp;var-instance=All&amp;refresh=5s\">feed-aggregator-secondary Telegraf</a><br />Envoy: <a href=\"https://grafana-prd.meeshogcp.in/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-feed-aggregator-secondary_prd-feed-aggregator-secondary-primary_80&amp;var-hosts=All&amp;refresh=10s&amp;from=now-1h&amp;to=now\">feed-aggregator-secondary Envoy</a><br />ArgoCD: <a href=\"https://argocd-demand-prd.meeshogcp.in/applications/prd-feed-aggregator-secondary?resource=\">feed-aggregator-secondary ArgoCD</a><br />Application: Does not exist</p></td></tr><tr><td class=\"numberingColumn\">9</td><td><p>segment-store</p></td><td><p>Service that takes care of user segmentation</p></td><td><p /></td><td><p>Application: <a href=\"https://grafana-prd.meeshogcp.in/d/ac6e3715-2471-4cdd-a6d5-aa64792d7726/all-about-segment-store?orgId=1\">segment-store Application</a></p></td></tr><tr><td class=\"numberingColumn\">10</td><td><p>exploit-online-cg</p></td><td><p /></td><td><p /></td><td><p>Application: <a href=\"https://grafana-prd.meeshogcp.in/d/c2162938-4ee5-49b3-8fd8-5320cce3e229/exploit-online-cg?orgId=1&amp;refresh=5s&amp;from=now-30m&amp;to=now\">exploit-online-cg Application</a><br />Envoy: <a href=\"https://grafana-prd.meeshogcp.in/d/000000003/envoy-proxy?orgId=1&amp;var-cluster=prd-exploit-online-cg-web_prd-exploit-online-cg-web-primary_80&amp;var-hosts=All&amp;refresh=5s&amp;from=now-30m&amp;to=now\">exploit-online-cg ENVOY</a></p></td></tr><tr><td class=\"numberingColumn\">11</td><td><p>System Uptime</p></td><td><p>Query to provide system uptime of a service chosen from dropdown.<br />Note: The query used is a complex query which also includes latency as a parameter.<br />Ref: <a href=\"https://docs.google.com/document/d/1XBB5NxiKfX4YmEBvxR6A1f1xEey3x4uROJpL7S7DbSU/edit\">System Uptime Implementation</a> </p></td><td><p /></td><td><p><a href=\"https://grafana-prd.meeshogcp.in/d/w72igLl4k/system-uptime-stateless?orgId=1&amp;var-telegraf_service=relevance-web&amp;var-cluster=prd-relevance-web_prd-relevance-web-primary_80&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.99&amp;var-latency_percentile_period=5m&amp;var-latency_percentile_threshold=0.999&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d&amp;var-uri=All&amp;from=now-24h&amp;to=now\">System Uptime</a></p></td></tr></tbody></table><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":fire:\" ac:emoji-id=\"1f525\" ac:emoji-fallback=\"🔥\" />&nbsp;Known errors &amp; <ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":fire_extinguisher:\" ac:emoji-id=\"1f9ef\" ac:emoji-fallback=\"🧯\" />Troubleshooting</h2><table data-table-width=\"1011\" data-layout=\"wide\" ac:local-id=\"2a168de9-9666-40af-88d8-996abe180323\"><colgroup><col /><col style=\"width: 160.0px;\" /><col style=\"width: 158.0px;\" /><col style=\"width: 255.0px;\" /><col style=\"width: 396.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>Error</strong></p></th><th><p><strong>Error time</strong></p></th><th><p><strong>Error report</strong></p></th><th><p><strong>Troubleshooting</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p>offline-cg 5xx at 12AM</p></td><td><p>Everyday around 00:00</p></td><td><p>Seems to be happening everyday at same time. Alert threshold is &gt; 10</p></td><td><p>Need to check why it happens and if we need to tweak the alert.</p></td></tr><tr><td class=\"numberingColumn\">2</td><td><p>online-feature-store-consumer-beta POD CPU threshold crossed</p></td><td><p /></td><td><p>CPU is going till ~120 of request.<br />Autoscaling is set to 30% CPU</p></td><td><p><ac:inline-comment-marker ac:ref=\"b8255e36-6da8-4caa-955e-de272c68f96c\">Have increased alert threshold so should be fine now.</ac:inline-comment-marker><br />If still comes, need to check why autoscaling did not happen.</p></td></tr><tr><td class=\"numberingColumn\">3</td><td><p>System-Uptime-Producer-alert - datascience - ml-platform - real-time-feature-processor</p></td><td><p /></td><td><p /></td><td><p>These alerts were onboarded recently. We need to understand the alert query and tweak if needed.</p></td></tr></tbody></table><h2>🚀 Weekly Oncall Tracking</h2><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" data-layout=\"wide\" ac:macro-id=\"ac4fc2de-c879-4180-9a92-815d297532b2\"><ac:parameter ac:name=\"title\">25-31 March 2024 Primary Oncall: Abhishek Pasi</ac:parameter><ac:rich-text-body><table data-table-width=\"1011\" data-layout=\"wide\" ac:local-id=\"a382a2c1-c90e-4379-9031-1a88250158fd\"><colgroup><col /><col style=\"width: 159.0px;\" /><col style=\"width: 152.0px;\" /><col style=\"width: 246.0px;\" /><col style=\"width: 382.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>Issue</strong></p></th><th><p><strong>Issue time</strong></p></th><th><p><strong>Issue report</strong></p></th><th><p><strong>Actions Taken</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p>MP short-circuited</p></td><td><p><time datetime=\"2024-03-25\" /> </p><p>~7:30AM - ~10:30AM</p></td><td><p>MP short-circuited at relevance service and hence RPS dropped at MP and pods did not scale(majorly <code>prd-model-proxy-service-fy-organic</code>) up till it was short-circuited.<br />MP service CPU had shoot upto 90%, seems like this caused some issue as the service coule not scaleup fast enopugh.</p></td><td><p>Immediate: Increased min count from 2 to 12(BAU value for that TOD).<br />Decreased the CPU threshold for MP clusters to 70% for fast scaling.</p><p>Decreasing to 70% worked well, did not get the issue again.</p></td></tr><tr><td class=\"numberingColumn\">2</td><td><p>offline-cg 5xx at 12AM</p></td><td><p><time datetime=\"2024-03-25\" /> <br />12:00-12:01AM</p></td><td><p>Seems to be happening everyday at same time. Alert threshold is &gt; 10</p></td><td><p>Need to check why it happens and if we need to tweak the alert.</p></td></tr><tr><td class=\"numberingColumn\">3</td><td><p>online-feature-store-consumer-beta POD CPU threshold crossed</p></td><td><p><time datetime=\"2024-03-25\" /> </p><p>and few times more in the week</p></td><td><p>CPU is going till ~120 of request.<br />Autoscaling is set to 30% CPU</p></td><td><p>Increased alert threshold to 120% for now since its a consumer.<br />Need to check why autoscaling did not happen.</p></td></tr><tr><td class=\"numberingColumn\">4</td><td><p>exploit-cg-online p99 latency</p></td><td><p><time datetime=\"2024-03-25\" /> </p></td><td><p>Alert came once in the week.<br />Spike is present in latency and upstream connections. exploit-cg-online infra was stable</p></td><td><p /></td></tr><tr><td class=\"numberingColumn\">5</td><td><p>pdp-iacg-service 5xx</p></td><td><p><time datetime=\"2024-03-25\" /> </p></td><td><p>pods became unavailable.</p></td><td><p>Did not find the reason for it.</p></td></tr><tr><td class=\"numberingColumn\">6</td><td><p>System-Uptime-Producer-alert - datascience - ml-platform - real-time-feature-processor</p></td><td><p><time datetime=\"2024-03-26\" /></p></td><td><p /></td><td><p>As commented by Mohit:<br />It's fine. We need to tune the thresholds, MQ team have setup these alerts recently.<br />Yet to be tuned.<br />Have asked Avinash if any tweaking required and also to understand the query: <a href=\"https://meesho.slack.com/archives/C04TUUBHFV4/p1711633382629189?thread_ts=1711630327.189349&amp;cid=C04TUUBHFV4\">Slack Thread</a> </p></td></tr><tr><td class=\"numberingColumn\">7</td><td><p>FY_CACHE_HIT_BELOW_THRESHOLD_ALERT_CT_GKE</p></td><td><p><time datetime=\"2024-03-26\" /></p></td><td><p>saw that user coverage has dropped and cache hit dropped by same.</p></td><td><p>Checked with Rohith Balaji. They found that experiment got expired and hence this alert. We snoozed this alert for 2 hrs.</p></td></tr><tr><td class=\"numberingColumn\">8</td><td><p>pdp-iacg-service 5xx</p></td><td><p><time datetime=\"2024-03-26\" /></p></td><td><p>checked with dhaval parmar, currently we are ignoring sine this will be moved to re-arch infra</p></td><td><p /></td></tr><tr><td class=\"numberingColumn\">9</td><td><p>AB service v2 bulk experiment fetch API latency increase</p></td><td><p><time datetime=\"2024-03-28\" /></p></td><td><p>Latency base increased from 20th March i.e. post sale. P50 shows the increase clearly.<br />Impacted catalog-filtering-service and vision-reco API latencies where this is a downstream.</p></td><td><p>Raised to AB team. They said to check: <a href=\"https://app.slack.com/client/T0S2UJU8H/C028YP2QAQH\">Slack Thread</a> <br />As on <time datetime=\"2024-04-01\" /> the latency at AB seems to come back down, but it can be due to sale scaleup. We need to check again post scale down and if latency keeps high we need to ask them again.</p></td></tr><tr><td class=\"numberingColumn\">10</td><td><p>reco-ibcg 5xx</p></td><td><p><time datetime=\"2024-03-28\" />multiple times in the day</p></td><td><p>5xx seems to be coming everyday.</p></td><td><p>Need to check the reason.</p></td></tr><tr><td class=\"numberingColumn\">11</td><td><p>vss 5xx</p></td><td><p><time datetime=\"2024-03-28\" /></p></td><td><p /></td><td><p>Need to check the reason.</p></td></tr><tr><td class=\"numberingColumn\">12</td><td><p>pdp-iacg 5xx(503)</p></td><td><p><time datetime=\"2024-03-28\" /></p></td><td><p>Only 503s coming, no 500s.<br />Can see downstream VSS latency spikes happening and 503 pattern matching with this. There are CB failures also.<br />The monitoring panels' query was bit wrong and not showing circuit opens etc, corrected it.<br />CB CallNotPermitted, CircuitBreaker 'vss-service-breaker' is OPEN and does not permit further calls.<br />VSS timeout seems to be quite high but maybe we need to revisit.</p></td><td><p>Need to revisit the VSS timeouts.</p></td></tr><tr><td class=\"numberingColumn\">13</td><td><p>Feature Store issue: pipeline broken for user entity.</p></td><td><p><time datetime=\"2024-03-29\" /> </p></td><td><p>Issue happened due to incomplete onboarding of couple new features which broke the pipeline for user entity.<br />Data for new features was pushed from DS end but the feature was not onboarded at backend side(API was called but the restart of services was not done)</p></td><td><p>Restarted the services, which finished the onboarding of the new features and the issue got fixed.</p></td></tr><tr><td class=\"numberingColumn\">14</td><td><p>feed-aggregator producer latency increased at MQ side</p></td><td><p><time datetime=\"2024-03-30\" /> </p></td><td><p>There was no issue on producing just the latency was increased which was discussed with MQ team</p></td><td><p><ac:link><ri:user ri:account-id=\"628667c756b847006978837c\" /></ac:link> / <ac:link><ri:user ri:account-id=\"6320b1d3ed8abffd7ffcaca0\" /></ac:link> Anything open in this issue? Please update this if i missed anything.</p><p>Ref: <a href=\"https://meesho.slack.com/archives/C04NQ6VQLMB/p1711777681979499\">Slack Thread</a> </p></td></tr></tbody></table><p><strong>Open items</strong></p><ul><li><p>Nothing critical is open.</p></li><li><p>Can check VSS high latency and failures.</p></li></ul></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" data-layout=\"wide\" ac:macro-id=\"4e875f69-e0a5-4e87-b789-4bc28c694bc2\"><ac:parameter ac:name=\"title\">1-7 April 2024 Primary Oncall: Jigar</ac:parameter><ac:rich-text-body><table data-table-width=\"1011\" data-layout=\"wide\" ac:local-id=\"50394dc5-79a8-4274-a821-0fb11038a00e\"><colgroup><col /><col style=\"width: 194.0px;\" /><col style=\"width: 134.0px;\" /><col style=\"width: 229.0px;\" /><col style=\"width: 382.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>Issue</strong></p></th><th><p><strong>Issue time</strong></p></th><th><p><strong>Issue report</strong></p></th><th><p><strong>Actions Taken</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p>Scylla DB latency spiked.</p></td><td><p>4th April <br />7: 30 pm to 7:45 pm</p></td><td><p>All the scylla DB clusters used in feature-store, interaction-store, VSS experienced latency spike during this time.<br /><strong>Impact</strong> : 5xx on the services increased and settled down.<br />For reco-ibcg-web one of the pods were giving </p></td><td><p>DB team has raised this to GCP team. <br />slack thread : <a href=\"https://meesho.slack.com/archives/C05310NH6MQ/p1712242314120139\" data-card-appearance=\"inline\">https://meesho.slack.com/archives/C05310NH6MQ/p1712242314120139</a> </p></td></tr><tr><td class=\"numberingColumn\">2</td><td><p>Qdrant OOM issue for similarity image.</p></td><td><p>3rd April</p></td><td><p /></td><td><p>Known issue. Qdrant server needs to scaled.</p></td></tr><tr><td class=\"numberingColumn\">3</td><td><p>reco-ibcg 5xx</p></td><td><p>Frequently </p></td><td><p>5xx </p></td><td><p>Needs to be debugged by <ac:link><ri:user ri:account-id=\"60791dd39361560068fd7a31\" /></ac:link> </p></td></tr><tr><td class=\"numberingColumn\">4</td><td><p>clp-iop 5xx</p></td><td><p>Intermittently everyday</p></td><td><p /></td><td><p /></td></tr><tr><td class=\"numberingColumn\">5</td><td><p>feed-agreegator issue</p></td><td><p>5th April.</p></td><td><p>pods was going unhealthy causing 5xx.</p></td><td><p><a href=\"https://meesho.slack.com/archives/C1DU1NURF/p1712319098934529\" data-card-appearance=\"inline\">https://meesho.slack.com/archives/C1DU1NURF/p1712319098934529</a> </p></td></tr><tr><td class=\"numberingColumn\">6</td><td><p>Item-scoring/ entity scoring uptime </p></td><td><p>Intermittently everyday</p></td><td><p /></td><td><p>Needs to be debugged by <ac:link><ri:user ri:account-id=\"5efef164e0043f0bacb31430\" /></ac:link> </p></td></tr><tr><td class=\"numberingColumn\">7</td><td><p>recent-explore cg uptime </p></td><td><p>Intermittentlyeveryday</p></td><td><p /></td><td><p>Needs to be debugged looks like because of item-scoring / entity-scoring</p></td></tr></tbody></table></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" data-layout=\"wide\" ac:macro-id=\"6020f6e5-ae10-4a64-baf2-c84b0c5eb99f\"><ac:parameter ac:name=\"title\">8-14 April 2024 Primary Oncall: Shubham</ac:parameter><ac:rich-text-body><table data-table-width=\"1011\" data-layout=\"wide\" ac:local-id=\"c731bc49-80da-4cd0-9877-633eade4583e\"><colgroup><col /><col style=\"width: 194.0px;\" /><col style=\"width: 134.0px;\" /><col style=\"width: 229.0px;\" /><col style=\"width: 382.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>Issue</strong></p></th><th><p><strong>Issue time</strong></p></th><th><p><strong>Issue report</strong></p></th><th><p><strong>Actions Taken</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p /></td><td><p /></td><td><p /></td><td><p /></td></tr></tbody></table></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" data-layout=\"wide\" ac:macro-id=\"a54072fb-76bd-45c6-bd43-b13aa3749d7a\"><ac:parameter ac:name=\"title\">22-28 April 2024 Primary Oncall: Dhaval</ac:parameter><ac:rich-text-body><table data-table-width=\"1011\" data-layout=\"wide\" ac:local-id=\"9d5bb048-d9af-4083-8e8d-80b77b5a94be\"><colgroup><col /><col style=\"width: 194.0px;\" /><col style=\"width: 134.0px;\" /><col style=\"width: 229.0px;\" /><col style=\"width: 382.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>Issue</strong></p></th><th><p><strong>Issue time</strong></p></th><th><p><strong>Issue report</strong></p></th><th><p><strong>Actions Taken</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p>Feed aggregator sort-filter api 4xx</p></td><td><p /></td><td><p /></td><td><p /></td></tr></tbody></table></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" data-layout=\"wide\" ac:macro-id=\"38e4cf77-8bfc-4132-8f62-f181efe2858d\"><ac:parameter ac:name=\"title\">20-27 May 2024 Primary Oncall: Shreyans</ac:parameter><ac:rich-text-body><table data-table-width=\"1011\" data-layout=\"wide\" ac:local-id=\"853e743f-5b1f-43e0-98a1-23225e5a18ed\"><colgroup><col /><col style=\"width: 194.0px;\" /><col style=\"width: 134.0px;\" /><col style=\"width: 229.0px;\" /><col style=\"width: 382.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>Issue</strong></p></th><th><p><strong>Issue time</strong></p></th><th><p><strong>Issue report</strong></p></th><th><p><strong>Actions Taken</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p>Frequent timeouts on SF for rx-fa-reco</p></td><td><p /></td><td><p>wrongly configured taxonomy timeouts</p></td><td><p>Corrected the slow call duration of taxonomy</p></td></tr><tr><td class=\"numberingColumn\">2</td><td><p>clp-iop 5xx alerts</p></td><td><p>22nd May, 12:07 AM and 1:07 AM</p></td><td><p>pods were coming up or going down, causing 503</p></td><td><p>none. was corrected when pods became active</p></td></tr><tr><td class=\"numberingColumn\">3</td><td><p>fy-iop-web alerts</p></td><td><p>Multiple times</p></td><td><ol start=\"1\"><li><p>Deployments</p></li><li><p>Traffic migration</p></li></ol></td><td><p /></td></tr><tr><td class=\"numberingColumn\">4</td><td><p>Organic cache hit ratio decreased for fy</p></td><td><p>early morning of 22nd May</p></td><td><p /></td><td><p /></td></tr><tr><td class=\"numberingColumn\">5</td><td><p /></td><td><p /></td><td><p /></td><td><p /></td></tr></tbody></table></ac:rich-text-body></ac:structured-macro><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" data-layout=\"wide\" ac:macro-id=\"e42d0d57-dcf4-413f-b1ec-d6b626546fac\"><ac:parameter ac:name=\"title\">27 May- 2 June 2024 Primary Oncall: Abhishek</ac:parameter><ac:rich-text-body><table data-table-width=\"1011\" data-layout=\"wide\" ac:local-id=\"1ed40906-b579-4d92-b123-714b906316f2\"><colgroup><col /><col style=\"width: 194.0px;\" /><col style=\"width: 134.0px;\" /><col style=\"width: 229.0px;\" /><col style=\"width: 382.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>Issue</strong></p></th><th><p><strong>Issue time</strong></p></th><th><p><strong>Issue report</strong></p></th><th><p><strong>Actions Taken</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p>Dragonfly latency spike caused Model Proxy latency spikes at PDP IOP which caused PDP-IOP to get short circuited at rx-fa-recommendations. Organic and CT traffic dropped to very low levels and ADs traffic was consistent.</p></td><td><p>On 28th May night, 8:45 - 9:50PM</p></td><td><p>Ads revenue went up during the period due to Organic and CT catalogs count in feed would have dropped.</p></td><td><p><strong>Mitigation:</strong> <br />From rx-fa-recommendations ZK, moved organic traffic from PDP-IOP to vision-reco. Stopped CT traffic meanwhile and then slowly increased traffic to 100% when MLP service was fine.<br />MLP team diverted traffic from dragonfly to Redis. Made it 50-50% each. They will debug deeper after sale.</p><p>RPS: <a href=\"https://grafana-prd.meeshogcp.in/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1&amp;from=1716834600000&amp;to=1716920999000&amp;viewPanel=273\"><u>https://grafana-prd.meeshogcp.in/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1&amp;from=1716834600000&amp;to=1716920999000&amp;viewPanel=273</u></a></p><p>Ads revenue: <a href=\"https://grafana-prd.meeshogcp.in/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1&amp;from=1716834600000&amp;to=1716920999000&amp;viewPanel=244\"><u>https://grafana-prd.meeshogcp.in/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1&amp;from=1716834600000&amp;to=1716920999000&amp;viewPanel=244</u></a></p></td></tr><tr><td class=\"numberingColumn\">2</td><td><p>Entity scoring failed from majorly causing failures in multiple services, exploit-cg, recent-explore-cg and FY CT RPS and cache.<br />There was an experiment which didn't had index in mongoDB due to index limit reached. This index used to be there before but someone removed it to put their own new index as indexes are limited and this in dex was not being used for months.</p></td><td><p>On 29th May morning, 8:30 - 9:10AM</p></td><td><p>Failures in multiple services, exploit-cg, recent-explore-cg and FY CT RPS and cache</p></td><td><p>The experiment which was causing this was stopped later and planned top be run after creating index.</p></td></tr><tr><td class=\"numberingColumn\">3</td><td><p>Many services across Meesho affected including ranking service FY-IOP, CLP-IOP, multiple cgs</p></td><td><p>On 30th May morning, 7:30 - 11:30AM</p></td><td><p>Perceived latency at caller services much higher than envoy latency for downstreams.</p></td><td><p>Nodepool reverted with devops and service restarted and it got fixed at 11:15AM suddenly</p></td></tr><tr><td class=\"numberingColumn\">4</td><td><p>Similar issue as on 28th May. Dragonfly latency spikes. PDP-IOP RPS dropped due to CB open at rx-fa-recommendations due to MP latency high.</p></td><td><p>On 1st June evening, 3:30 - 5PM</p></td><td><p>Ads revenue went up during the period due to Organic and CT catalogs count in feed would have dropped.</p></td><td><p><strong>Mitigation</strong>: <br />From rx-fa-recommendations ZK, moved organic traffic from PDP-IOP to vision-reco. Stopped CT traffic meanwhile and then slowly increased traffic to 100% when MLP service was fine.<br />MLP team diverted traffic from dragonfly to Redis. Made it 100% Redis. They will debug deeper after sale.</p><p>RPS: <a href=\"https://grafana-prd.meeshogcp.in/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1&amp;from=1717180200000&amp;to=1717266599000&amp;viewPanel=273\"><u>https://grafana-prd.meeshogcp.in/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1&amp;from=1717180200000&amp;to=1717266599000&amp;viewPanel=273</u></a></p><p>Ads Revenue: <a href=\"https://grafana-prd.meeshogcp.in/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1&amp;from=1717180200000&amp;to=1717266599000&amp;viewPanel=244\"><u>https://grafana-prd.meeshogcp.in/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1&amp;from=1717180200000&amp;to=1717266599000&amp;viewPanel=244</u></a></p></td></tr><tr><td class=\"numberingColumn\">5</td><td><p>Alert noise handled for empty feed length in PDP-IOP</p></td><td><p>28 May</p></td><td><p>The new-ads tenant empty feed spikes up sometimes.</p></td><td><p>Currently kept a separate alert for them since their threshold value is different but we can look into merging it and notifying them when common alert happens. Shreyans and Dhaval were looking into solving this.</p></td></tr><tr><td class=\"numberingColumn\">6</td><td><p>Alert query improved in vision and vision-reco</p></td><td><p>1st June</p></td><td><p>Catalog Hidden rate alert was configured on absolute rate of this metric but in Sale the total requests increased organically hence this alert started creating noise.</p></td><td><p>Catalog Hidden rate query was improved by including RPS in it for vision and vision-reco</p></td></tr><tr><td class=\"numberingColumn\">7</td><td><p>Redis latency spike in Catalog Detail Redis used at vision-web, vision-reco etc</p></td><td><p>28 May and 2 June</p></td><td><p>There were spikes in Redis latency P99</p></td><td><p>Raised to devops since we can only see avergae latency at BDB dashboard.</p></td></tr></tbody></table></ac:rich-text-body></ac:structured-macro><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":man_detective:\" ac:emoji-id=\"1f575-fe0f-200d-2642-fe0f\" ac:emoji-fallback=\"🕵️&zwj;♂️\" />&nbsp;RCAs</h2><table data-table-width=\"1011\" data-layout=\"wide\" ac:local-id=\"804fcbcb-ea7f-49b8-a185-b27dace1ae55\"><colgroup><col /><col style=\"width: 270.0px;\" /><col style=\"width: 277.0px;\" /><col style=\"width: 422.0px;\" /></colgroup><tbody><tr><th class=\"numberingColumn\" /><th><p><strong>RCA description</strong></p></th><th><p><strong>RCA date</strong></p></th><th><p><strong>RCA report</strong></p></th></tr><tr><td class=\"numberingColumn\">1</td><td><p /></td><td><p><ac:placeholder>Type // to add a date</ac:placeholder></p></td><td><p>Attach the RCA doc</p></td></tr><tr><td class=\"numberingColumn\">2</td><td><p /></td><td><p /></td><td><p /></td></tr><tr><td class=\"numberingColumn\">3</td><td><p /></td><td><p /></td><td><p /></td></tr></tbody></table>",
        "representation": "storage",
        "word_count": 1977
      },
      "version": {
        "number": 13,
        "when": "2024-06-03T11:28:04.887Z",
        "by": "Abhishek Pasi"
      },
      "labels": []
    },
    {
      "id": "3247308801",
      "title": "Meecom Oncall Runbook:  11/03 - 18/03 2024",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3247308801",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>On-call : <ac:link><ri:user ri:account-id=\"62c2a907a152cf973643e116\" /></ac:link> </p><table ac:local-id=\"615bb926-f562-458f-9b3d-c096ef3dfca0\" data-layout=\"default\" data-table-width=\"1011\"><colgroup><col style=\"width: 67.0px;\" /><col style=\"width: 188.0px;\" /><col style=\"width: 134.0px;\" /><col style=\"width: 253.0px;\" /><col style=\"width: 369.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p /></th><th><p><strong>Alert</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Alert date</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Alert report</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Action</strong></p></th></tr><tr><td><p>1</p></td><td><p>Supplier WA campaigns getting marked as completed but sent count not getting updated in Meecom</p></td><td><p>11/03-13/03</p></td><td><p>the messages failed from provider end with the error<br /><code>Message does not match WhatsApp HSM template.</code></p></td><td><p>Got on a call with gupshup POCs to get this resolved as some of the templates that were used earlier were also returning the same error .<br />The gupshup POCs did not revert due to sale P0s , hence we asked the team to create new templates and then test them out on meecom itself.</p></td></tr><tr><td><p>2</p></td><td><p>whatsapp number integration</p></td><td><p>13/03</p></td><td><p>this was done by <ac:link><ri:user ri:account-id=\"624a8d8af6a26900695f46f5\" /></ac:link> </p></td><td><p>Amit please add the same here or some comms doc as we are getting the same requirement from superstore team as well</p></td></tr><tr><td><p>3</p></td><td><p>Pattern shift in rate limit graph for FY</p></td><td><p>happening from last week , gradually worsened</p></td><td><p>There were multiple things identified and we tried number of fixes.<br />The latency had gone up from the same time the pattern started shifting. <br />There were delImmdRetries up as well</p></td><td><p>we tried changing the FCM_CUSTOM_HTTP_CONNECTION_POOL_SIZE<br />a bit . When the lag started getting accumulated on kafka , we changed some configs <br />MQ ID: 306Max.wait 10K -&gt;5K<br />Fetch.min.bytes 512-&gt;256<br />concurrency delivery very low&nbsp; 3-&gt;4<br /></p></td></tr><tr><td><p>4</p></td><td><p>Fcm Latency spiked to  <br />19-20k max <br />avg - 12k for 2 hours</p></td><td><p>15/03</p></td><td><p>This caused some lag to build up  and the pns were sent late and there was some loss for some slots as well</p></td><td><p>Ticket raised : </p><p><a href=\"https://console.cloud.google.com/support/cases/detail/v2/50165496?project=meesho-supply\">https://console.cloud.google.com/support/cases/detail/v2/50165496?project=meesho-supply</a></p></td></tr><tr><td><p>5</p></td><td><p>SMS otp failure</p><p>[11 pm - 1 am]</p></td><td><p>15/03</p></td><td><p>It was observed that from 11 pm onwards , the RPM of otp sms dropped <br />No template was able to send OTPs</p></td><td><p>The error was from gupshup end , they had actually changed their URL from having &ldquo;http&rdquo;<br />to &ldquo;https&rdquo; at the same time.<br />This caused multiple retries with all resulting in failures , which in turn led to lag built up in sms delivery queue due to which all other otp providers were not able to send any message only.<br />This was resolved around 12:50 by making the fixes around URLs .</p></td></tr><tr><td><p>6</p></td><td><p>Duplicate Pns issue</p></td><td><p>14/03</p></td><td><p>Duplicate pns were sent around 2:14 pm to one of the user ids<br />11818176</p></td><td><p>The same was not reflecting in our Pn record tables or the error logs in the same time frame.<br />was not debugged</p></td></tr><tr><td><p>7</p></td><td><p>Duplicate whatsapp issue</p></td><td><p>15/03</p></td><td><p>Duplicate whatsapp messages were sent to multiple users , and processing files had duplicate entries in them which were double the size of audience files provided</p></td><td><p><ac:link><ri:user ri:account-id=\"62c2a90ba152cf973643e11b\" /></ac:link> was debugging around this.<br />she had added a fix  at 11:45 ,15/03 , please add more details here </p></td></tr><tr><td><p>8</p></td><td><p>Campaign creators not able to select certain types of columns in meecom</p></td><td><p>14/03</p></td><td><p>this was an access issue </p></td><td><p>this was handled</p></td></tr><tr><td><p>9</p></td><td><p>campaign creator Pocs</p></td><td><p>pre sale</p></td><td><p>There were different queries from campaign creator POCs on why there were some errors showing up on meecom</p></td><td><p>These were <br />1 - character limit on column_names<br />2- invalid template did not call out any specific error, just something went wrong<br />3- viewed metrics not available on meecom for popup channels<br />Need help from web to support these</p></td></tr><tr><td><p>10</p></td><td><p>Whatsapp IOS login</p></td><td><p>13/03</p></td><td><p>One particular user was not getting the deep link when trying to login via whatsapp on IOS.<br />This was raised in meesho dogfooding session.<br />we went around the flow and checked in the login flow , there were some errors and deeplink did not get created.</p></td><td><p>When debugging with central team POC to check the message in confluent kafka, we wanted to debug it live as there no error logs in their service <br />User did not revert. But it started working for the user later<br /><br />Same issue came between sale time <br /><ac:link><ri:user ri:account-id=\"62c2a982eda95d2e3a68cbab\" /></ac:link> can you mention the issues around that time?</p></td></tr></tbody></table><p>PD alerts report : <a href=\"https://meesho.pagerduty.com/analytics/insights/service-performance-report?end=2024-03-18T10%3A41%3A30&amp;priority=all&amp;service_ids=PA84F2R-PJK9SWK-PHRMHPU-PJRGYN5-P110S9D-PBCUX8T-PY36WW3-PUTIVGD-PRW3X3D-P0KHXXI-PQHXV4Q-P251HEA-PIZ2PET-PV6P5XZ-P3C42E6-PIM22P0-PBDT0KN-P30YJC4-PLJHJMO-PHQ5TCR-PQY0RRR-PLZPPXV-P5LREPS-PDMDCVW-PNOKE2K-P33OCB7-P6CZDQX-PWE4B3G-PT4GART-P8J7YMD-PUIVPLL-PR97P9Y-PNZJOX8-PGGTJ8B-PDDHT01-P0M3RVL-PUTT06U-PXUK5V1-PRKRWAD-PUK4YY1&amp;start=2024-03-11T00%3A00%3A00&amp;team_ids=PL1MGGD-PUQT6JF-POM9QII&amp;urgency=all\">https://meesho.pagerduty.com/analytics/insights/service-performance-report?end=2024-03-18T10%3A41%3A30&amp;priority=all&amp;service_ids=PA84F2R-PJK9SWK-PHRMHPU-PJRGYN5-P110S9D-PBCUX8T-PY36WW3-PUTIVGD-PRW3X3D-P0KHXXI-PQHXV4Q-P251HEA-PIZ2PET-PV6P5XZ-P3C42E6-PIM22P0-PBDT0KN-P30YJC4-PLJHJMO-PHQ5TCR-PQY0RRR-PLZPPXV-P5LREPS-PDMDCVW-PNOKE2K-P33OCB7-P6CZDQX-PWE4B3G-PT4GART-P8J7YMD-PUIVPLL-PR97P9Y-PNZJOX8-PGGTJ8B-PDDHT01-P0M3RVL-PUTT06U-PXUK5V1-PRKRWAD-PUK4YY1&amp;start=2024-03-11T00%3A00%3A00&amp;team_ids=PL1MGGD-PUQT6JF-POM9QII&amp;urgency=all</a></p><p><br />Detailed report : <a data-card-appearance=\"inline\" href=\"https://docs.google.com/spreadsheets/d/1TFhkDou3L388EMpoBhDM8SJhRHt6PsGXm7vM1BK9Lfk/edit#gid=0\">https://docs.google.com/spreadsheets/d/1TFhkDou3L388EMpoBhDM8SJhRHt6PsGXm7vM1BK9Lfk/edit#gid=0</a> </p>",
        "representation": "storage",
        "word_count": 640
      },
      "version": {
        "number": 2,
        "when": "2024-03-18T09:12:06.889Z",
        "by": "Anurag Thakur"
      },
      "labels": []
    },
    {
      "id": "3234136120",
      "title": "Runbook: Leaders Cost Analysis Dashboard",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3234136120",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<h3>Introduction:</h3><p>This document serves as the comprehensive guide for navigating and interpreting the Cost Analysis Dashboard. Designed to provide detailed insights into cost data for specific resources, this documentation outlines the dashboard's functionality and usage instructions to facilitate effective cost analysis.</p><h3>Data Source:</h3><p>The Cost Analysis Dashboard retrieves its data from Google Cloud Platform's BigQuery billing <a href=\"https://console.cloud.google.com/bigquery?referrer=search&amp;project=meesho-admin-prd-0622&amp;ws=!1m12!1m3!8m2!1s476413035414!2s2de7fe2473db4960b7513034e067eebf!1m3!8m2!1s476413035414!2s9b096ddaf87340019b09cb88007a21c1!1m3!3m2!1smeesho-admin-prd-0622!2sdetailed_usage_cost_us\">dataset</a>. This dataset contains comprehensive information regarding organization's billing and usage details within GCP. For further details on billing data and its structure, please refer to the <a href=\"https://cloud.google.com/billing/docs/how-to/export-data-bigquery\">Google Cloud Platform Billing documentation.</a> </p><h3><strong>Dashboard Components:</strong></h3><h3><u>Section 1 : Cost by Leaders:</u> </h3><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"317\" ac:original-width=\"401\" ac:custom-width=\"true\" ac:alt=\"image-20240312-144345.png\" ac:width=\"401\"><ri:attachment ri:filename=\"image-20240312-144345.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Tagging and Resource Mapping</strong></p><p>When creating resources in Google Cloud Platform (GCP), each resource is tagged with a label identifying the associated team. These labels are crucial for mapping resources to specific team leaders and aggregating costs effectively.</p><h4>Mapping Resources</h4><ul><li><p><strong>Mapping to Team Leaders</strong>: Resources tagged with a team label are mapped to the corresponding team leader, allowing for accurate cost attribution. The details for Team &rarr; Leaders mapping is available here <a href=\"https://docs.google.com/spreadsheets/d/17QDEsfIdsY9wQXhyctuXZfg1_pkfHOXiafv7gsKhr3I/edit?usp=sharing\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/17QDEsfIdsY9wQXhyctuXZfg1_pkfHOXiafv7gsKhr3I/edit?usp=sharing</a> </p></li></ul><h4>Handling Untagged Resources(NotTagged bucket under Leaders):</h4><ul><li><p><strong>NotTagged Resources</strong>: In cases where resources are not tagged with a team label and where k8s deployments doesn&rsquo;t have the label team, they are categorized as &quot;<strong>NotTagged</strong>&quot; in the leaders section. These untracked resources pose a challenge as they cannot be directly mapped to a specific person or team, leading to potential cost discrepancies.</p></li></ul><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"4e42bf2b-ddb4-4af8-8b3a-cd11f1ac0554\"><ac:rich-text-body><p>In some cases we found out the naming convention of the <code>team</code> label is not proper so we can&rsquo;t able to map that specific teams to Leaders,, As a result, resources labeled with a team identifier but lacking associated mapping details are classified as '<strong>Unmapped-team</strong>'.</p></ac:rich-text-body></ac:structured-macro><h3><u>Section 2 : Tracked Cost - Team:</u> </h3><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"377\" ac:original-width=\"405\" ac:custom-width=\"true\" ac:alt=\"image-20240312-150858.png\" ac:width=\"405\"><ri:attachment ri:filename=\"image-20240312-150858.png\" ri:version-at-save=\"1\" /></ac:image><h3>Mapping Teams to Resources</h3><p>The team-to-resource mapping is primarily based on the presence of labels. If a resource is tagged with a label indicating the associated team, it will be reflected in the mapping. However, there's an exception for resources related to Compute Engine.</p><h4>Compute Engine Exception</h4><p>Compute Engine resources present a unique challenge due to their dual tracking components: Kubernetes (k8s) and non-Kubernetes (non-k8s) costs. Given that we've enabled Google Kubernetes Engine (GKE) cost allocation within our clusters, we can use k8s-namespace to map it to a specific team to track costs seamlessly..</p><h4>Kubernetes Namespace Mapping</h4><p>For Kubernetes resources labeled with a k8s-namespace, we've implemented a specific mapping strategy to assign them to a team. The mapping details are in the following sheet..<a href=\"https://docs.google.com/spreadsheets/d/18AnxQ2L6BArdLWh77h2t7lT4gNTrTa53c6uAw7pL75c/edit?usp=sharing\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/18AnxQ2L6BArdLWh77h2t7lT4gNTrTa53c6uAw7pL75c/edit?usp=sharing</a> </p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"d0c3ca8b-2359-4d64-8ccf-7364afc25f68\"><ac:rich-text-body><p>So the above Tracked Costs Section will contain both k8s and Non k8s cost accumulated , if you want to view this separately then please refer this <a href=\"https://lookerstudio.google.com/reporting/dee80432-a410-4d16-b142-1d9c8931b582/page/p_3t8w5y3cfd\">dashboard</a>  </p></ac:rich-text-body></ac:structured-macro><h3><u>Section 3 : Untracked Cost :</u> </h3><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"483\" ac:original-width=\"1265\" ac:custom-width=\"true\" ac:alt=\"image-20240312-152415.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240312-152415.png\" ri:version-at-save=\"1\" /></ac:image><p>This section encompasses expenses associated with resources that cannot be directly attributed to a specific individual or team. This category includes both Kubernetes (k8s) and non-Kubernetes (non-k8s) costs. </p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"e221b787-3213-407e-b068-7285884e3879\"><ac:rich-text-body><p>The Kubernetes resource consumption is highlighted within the total untracked costs ( the 2nd panel in the above image ).</p></ac:rich-text-body></ac:structured-macro><h4>Kubernetes Resource Considerations</h4><p>Even within Kubernetes environments, certain resources pose challenges in terms of tracking and attribution. This is particularly evident in cases where team names cannot be extracted from the deployments associated with Kubernetes namespaces.</p><h4>K8s Resource Breakdown</h4><p>Within the Kubernetes cost allocation, three significant cost components contribute to the untracked expenses:</p><ol start=\"1\"><li><p><strong>kube:unallocated</strong>: Resources are neither requested by workloads nor requested for system overhead</p></li><li><p><strong>goog-k8s-unsupported-sku</strong>: GKE cost allocation does not support this SKU.</p></li><li><p><strong>kube:system-overhead</strong>: Resources are reserved for system overhead.</p></li></ol><p>For further insights into Kubernetes cost allocation and understanding these components, please refer to the Google Cloud Platform <a href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/cost-allocations\">GKE cost allocation documentation.</a></p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"84565da0-2ed1-41f7-a698-2acd329b4618\"><ac:rich-text-body><p>It's important to note that GKE cost allocation is calculated based on resource requests rather than consumption. Therefore, costs for pods exceeding their resource requests fall under <strong>kube:unallocated </strong>category</p></ac:rich-text-body></ac:structured-macro><p><strong>Filters :</strong></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"69\" ac:original-width=\"1239\" ac:custom-width=\"true\" ac:alt=\"image-20240312-154206.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240312-154206.png\" ri:version-at-save=\"1\" /></ac:image><p>The dashboard offers a range of filters designed to enable users to refine data to a granular level. These filters empower users to select specific criteria, such as directors, teams, date ranges, projects, business units, services, SKUs, labels, and Kubernetes namespaces, allowing for precise control over the displayed data.</p><h3>Conclusion</h3><p>Certainly, summarizing the key points from the three sections:</p><ol start=\"1\"><li><p><strong>Tagging Importance</strong>: Tagging resources with appropriate labels is crucial for effective cost tracking and attribution. It enables the mapping of resources to specific individuals or teams, facilitating accurate cost allocation.</p></li><li><p><strong>Untracked Costs</strong>: Certain costs, such as kube:unallocated or networking expenses, may remain untracked. However, our primary focus should be on minimizing untracked costs by ensuring thorough standard practices( for example.. we can reduce the costs under <code>kube:unallocated</code> by appropriately specifying the requests need for a service )</p></li><li><p><strong>Minimizing Untracked Costs</strong>: While some costs may inevitably remain untracked, our goal is to minimize this as much as possible. By emphasizing the importance of tagging and implementing robust tagging conventions, we can enhance cost visibility and optimize resource utilization.</p></li></ol>",
        "representation": "storage",
        "word_count": 828
      },
      "version": {
        "number": 1,
        "when": "2024-03-14T17:01:56.671Z",
        "by": "Amlan Sekhar Das"
      },
      "labels": []
    },
    {
      "id": "3228500627",
      "title": "Meecom Oncall Runbook: 4 Mar - 11 Mar 2024",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3228500627",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>On-call : <ac:link><ri:user ri:account-id=\"6214bcd09cd4a70071dde588\" /></ac:link> </p><table data-table-width=\"1011\" data-layout=\"default\" ac:local-id=\"615bb926-f562-458f-9b3d-c096ef3dfca0\"><colgroup><col style=\"width: 67.0px;\" /><col style=\"width: 188.0px;\" /><col style=\"width: 134.0px;\" /><col style=\"width: 253.0px;\" /><col style=\"width: 369.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p /></th><th><p><strong>Alert</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Alert date</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Alert report</strong></p></th><th data-highlight-colour=\"var(--ds-background-accent-gray-subtlest, #F4F5F7)\"><p><strong>Action</strong></p></th></tr><tr><td><p>1</p></td><td><p>Redis memory spike</p></td><td><p>06/03</p></td><td><p>Redis memory utilisation baseline shifted and crossed over 85% </p></td><td><p>Increased redis memory from 115 GB to 135 GB and iops to 1150000 to 1350000</p></td></tr><tr><td><p>2</p></td><td><p>Campaign creation failures for audience Id 113586</p></td><td><p>08/03</p></td><td><p>Continuous update in audience file was leading to latency in api : <code>v1/audience/fetchById/113586</code></p></td><td><p>Raised to ab-team. Was told to not create such query for audience creation.</p></td></tr><tr><td><p>3</p></td><td><p>4xx &amp; 5xx alerts in communicator admin</p></td><td><p>multiple times</p></td><td><p>Due to bad requests by users</p></td><td><p>Task to evaluate alerts <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"aacbd74f-e8f6-426d-b80f-2b687f9b3855\"><ac:parameter ac:name=\"key\">UC-939</ac:parameter><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro>  </p></td></tr></tbody></table><p>PD alerts report : <a href=\"https://meesho.pagerduty.com/analytics/insights/service-performance-report?end=2024-03-11T12%3A54%3A15&amp;priority=all&amp;service_ids=PA84F2R-PJK9SWK-PHRMHPU-PJRGYN5-P110S9D-PBCUX8T-PY36WW3-PUTIVGD-PRW3X3D-P0KHXXI-PQHXV4Q-P251HEA-PIZ2PET-PV6P5XZ-P3C42E6-PIM22P0-PBDT0KN-P30YJC4-PLJHJMO-PHQ5TCR-PQY0RRR-PLZPPXV-P5LREPS-PDMDCVW-PNOKE2K-P33OCB7-P6CZDQX-PWE4B3G-PT4GART-P8J7YMD-PUIVPLL-PR97P9Y-PNZJOX8-PGGTJ8B-PDDHT01-P0M3RVL-PUTT06U-PXUK5V1-PRKRWAD-PUK4YY1&amp;start=2024-03-04T00%3A00%3A00&amp;team_ids=PL1MGGD-PUQT6JF-POM9QII&amp;urgency=all\">https://meesho.pagerduty.com/analytics/insights/service-performance-report?end=2024-03-11T12%3A54%3A15&amp;priority=all&amp;service_ids=PA84F2R-PJK9SWK-PHRMHPU-PJRGYN5-P110S9D-PBCUX8T-PY36WW3-PUTIVGD-PRW3X3D-P0KHXXI-PQHXV4Q-P251HEA-PIZ2PET-PV6P5XZ-P3C42E6-PIM22P0-PBDT0KN-P30YJC4-PLJHJMO-PHQ5TCR-PQY0RRR-PLZPPXV-P5LREPS-PDMDCVW-PNOKE2K-P33OCB7-P6CZDQX-PWE4B3G-PT4GART-P8J7YMD-PUIVPLL-PR97P9Y-PNZJOX8-PGGTJ8B-PDDHT01-P0M3RVL-PUTT06U-PXUK5V1-PRKRWAD-PUK4YY1&amp;start=2024-03-04T00%3A00%3A00&amp;team_ids=PL1MGGD-PUQT6JF-POM9QII&amp;urgency=all</a></p>",
        "representation": "storage",
        "word_count": 119
      },
      "version": {
        "number": 3,
        "when": "2024-03-18T09:11:36.696Z",
        "by": "Anurag Thakur"
      },
      "labels": []
    },
    {
      "id": "3184066898",
      "title": "CI/CD Oncall Issues Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3184066898",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h3>This runbook is to note down minor issues we face sometimes and their 1 click solutions.</h3><p>========================================================================================================================================</p><h3>Issue - <code>Not able to find main|master and PR requests</code></h3><p>Solution - Use the below job<strong> - </strong><a href=\"https://jenkins-prd.meeshogcp.in/job/gcp-migration-pipeline-test/\"><strong>https://jenkins-prd.meeshogcp.in/job/gcp-migration-pipeline-test/</strong></a><br />Also click on<strong> </strong><code>scan repository now</code><strong> </strong>post running this job.</p><p /><p>========================================================================================================================================</p><p /><h3>Issue - <code>Could not determine exact tip revision of &lt;branch name&gt;</code></h3><p>Solution - In the jenkins job, on the left side, click on <code>Scan Repository Now</code> or <code>Scan Multibranch Pipeline Now</code>. Once finished, re-run the job.</p><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"494\" ac:original-width=\"507\" ac:custom-width=\"true\" ac:alt=\"image-20240215-115044.png\" ac:width=\"507\"><ri:attachment ri:filename=\"image-20240215-115044.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"532\" ac:original-width=\"644\" ac:custom-width=\"true\" ac:alt=\"image-20240215-115059.png\" ac:width=\"480\"><ri:attachment ri:filename=\"image-20240215-115059.png\" ri:version-at-save=\"1\" /></ac:image><p /><p>========================================================================================================================================</p><p /><h3>Issue - 401 on Jfrog API Calls from local</h3><p>Solution - 401 comes in local build only. Ask service owners to re-check their local <code>~/.m2/settings.xml</code>. I have created proper settings.xml in this doc - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Jfrog Migration - Developers Prerequisites\" ri:version-at-save=\"8\" /><ac:link-body>Jfrog Migration - Developers Prerequisites</ac:link-body></ac:link> </p><p /><p>========================================================================================================================================</p><p /><h3>Issue - 503 on Jfrog API Calls from local</h3><ul><li><p>On Downloading - <br /><br />Solution - Update <code>settings.xml</code> from this doc - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Jfrog Migration - Developers Prerequisites\" ri:version-at-save=\"8\" /><ac:link-body>Jfrog Migration - Developers Prerequisites</ac:link-body></ac:link> <br /></p></li><li><p>On Uploading/Pushing artifacts to Jfrog -<br /><br />Solution - We have created a job for such requirement, as push from local was turning into prod downtimes (as backend depednancy was updated from local with same name and version). <br />Jenkins Job for the same - <br /><br />PRD - <a href=\"https://jenkins-prd.meeshogcp.in/job/common-dependancies-CI-only/\">https://jenkins-prd.meeshogcp.in/job/common-dependancies-CI-only/</a><br />DEV - <a href=\"https://jenkins-dev.meeshogcp.in/job/common-dependancies-CI-only/\">https://jenkins-dev.meeshogcp.in/job/common-dependancies-CI-only/</a></p></li></ul><p /><p>========================================================================================================================================</p><h3>Issue - </h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"015a84ab-0d60-4f40-a901-91404bcf67de\"><ac:plain-text-body><![CDATA[Failure to find org.apache.maven.plugins:maven-plugins:pom:38 in https://jfrog-prd.meeshogcp.in/artifactory/MeeshoUtilities was cached in the local repository, \nresolution will not be reattempted until the update interval of central has elapsed or updates are forced]]></ac:plain-text-body></ac:structured-macro><p>Solution - Update <code>settings.xml</code> from this doc - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Jfrog Migration - Developers Prerequisites\" ri:version-at-save=\"8\" /><ac:link-body>Jfrog Migration - Developers Prerequisites</ac:link-body></ac:link> </p><p /><p>========================================================================================================================================</p><h3>Issue - In Prod Jenkins <code>ERROR: Error in App Syncorg.jenkinsci.plugins.credentialsbinding.impl.CredentialNotFoundException: Could not find credentials entry with ID 'argocd-ftr-creds</code></h3><p>Solution - It is obvious that in PRD jenkins, it shouldn&rsquo;t have asked for <code>FTR</code> creds (only PRD or INT are allowed in PRD Jenkins). Following could be reasons -</p><ol start=\"1\"><li><p>Something is wrong with either <code>Target Branch</code>i.e., PR is raised against a branch other than - <code>main, master, gcp-main, gcp-master</code></p></li><li><p>Some older branch is written in <code>Jenkinsfile</code></p></li></ol><p /><p>========================================================================================================================================</p><p /><h3>Issue - In the below example, I have substituted original branch name with a var - <code>&lt;some-branch-name&gt;</code></h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"fb2ec69a-4480-4f67-820a-28c81e6f9a46\"><ac:plain-text-body><![CDATA[ERROR: Could not resolve <some-branch-name>\nhudson.plugins.git.GitException: Command \"git rev-parse <some-branch-name>^{commit}\" returned status code 128:\nstdout: <some-branch-name>^{commit}]]></ac:plain-text-body></ac:structured-macro><p>Solution - Please check Jenkinsfile, deleted or non-existent branch will be mentioned.</p><p /><p>========================================================================================================================================</p><p /><h3>Issue - Improper MVN Conf, main line to address this issue <code>from/to maven-default-http-blocker (http://0.0.0.0/)</code></h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"6894b917-7fcc-4693-b850-53ed756fa3eb\"><ac:plain-text-body><![CDATA[[ERROR] Plugin org.apache.maven.plugins:maven-clean-plugin:3.1.0 or one of its dependencies could not be resolved: \nFailed to read artifact descriptor for org.apache.maven.plugins:maven-clean-plugin:jar:3.1.0: \nThe following artifacts could not be resolved: org.apache.maven.plugins:maven-clean-plugin:pom:3.1.0 (present, but unavailable): \nCould not transfer artifact org.apache.maven.plugins:maven-clean-plugin:pom:3.1.0 from/to maven-default-http-blocker (http://0.0.0.0/): \nBlocked mirror for repositories: [central (http://jfrog-prd.meeshogcp.in/artifactory/MeeshoUtilities, default, releases), snapshots (http://jfrog-prd.meeshogcp.in/artifactory/MeeshoUtilities, default, releases+snapshots)] -> [Help 1]]]></ac:plain-text-body></ac:structured-macro><p>Solution - Open file <code>${MAVEN_HOME}/conf/settings.xml</code></p><p>And Comment these lines.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"8597a072-6632-4d59-b16d-7892f408e89b\"><ac:plain-text-body><![CDATA[<mirror>\n    <id>maven-default-http-blocker</id>\n    <mirrorOf>external:http:*</mirrorOf>\n    <name>Pseudo repository to mirror external repositories initially using HTTP.</name>\n    <url>http://0.0.0.0/</url>\n    <blocked>false</blocked>\n</mirror>]]></ac:plain-text-body></ac:structured-macro><p>FYR - <a href=\"https://stackoverflow.com/questions/66980047/maven-build-failure-dependencyresolutionexception\" data-card-appearance=\"inline\">https://stackoverflow.com/questions/66980047/maven-build-failure-dependencyresolutionexception</a> </p><p /><p>========================================================================================================================================</p><p /><h3>Issue - CICD Job Failed in auto-merging step</h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"8c87b3b7-fc13-4728-9813-3391f97b4b33\"><ac:plain-text-body><![CDATA[ERROR: java.lang.IllegalStateException: hudson.AbortException: \nFail: Status code 405 is not in the accepted range: 200 while calling https://api.github.com/repos/Meesho/devops-helm-charts/pulls/71805/merge]]></ac:plain-text-body></ac:structured-macro><p>Solution - Re-run the job, this happens sometimes while auto-merging PRs.</p>",
        "representation": "storage",
        "word_count": 550
      },
      "version": {
        "number": 11,
        "when": "2024-02-29T12:41:53.799Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3171745853",
      "title": "Runbook : Creating Log based Alerts in ElasticSearch",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/SRE/pages/3171745853",
      "space": {
        "key": "SRE",
        "name": "SRE"
      },
      "content": {
        "body": "<ul><li><p>Navigate to <strong>Analytics</strong> section in the elasticsearch dashboard and click on <strong>Discover</strong></p></li><li><p>Select <code>Prd Gcp Meesho</code> Data view for gcp logs</p></li><li><p>Filter the logs based on the KQL( Kibana Query Language) and click on <strong>Alerts &rarr; Create search threshold rule</strong></p><ul><li><p>Example KQL : To filter logs based on application name and the text in the logs </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"42b078b8-fb7e-4609-845b-57239010237e\"><ac:plain-text-body><![CDATA[application_name : storefront-int-app-feed and text : \"command: UserProfile_GetSocialProfilesCommand\"]]></ac:plain-text-body></ac:structured-macro></li></ul></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"665\" ac:original-width=\"1493\" ac:custom-width=\"true\" ac:width=\"736\"><ri:attachment ri:filename=\"Screenshot 2024-02-08 at 9.35.12 AM.png\" ri:version-at-save=\"1\" /></ac:image><ul><li><p>Fill the necessary details in the <strong>Elastic Search Query</strong> section in the  Create rule dashboard.<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"754\" ac:original-width=\"620\" ac:custom-width=\"true\" ac:width=\"620\"><ri:attachment ri:filename=\"Screenshot 2024-02-08 at 9.45.51 AM.png\" ri:version-at-save=\"2\" /></ac:image></li><li><p>Modify this parameter based on how frequently this rule should be evaluated&hellip;</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"98\" ac:original-width=\"604\" ac:custom-width=\"true\" ac:alt=\"image-20240208-050409.png\" ac:width=\"604\"><ri:attachment ri:filename=\"image-20240208-050409.png\" ri:version-at-save=\"1\" /></ac:image></li><li><p>In the <strong>Actions </strong>section click on <strong>Pagerduty</strong> and select <strong>Meesho Pagerduty</strong>, fill the options as per the image and choose <strong>serverity</strong> as per your preference..</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"696\" ac:original-width=\"625\" ac:custom-width=\"true\" ac:width=\"625\"><ri:attachment ri:filename=\"Screenshot 2024-02-08 at 10.07.33 AM.png\" ri:version-at-save=\"1\" /></ac:image></li><li><p>For component and group in the pagerduty section below,  Fill the <strong>service name</strong> for component and <strong>Section</strong> for group.. This service name and section details you can retrieve it from the pulse-dashboard <a href=\"https://pulse.meeshogcp.in/alert/alerts\">pulse</a> and search your service and click on edit&hellip;</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"265\" ac:original-width=\"584\" ac:custom-width=\"true\" ac:alt=\"image-20240208-044532.png\" ac:width=\"584\"><ri:attachment ri:filename=\"image-20240208-044532.png\" ri:version-at-save=\"1\" /></ac:image></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"670\" ac:original-width=\"1469\" ac:custom-width=\"true\" ac:alt=\"image-20240208-044345.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20240208-044345.png\" ri:version-at-save=\"1\" /></ac:image><p /><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"eaebde42-48e6-4398-aa00-fb34e056f75a\"><ac:rich-text-body><p>Note: The above action is added to <strong>trigger</strong> the pagerduty alert when the query condition is matched,  <br />We can also auto resolve the alert when the query is recovered by adding the below action if needed</p></ac:rich-text-body></ac:structured-macro><h4>Auto Resolving of Triggered alert:</h4><ul><li><p>Click on add action and select connector type <strong>Pagerduty</strong> and choose <strong>Meesho Pagerduty. </strong>Now Change the <code>Run When</code> parameter to Recovered and <code>Event action</code> to Resolve for auto resolving of alerts..</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"546\" ac:original-width=\"619\" ac:custom-width=\"true\" ac:width=\"619\"><ri:attachment ri:filename=\"Screenshot 2024-02-08 at 10.24.03 AM.png\" ri:version-at-save=\"1\" /></ac:image></li><li><p>Click on <strong>Save.</strong></p></li><li><p>You can view the created alert by Navigating to Management &rarr; StackManagement &rarr; Rules and search for the rule you created.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"850\" ac:original-width=\"1496\" ac:custom-width=\"true\" ac:alt=\"image-20240208-045831.png\" ac:width=\"736\"><ri:attachment ri:filename=\"image-20240208-045831.png\" ri:version-at-save=\"1\" /></ac:image></li></ul>",
        "representation": "storage",
        "word_count": 343
      },
      "version": {
        "number": 2,
        "when": "2024-03-14T01:30:52.570Z",
        "by": "Naveen Vellingiri"
      },
      "labels": []
    },
    {
      "id": "3152871482",
      "title": "RunBook - Refactoring For Tech Debt",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3152871482",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>This run-book provides a structured approach for refactoring to enhance code maintainability and readability by organising and encapsulating business logic.</p><p><strong>Step 1: Understand Existing Code Flow</strong></p><ul><li><p>Review the current ViewModel codebase to understand structure and functionality and list down them.</p></li><li><p>Focus on identifying functionality that can be encapsulated for better maintainability.</p></li><li><p>Identify areas where business logic is tightly coupled with UI-related tasks.</p></li></ul><p><strong>Step 2: Evaluate UseCase Delegation</strong></p><ul><li><p>Identify piece of code in the ViewModel that represent distinct business logic or functionality.</p></li><li><p>Determine if each identified piece of business logic can be encapsulated in a separate UseCase.</p></li><li><p>Assess whether the business logic is straightforward enough to be directly implemented in the ViewModel without compromising readability or maintainability.</p></li><li><p>Evaluate scenarios where creating a dedicated UseCase may not be necessary, such as straightforward business logic.</p></li><li><p>Consider factors like scalability, re-usability, testability, and potential future enhancements to determine the need of UseCase.</p></li></ul><p><strong>Step 3. List UseCases</strong></p><ul><li><p>Create a list of UseCases based on the identified pieces of business logic.</p></li><li><p>Ensure each UseCase class encapsulates a single responsibility or functionality in codebase.</p></li><li><p>Define a mandatory <code>execute()</code> function in each UseCase to provide a singular entry point for executing the encapsulated functionality.</p></li></ul><p><strong>Step 4: Implement UseCases in ViewModels</strong></p><h4>Creating UseCases:</h4><ul><li><p>For each identified piece of business logic, create a corresponding UseCase class <strong>with a suffix 'Uc'.</strong></p></li><li><p>If required, encapsulate specific functionality within each UseCase class.</p></li></ul><h4>Delegate Responsibilities:</h4><ul><li><p>Move relevant code from the ViewModel to its corresponding UseCase class.</p></li><li><p>The ViewModel should delegate the responsibility of handling specific business logic to the associated UseCase.</p></li></ul><h4>Dependency Injection:</h4><ul><li><p>Inject instances of UseCase classes into the ViewModel.</p></li><li><p>Also ensure that the ViewModel is injectable. If required, consider adding an <code>AssistedFactory</code>.<br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c49b7b4a-c9cc-4349-a4cf-0cb161358dd8\"><ac:plain-text-body><![CDATA[class MallVm\n@AssistedInject\nconstructor(\n    @Assisted val screenEntryPoint: ScreenEntryPoint,\n    @Assisted val pagingBody: PagingBody,\n    @Assisted private val disposables: CompositeDisposable,\n   ...\n) : ViewModel {\n    ...\n    @AssistedFactory\n    interface Factory {\n        fun create(\n            screenEntryPoint: ScreenEntryPoint,\n            pagingBody: PagingBody,\n            disposables: CompositeDisposable,\n        ): MallVm\n    }\n}]]></ac:plain-text-body></ac:structured-macro></li></ul><h4>ViewModel Simplification:</h4><ul><li><p>With business logic moved to UseCases, the ViewModel can focus more on managing UI-related tasks.</p></li><li><p>Refactor ViewModels to delegate business logic to newly created UseCase classes.</p></li><li><p>Update ViewModel methods to invoke the appropriate UseCase.</p></li></ul><p><strong>Step 5: Maintain Data Flow and Separation of Concerns - </strong><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3113582930/Refactoring+For+Tech+Debt#Maintaining-Data-Flow-and-Separation-of-Concerns\"><strong>more details</strong></a></p><ul><li><p>Ensure that ViewModels communicate with UseCases to manage business logic and data retrieval from repositories.</p></li><li><p>Avoid passing ViewModels as input parameters to UseCases.</p></li></ul><p><strong>Step 6: Unit Testing</strong></p><ul><li><p>Write unit tests for both UseCases and ViewModels.<em>[Optional if UT is not present]</em></p></li><li><p>Use unit tests for UseCases to validate the correct behaviour of encapsulated business logic.</p></li><li><p>Similarly, utilise unit tests for ViewModels to verify the interaction between the UI, LiveData, and user actions.</p></li></ul><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"035b759d-1c66-4513-be53-2dea285a3514\"><ac:rich-text-body><p>We've made writing UT optional if not already in place. However, if the logic is complex, it's advisable to begin with unit tests. Afterward, refactor the class and then update the corresponding unit tests accordingly. </p></ac:rich-text-body></ac:structured-macro><p><strong>Step 7: Standardising Analytics Pattern</strong></p><ul><li><p>Create dedicated analytics classes for each feature to ensure consistent structure and promote modular and maintainable analytics integration.</p></li><li><p>Use dependency injection to inject analytics manager instances into analytics classes.</p></li><li><p>Follow a standardised naming convention for analytics classes -  <strong>[FeatureName]Analytics</strong>.<br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"55aa10e2-5b1d-4a11-96db-37e8b3529c98\"><ac:plain-text-body><![CDATA[class UserProfileAnalytics @Inject constructor(val analyticsManager: AnalyticsManager) : AnalyticsAware{\n    fun trackUserProfileScreenViewed(userId: Int) {\n      ...\n        analyticsManager.pushAnalyticsEvent()\n    }\n}]]></ac:plain-text-body></ac:structured-macro></li></ul><p><strong>Step 8: Refactor UI Layer (Activity/Fragment)</strong></p><ul><li><p>Offload business logic from Activities/Fragments to dedicated components like UseCases or ViewModels.</p></li><li><p>Create dedicated ViewControllers for specific features to manage feature-specific logic for views..</p></li><li><p>These ViewControllers should focus solely on UI-related concerns and be Lifecycle-aware if required.</p></li></ul><p><strong>Step 9: Testing and Validation</strong></p><ul><li><p>List down all the impacted feature and thoroughly test components to ensure functionality is preserved and bugs are minimised.</p></li><li><p>Take help from QA(Should not add much effort as UI automation is there for most for features).</p></li></ul><p><strong>Step 10: Documentation and Knowledge Sharing</strong></p><ul><li><p>Document the refactoring process, including any changes made to architecture or code structure.</p></li><li><p>Share knowledge and best practices with team members to ensure consistency and understanding across the development team.</p></li></ul><h3>Common challenges : </h3><p><strong>1. Identifying Business Logic:</strong></p><ul><li><p><strong>Challenge:</strong> It can be difficult to distinguish between UI-related code and actual business logic, especially in complex ViewModels.</p></li><li><p><strong>Solution:</strong> Take time to thoroughly analyse the codebase. Use code comments, documentation, or even discussions with team members to clarify the purpose of each piece of code.</p></li></ul><p><strong>2. Refactoring Legacy Code:</strong></p><ul><li><p><strong>Challenge:</strong> Refactoring large and legacy codebases can be can be scary because you might accidentally break things.</p></li><li><p><strong>Solution:</strong> Start small and focus on incremental improvements. Identify easy opportunities where business logic can be easily extracted into UseCases without causing major disruptions. Refactor gradually, constantly testing and validating changes to ensure stability. </p></li></ul>",
        "representation": "storage",
        "word_count": 716
      },
      "version": {
        "number": 4,
        "when": "2024-02-13T12:01:22.496Z",
        "by": "Ashish Kumar"
      },
      "labels": []
    },
    {
      "id": "3143106601",
      "title": "Runbook : Creating Log based Alerts in ElasticSearch",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3143106601",
      "space": {
        "key": "DEVOPS",
        "name": "DevOps"
      },
      "content": {
        "body": "<ac:adf-extension><ac:adf-node type=\"panel\"><ac:adf-attribute key=\"panel-type\">note</ac:adf-attribute><ac:adf-content><p>Note: To Create alerts and rules in Elastic Search additional permissions are needed. Please reach out to <strong>sre-team </strong>for permissions if create rule option is not enabled</p></ac:adf-content></ac:adf-node><ac:adf-fallback><div class=\"panel conf-macro output-block\" style=\"background-color: rgb(234,230,255);border-color: rgb(153,141,217);border-width: 1.0px;\"><div class=\"panelContent\" style=\"background-color: rgb(234,230,255);\">\n<p>Note: To Create alerts and rules in Elastic Search additional permissions are needed. Please reach out to <strong>sre-team </strong>for permissions if create rule option is not enabled</p>\n</div></div></ac:adf-fallback></ac:adf-extension><ul><li><p>Navigate to observability section in the elasticsearch dashboard and click on Alerts &rarr; Manage Rules </p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"795\" ac:original-width=\"1493\" ac:custom-width=\"true\" ac:width=\"736\"><ri:attachment ri:filename=\"Screenshot 2024-01-23 at 10.23.11 PM.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>In the rules dashboard click on create rule</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"433\" ac:original-width=\"1249\" ac:custom-width=\"true\" ac:alt=\"image-20240123-165655.png\" ac:width=\"736\"><ri:attachment ri:filename=\"image-20240123-165655.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>In the Create rule dashboard select the rule type based on your preferences(for example - Log threshold (this rule type alerts when logs breached certain threshold)) and filter the error logs based on the below options available (for example filter based on application, text message in the log, service name etc..)<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"664\" ac:original-width=\"619\" ac:custom-width=\"true\" ac:width=\"619\"><ri:attachment ri:filename=\"Screenshot 2024-01-24 at 8.04.10 AM.png\" ri:version-at-save=\"1\" /></ac:image><p><br /></p></li><li><p>The next step is to configure action for the alert when the alert condition met, for this <br />select the connector type and configure it as per your preference <br />it can be slack or email or pd etc.. and then click on save.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"669\" ac:original-width=\"615\" ac:custom-width=\"true\" ac:alt=\"image-20240123-171511.png\" ac:width=\"615\"><ri:attachment ri:filename=\"image-20240123-171511.png\" ri:version-at-save=\"1\" /></ac:image><p><br /></p></li><li><p>once everything is created you can view the alerts in the alerts dashboard or rules dashboard</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"598\" ac:original-width=\"1253\" ac:custom-width=\"true\" ac:alt=\"image-20240123-171730.png\" ac:width=\"736\"><ri:attachment ri:filename=\"image-20240123-171730.png\" ri:version-at-save=\"1\" /><ac:caption><p><em><strong><span style=\"color: rgb(54,179,126);\">Connectors Overview</span></strong></em></p></ac:caption></ac:image></li><li><p><strong><u>Pagerduty Connector</u></strong><u>:</u></p><ul><li><p>In the Pagerduty console Navigate to Services &rarr; services directory and select your service where the alerts needs to be sent.</p></li><li><p> In the below page click on add integration and then choose <code>Events Api v2</code> , and then click on create, New integration key will be shown in the dashboard, copy that integration key.<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"771\" ac:original-width=\"1494\" ac:custom-width=\"true\" ac:alt=\"image-20240125-025743.png\" ac:width=\"712\"><ri:attachment ri:filename=\"image-20240125-025743.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>For creating connectors while creating alerts in Elastic search Click on Pagerduty connector and enter the connector name as per your need and paste the integration key and then click on save.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"508\" ac:original-width=\"705\" ac:custom-width=\"true\" ac:alt=\"image-20240125-042349.png\" ac:width=\"705\"><ri:attachment ri:filename=\"image-20240125-042349.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>Now in the actions, configure the necessary fields for your pagerduty alerts based on your preference and click on save</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"668\" ac:original-width=\"620\" ac:custom-width=\"true\" ac:alt=\"image-20240125-042617.png\" ac:width=\"620\"><ri:attachment ri:filename=\"image-20240125-042617.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>What to do to test if my pagerduty connector is configured correctly?<br /><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":hugging:\" ac:emoji-id=\"1f917\" ac:emoji-fallback=\"🤗\" /> Fortunately we have a way to do that..</p><ul><li><p>For this Navigate to Management &rarr; Connectors and there click on run option</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"509\" ac:original-width=\"1247\" ac:custom-width=\"true\" ac:alt=\"image-20240125-042927.png\" ac:width=\"688\"><ri:attachment ri:filename=\"image-20240125-042927.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>Now fill out some details and then click on run option</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"738\" ac:original-width=\"750\" ac:custom-width=\"true\" ac:alt=\"image-20240125-043107.png\" ac:width=\"688\"><ri:attachment ri:filename=\"image-20240125-043107.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>Now there will be an alert triggered in PD<ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":no_mouth:\" ac:emoji-id=\"1f636\" ac:emoji-fallback=\"😶\" /> </p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"385\" ac:original-width=\"1451\" ac:custom-width=\"true\" ac:alt=\"image-20240125-043204.png\" ac:width=\"688\"><ri:attachment ri:filename=\"image-20240125-043204.png\" ri:version-at-save=\"1\" /></ac:image><p /></li></ul></li></ul></li><li><p><strong><u>Slack Connector:</u></strong></p><ul><li><p>For slack connector you just need to provide the slack&rsquo;s webhook url and then configure the necessary fields as per your preference and click on save. For testing you can actually follow the same steps as PD.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"503\" ac:original-width=\"690\" ac:custom-width=\"true\" ac:alt=\"image-20240125-043432.png\" ac:width=\"690\"><ri:attachment ri:filename=\"image-20240125-043432.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>How can i get my slacks webhook url??<a href=\"https://api.slack.com/messaging/webhooks\">Configure slack webhooks</a></p></li></ul></li></ul>",
        "representation": "storage",
        "word_count": 551
      },
      "version": {
        "number": 2,
        "when": "2024-01-25T04:38:35.158Z",
        "by": "Naveen Vellingiri"
      },
      "labels": []
    },
    {
      "id": "3122135141",
      "title": "VSS RUNBOOK",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3122135141",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<ol start=\"1\"><li><p>Consumers Pause/Resume is not working on GCP. The API is working fine and on mq-ui the active flag is getting changes to false but consumers are still consuming the data. (DEBUGGING IS NEEDED)</p></li><li><p>Right Now we are triggering GCP Admin via our AWS databricks job so that both AWS and GCP Models remain in sync.</p><ol start=\"1\"><li><p>When we start resetting model from GCP databricks via the GCS path. we have to trigger AWS admin via GCP Databricks to again keep the models in sync.</p></li><li><p>Change all the training_data_paths to GCS paths in ZK</p></li><li><p>I have made all the changes in the training-path-vss-embedding-processor (This can be made even more robust)<br /><a href=\"https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#notebook/2782671666542775/command/3723062291511625\">https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#notebook/2782671666542775/command/3723062291511625</a><br /><strong>TESTING IS STILL PENDING</strong></p></li></ol></li><li><p>Whichever model we are starting in GCP, corresponding model job should be paused in AWS. (As we are using the same kafka-topics here)</p></li><li><p>initial-model-ingestion-script and manual-ingestion-with-payload still uses kafka client instead of mq. Hence dev is needed here to shift these jobs to mq as well.</p></li></ol><p><strong>LINKS TO DATABRICKS JOBS</strong></p><p>Initial Onboarding: <a href=\"https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#job/254236390936049/tasks/task/initial-onboarding\">https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#job/254236390936049/tasks/task/initial-onboarding</a></p><p>Initial Model ingestion: <a href=\"https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#job/400576193703118\">https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#job/400576193703118</a></p><p>Training Path: <a href=\"https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#job/892602992592483\">https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#job/892602992592483</a></p><p>(Please add duplicate task and change Tenant and model_name for which you want to process)<br /></p><ol start=\"5\"><li><p>Cluster Authentication on Scylla is needed to be Added. Please follow the following process when Authentication is going to be enabled</p><ol start=\"1\"><li><p>Pause all the consumers in GCP Environment.</p></li><li><p>Enable Scylla Cluster Authentication (Also <strong>Cluster needs to be Scaled to 3x</strong>. We can take both of these activities together)</p></li><li><p>We have to do development in vss-gateway (gcp-main branch) to add code for password authentication. </p></li><li><p>Deploy the newer code and test for sanity. (you can do sanity via read clusters as well)</p></li><li><p>Once sanity is done, Resume the Consumers</p></li></ol></li></ol><p /><p><strong>IMPORTANT LINKS</strong></p><p>VSS READ API: <a href=\"https://grafana.meesho.com/d/VfcGv7aVl/vss-platform-v3?orgId=1&amp;refresh=5s&amp;var-service=vss-service&amp;from=now-12h&amp;to=now\">https://grafana.meesho.com/d/VfcGv7aVl/vss-platform-v3?orgId=1&amp;refresh=5s&amp;var-service=vss-service&amp;from=now-12h&amp;to=now</a></p><p>VSS CONSUMERS: <a href=\"https://grafana.meesho.com/d/dc70df17-69d2-4874-8315-db8110f53441/vss-consumers?orgId=1&amp;refresh=30s\">https://grafana.meesho.com/d/dc70df17-69d2-4874-8315-db8110f53441/vss-consumers?orgId=1&amp;refresh=30s</a><br />QDRANT METRICS: <a href=\"https://grafana.meesho.com/d/VfcGv7aVQ/qdrant-metrics?orgId=1&amp;from=now-1h&amp;to=now&amp;refresh=30s\">https://grafana.meesho.com/d/VfcGv7aVQ/qdrant-metrics?orgId=1&amp;from=now-1h&amp;to=now&amp;refresh=30s</a></p>",
        "representation": "storage",
        "word_count": 281
      },
      "version": {
        "number": 6,
        "when": "2024-01-11T07:59:34.979Z",
        "by": "Aditya Kumar Garg"
      },
      "labels": []
    },
    {
      "id": "3119513643",
      "title": "Feature Store RunBook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3119513643",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<ol start=\"1\"><li><p>if you get any error after onboarding feature (during read flow)<br />-&gt; probably you haven't restarted feature-store clusters<br />-&gt; online-feature-store-v4, online-feature-store-v4-mp, online-feature-store-v4-secondary (Restart all 3 of these) (Both GCP,AWS)</p></li><li><p>While onboarding any feature in feature-store AWS, Please onboard the same in GCP as well.<br />AWS Host: <a href=\"http://bac-p-o-feature-store-admin.meeshoint.in\">bac-p-o-feature-store-admin.meeshoint.in</a><br />GCP Host: online-feature-store-admin.prd.meesho.int</p></li><li><p>On onboarding any new Entity, you have to create the table if you are onboarding in scylla:</p></li></ol><p>Examples:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"76b39c05-58b2-4dcf-8da3-999d1da6e549\"><ac:plain-text-body><![CDATA[CREATE TABLE feature_store_v2.catalog_features_wide_table_2 (\n    catalog_id text PRIMARY KEY,\n    seg_0 text,\n    seg_1 text,\n    seg_2 text\n) WITH bloom_filter_fp_chance = 0.01\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'ALL'}\n    AND comment = ''\n    AND compaction = {'class': 'SizeTieredCompactionStrategy'}\n    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n    AND crc_check_chance = 1.0\n    AND dclocal_read_repair_chance = 0.0\n    AND default_time_to_live = 604800\n    AND gc_grace_seconds = 864000\n    AND max_index_interval = 2048\n    AND memtable_flush_period_in_ms = 0\n    AND min_index_interval = 128\n    AND read_repair_chance = 0.0\n    AND speculative_retry = '99.0PERCENTILE';]]></ac:plain-text-body></ac:structured-macro><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d2b336fc-55ac-4a34-b6d4-52fe8161241f\"><ac:plain-text-body><![CDATA[CREATE TABLE feature_store_v2.query_catalog_organic_features_wide_table (\n    query text,\n    catalog_id text,\n    seg_0 text,\n    seg_1 text,\n    seg_2 text,\n    PRIMARY KEY ((query, catalog_id))\n) WITH bloom_filter_fp_chance = 0.01\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'ALL'}\n    AND comment = ''\n    AND compaction = {'class': 'SizeTieredCompactionStrategy'}\n    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n    AND crc_check_chance = 1.0\n    AND dclocal_read_repair_chance = 0.0\n    AND default_time_to_live = 2630000\n    AND gc_grace_seconds = 864000\n    AND max_index_interval = 2048\n    AND memtable_flush_period_in_ms = 0\n    AND min_index_interval = 128\n    AND read_repair_chance = 0.0\n    AND speculative_retry = '99.0PERCENTILE';]]></ac:plain-text-body></ac:structured-macro><ol start=\"4\"><li><p>Onboarding a new feature-group you have to add the corresponding column in scylla in the correct table:</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"cc028b69-c7ff-4771-8b6e-cf772757ec13\"><ac:plain-text-body><![CDATA[Alter table <table_name> add <column_name> text;]]></ac:plain-text-body></ac:structured-macro><ol start=\"5\"><li><p>Please <strong>do not</strong> onboard anything to <strong>Cassandra</strong> as GCP <strong>does not</strong> support Cassandra. Onboard new entities to scylla or Redis only. <ac:link><ri:user ri:account-id=\"6320b1d3ed8abffd7ffcaca0\" /></ac:link> has idea when to onboard to Scylla/Redis</p></li><li><p>After onboarding is done, please restart all feature-store read clusters.</p></li><li><p>AWS</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"8a81c504-9a18-4a6d-853c-16bfd45c3c48\"><ac:plain-text-body><![CDATA[ssh -i ~/.ssh/prod.pem ubuntu@172.31.104.174\ncqlsh 172.31.104.174\nuse feature_store_v2;]]></ac:plain-text-body></ac:structured-macro><p style=\"margin-left: 30.0px;\">GCP</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"39adfcfd-0f1f-47b6-8483-eb30f26ebddb\"><ac:plain-text-body><![CDATA[gcloud compute ssh --zone \"asia-southeast1-a\" \"scyl-dsci-mlp-fs1-prd-ase1\" --tunnel-through-iap --project \"meesho-datascience-prd-0622\"\ncqlsh 10.138.64.7\nuse feature_store_v2;]]></ac:plain-text-body></ac:structured-macro><p><strong>Feature Store GCP Migration Pending Tasks</strong></p><ol start=\"1\"><li><p>Real Time Feature Processors -&gt; Not yet deployed on GCP (Backfilling + Rebase of rating, int_id_mapping feature groups)<br /><strong>update: Deployed on GCP and consumers/producers are running in GCP and paused in AWS</strong></p></li><li><p>As GCP will have GCS paths instead of S3, we need to run a Databricks script to convert all the paths from S3 to GCS for offline-feature-store mapping.<br />2.1. <strong>Mapping Sheet</strong> -&gt; <a href=\"https://docs.google.com/spreadsheets/d/1JGeUq7X_FPNri7gtPYDU2qYfh9WcYJVYOHCPcPy356Q/edit#gid=1134787532\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1JGeUq7X_FPNri7gtPYDU2qYfh9WcYJVYOHCPcPy356Q/edit#gid=1134787532</a> <br />2.2. <strong>Databricks Job</strong> -&gt; <a href=\"https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#notebook/636929337568690/command/636929337568691\">https://2516183257845181.1.gcp.databricks.com/?o=2516183257845181#notebook/636929337568690/command/636929337568691</a></p><ol start=\"1\"><li><p><code>s3a://</code>&nbsp; will be replaced by <code>gs://</code>&nbsp;<br /><strong>update: This is done</strong></p></li></ol></li><li><p>Feature Group Ingestion Job Owner List (can contact specific POC for bootstrap)<br />-&gt; <a href=\"https://docs.google.com/spreadsheets/d/1uS3kWOARmQHD82ngeS0GHQha-PjR2haGxMbxFIX5FUQ/edit#gid=782303163\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1uS3kWOARmQHD82ngeS0GHQha-PjR2haGxMbxFIX5FUQ/edit#gid=782303163</a> </p></li><li><p>As we have single topic in both AWS and GCP, if we are starting consumers in GCP, we should ideally pause the same in AWS or else the topic will get 2*data.</p></li><li><p>Scylla Authentication needs to be enabled on GCP. For this we have dev effort to add authentication in code</p><ol start=\"1\"><li><p>Please add password authentication in both fs-gateway (java) (gcp-master and gcp-main branches) and feature-store (golang) (gcp-main) branch.</p></li><li><p>Pause all the consumers when scylla authentication is getting enabled.</p></li><li><p>Deploy new code with password authentication enabled and if all is succesfully then resume all the consumers (alpha, beta, gamma, alpha-v2, beta-v2, gamma-v2)</p></li></ol></li></ol><p>IMPORTANT LINKS<br />1. CONSUMERS</p><p><a href=\"https://grafana.meesho.com/d/KzuiJ4m4L/feature-store-consumer?orgId=1&amp;refresh=5s&amp;var-instance=100.64.%2A&amp;var-service=online-feature-store-consumer-beta-v2\">https://grafana.meesho.com/d/KzuiJ4m4L/feature-store-consumer?orgId=1&amp;refresh=5s&amp;var-instance=100.64.*&amp;var-service=online-feature-store-consumer-beta-v2</a></p><ol start=\"2\"><li><p>READ</p></li></ol><p><a href=\"https://grafana.meesho.com/d/jA-iyAY4O/feature-store-v4?orgId=1&amp;refresh=30s&amp;from=now-3h&amp;to=now&amp;var-service=online-feature-store-read-v4\">https://grafana.meesho.com/d/jA-iyAY4O/feature-store-v4?orgId=1&amp;refresh=30s&amp;from=now-3h&amp;to=now&amp;var-service=online-feature-store-read-v4</a></p>",
        "representation": "storage",
        "word_count": 526
      },
      "version": {
        "number": 6,
        "when": "2024-01-17T12:05:48.184Z",
        "by": "Mohit Kumar"
      },
      "labels": []
    },
    {
      "id": "3091038282",
      "title": "CICD Runbook for GCP",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3091038282",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<ac:adf-extension><ac:adf-node type=\"panel\"><ac:adf-attribute key=\"panel-type\">note</ac:adf-attribute><ac:adf-content><ul><li><p>This runbook mostly guides through the initial deployment phases. Most of the activities in this runbook are one time tasks.</p></li><li><p>This doc is intended for both Service Owners and DevOps POCs because initial deployments will be a joint effort.</p></li><li><p>Some activities are on service owners, and some are to be owned by DevOps POCs.</p></li><li><p>In its current state, this doc is mostly <strong><u>relevant for services which are already onboarded to EKS</u></strong>. For new services, DevOps POCs can support with adhoc steps.</p></li></ul></ac:adf-content></ac:adf-node><ac:adf-fallback><div class=\"panel conf-macro output-block\" style=\"background-color: rgb(234,230,255);border-color: rgb(153,141,217);border-width: 1.0px;\"><div class=\"panelContent\" style=\"background-color: rgb(234,230,255);\">\n<ul><li><p>This runbook mostly guides through the initial deployment phases. Most of the activities in this runbook are one time tasks.</p></li><li><p>This doc is intended for both Service Owners and DevOps POCs because initial deployments will be a joint effort.</p></li><li><p>Some activities are on service owners, and some are to be owned by DevOps POCs.</p></li><li><p>In its current state, this doc is mostly <strong><u>relevant for services which are already onboarded to EKS</u></strong>. For new services, DevOps POCs can support with adhoc steps.</p></li></ul>\n</div></div></ac:adf-fallback></ac:adf-extension><h2>Jenkins Links</h2><ul><li><p>dev: <a href=\"https://jenkins-dev.meeshogcp.in\">https://jenkins-dev.meeshogcp.in</a></p></li><li><p>prd: <a href=\"https://jenkins-prd.meeshogcp.in\">https://jenkins-prd.meeshogcp.in</a></p></li></ul><h2>Pipeline structure and naming conventions</h2><p>In Jenkins, all pipelines are structure by BU and repos. Under the appropriate BU, create a Jenkins job by the name <code>{reponame}-cicd</code>.</p><p>Example: <a href=\"http://jenkins-prd.meeshogcp.in/view/Central/job/edge-proxy-cicd/\">http://jenkins-prd.meeshogcp.in/view/Central/job/edge-proxy-cicd/</a> (Edge proxy job under Central BU)</p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"089834d0-2c6b-4662-8bb7-208ea9a7ff7f\"><ac:rich-text-body><p><strong>For application teams:</strong> Take help from your DevOps POCs to get these created if they do not already exist.</p><p><strong>For DevOps: </strong>Create the job by keeping <code>edge-proxy-cicd</code> as the base.</p></ac:rich-text-body></ac:structured-macro><h2><ac:inline-comment-marker ac:ref=\"a7e9beff-af16-4c95-9ed9-34222fb590d9\">Setting Up Things Correctly Before Deploying</ac:inline-comment-marker></h2><p>Once the Jenkins Job is ready, ensure the following before proceeding:</p><ul><li><p><code>deployment.yaml</code>: In application repositories, under the <code>deployments</code> folder, same config is read for both EKS/GKE.</p></li><li><p><code>values_properties.yaml</code>: For GCP, there is a separate <code>values_properties</code> which sits <a href=\"https://github.com/Meesho/devops-helm-charts/tree/main/values_v2\">here</a>.<br />- <em>Note that for EKS the root directory is </em><code>charts</code><em> and for GKE the root directory is </em><code>values_v2</code><br />- <em>Use </em><code>values_properties</code><em> to override any config for particular cloud. It will be given precedence over    the config present in </em><code>deployment.yaml</code></p></li><li><p><strong>Config</strong>: Ensure that configuration for GKE is correct in Vault. GCP has its own isolated Vault</p><ul><li><p>dev: <a href=\"https://vault-dev.meeshogcp.in/\">https://vault-dev.meeshogcp.in/</a></p></li><li><p>prd: <a href=\"https://vault-prd.meeshogcp.in/\">https://vault-prd.meeshogcp.in/</a></p></li></ul></li><li><p><strong>Hostnames: </strong>In GCP, all services will have their own dedicated hostnames as well as shared hostnames (if absolutely needed). Hostnames can be configured in <code>values_properties.yaml</code>. The hostnames follow convention:</p><ul><li><p><code>&lt;app-name&gt;.&lt;env&gt;.meesho.int</code> (Example: <code>ums.prd.meesho.int</code>)</p></li></ul></li><li><p><strong>NodeSelectors</strong>: Take help from DevOps POCs to correctly configure the nodeselectors. These will decide on which type and size of nodes the pods will run.</p></li><li><p><strong>Ingress Class: </strong>Based on the nature and priority of the service, choose of the three ingress classes:</p><ul><li><p><code>contour-internal-0</code> : For internal up0/sp0/cp0 services</p></li><li><p><code>contour-internal-1</code>: For all other internal services which are NOT up0/sp0/cp0</p></li><li><p><code>contour-external</code>: For all external services</p></li></ul></li><li><p><strong>Temporary Redis: </strong>Till the time of actual traffic migration, we will be using a temporary redis created for the purpose of bringing up services and testing of flows. The details for this can be found <a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3078094899/GCP+Most+Important+Links#Redis-(Temporary-for-UP0)\">here</a>. Zookeeper details are also there in the same link.</p></li></ul><h2>gRPC?</h2><p>Instructions for setting up gRPC services can be <a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3109716403\">found here</a>.</p><h2>Running the Job</h2><p>Once the initial config looks good and both applications owners as well as DevOps POCs have verified things, we can proceed with running the job. The flow remains exactly the same as it used to be in EKS world.</p><h2>ArgoCD</h2><p>After a successful run of the job, the application should be visible in ArgoCD. We have different ArgoCD depending on the BU.</p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"8322c4cf-0244-47df-be3e-1fb97367e4fb\"><colgroup><col style=\"width: 189.0px;\" /><col style=\"width: 569.0px;\" /></colgroup><tbody><tr><th><p><strong>BU</strong></p></th><th><p><strong>Link</strong></p></th></tr><tr><td><p>Central (<strong>prd</strong>)</p></td><td><p><a href=\"https://argocd-central-prd.meeshogcp.in/\">https://argocd-central-prd.meeshogcp.in/</a></p></td></tr><tr><td><p>Demand (<strong>prd</strong>)</p></td><td><p><a href=\"https://argocd-demand-prd.meeshogcp.in/\">https://argocd-demand-prd.meeshogcp.in/</a></p></td></tr><tr><td><p>Supply (<strong>prd</strong>)</p></td><td><p><a href=\"https://argocd-supply-prd.meeshogcp.in/\">https://argocd-supply-prd.meeshogcp.in/</a></p></td></tr><tr><td><p>Datascience (<strong>prd</strong>)</p></td><td><p><a href=\"https://argocd-datascience-prd.meeshogcp.in/\">https://argocd-datascience-prd.meeshogcp.in/</a></p></td></tr><tr><td><p>Dataengg (<strong>prd</strong>)</p></td><td><p><a href=\"https://argocd-dataengg-prd.meeshogcp.in/\">https://argocd-dataengg-prd.meeshogcp.in/</a></p></td></tr><tr><td><p>Farmiso (<strong>prd</strong>)</p></td><td><p><a href=\"https://argocd-farmiso-prd.meeshogcp.in/\">https://argocd-farmiso-prd.meeshogcp.in/</a></p></td></tr></tbody></table><h2>DNS Creation</h2><p>DevOps POC should create DNS for the service in the <code>prd.meesho.int</code> hosted zone in <code>meesho-admin-prd-0622</code> project. <a href=\"https://console.cloud.google.com/net-services/dns/zones/pvt-meesho-admin-prd-meesho-int/details?project=meesho-admin-prd-0622\">Direct Link</a></p><p>Records for services should be created as CNAME records with 1 minute TTL. The value of the CNAME would be the corresponding contour hostname of the form: <code>contour-internal-0-demand-prd.prd.meesho.int</code></p><p>Ensure that the corresponding contour A record exists, if not create and put the IP for the LB as the value.</p><h2>Service to Service Communication and CoreDNS</h2><p>Once the DNS created in the above step has been tested, DevOps POC should ensure that an entry gets added to the CoreDNS of all clusters to handle the traffic internally for that service.</p><p>Soon we will have a job for this, but till then just make an entry in the CoreDNS configmap. For now please add an entry here which will put the config across all coredns in all clusters: <a href=\"https://github.com/Meesho/devops-infra-helm-charts/blob/main/helm-templates/coredns/values.yaml\">https://github.com/Meesho/devops-infra-helm-charts/blob/main/helm-templates/coredns/values.yaml</a> </p><p>The format is:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d0d2645d-afc0-4179-a5d7-f33c9f267b65\"><ac:plain-text-body><![CDATA[rewrites:\n  hostname: <contour-name>\n  storefront-int-app-feed.prd.meesho.int: contour-internal-0-demand-prd]]></ac:plain-text-body></ac:structured-macro><p>Then sync all the CoreDNS.</p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"9cd1123d-09f7-4e06-b7f9-479cf688e4d5\"><ac:rich-text-body><p>This activity is to be done by DevOps POCs.</p></ac:rich-text-body></ac:structured-macro><h2>Updating Downstream Endpoints</h2><p>We had copied all the EKS Vault configs to GKE Vault. This means that all configs will be outdated and the downstream endpoints might be of the type <code>*.meeshoint.in</code>. Once a service has been deployed and GCP endpoint has been found working, it is the responsibility of the service owners to get their new endpoints updated in all the upstreams. We have created Jenkins job for this which replaces a substring in all the config of all services in one go. <a href=\"https://jenkins-infra.meeshogcp.in/job/gcp-vault-config-update-job/\">Job Link</a> </p><p>Please take help from DevOps POCs to run this job.</p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"c3465bb8-4313-4a87-9b6d-4b90fdb938d3\"><ac:rich-text-body><p>Once the substring replacement job has been run with the help of DevOps POCs, it is up to the service owners to ensure that their upstream/downstream endpoints are pointing to GCP hostnames (<code>*.prd.meesho.int)</code> and <strong>NOT</strong> AWS hostnames <code>*.meeshoint.in</code></p></ac:rich-text-body></ac:structured-macro><h2>Accessing GCP and AWS Services</h2><p>Unlike EKS, things will change slightly on how we access GCP and AWS services (like S3, GCS, KMS etc.).</p><ul><li><p><strong>For GCP</strong>:  (GCS, Secret Manager, KMS etc.)</p><ul><li><p>Get a Service Account for your service/team created on GCP.</p></li><li><p>Ensure the Service Account has correct set of permissions. Keep it fine-grained.</p></li><li><p>With the help of your DevOPs POCs, set up workload identity (WI) for your service. (DevOps POCs please utilise the Jenkins job or CLI to setup Workload Identity)</p></li><li><p>Once, WI has been setup, remove the <code>GOOGLE_APPLICATION_CREDENTIALS</code>config from Vault as it is not needed with WI and taken care of automatically.</p></li></ul></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"e23f59df-9859-48b3-b2a6-758eff25ec05\"><ac:parameter ac:name=\"title\">Workload Identity Setup Instructions for DevOps POCs</ac:parameter><ac:rich-text-body><p>Job link: <a href=\"https://jenkins-infra.admin.meeshogcp.in/job/workload-identity-KSA-binding-job/\">https://jenkins-infra.admin.meeshogcp.in/job/workload-identity-KSA-binding-job/</a></p><ul><li><p>Create a GCP Service Account for the service or team.</p></li><li><p>Grant the permissions to this service account by confirming with team.</p></li><li><p>Run the job mentioned above to perform the binding.</p></li><li><p>Add annotation to GKE Service Account. To do that add the following to <code>values_properties.yaml</code></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"028d3216-2f95-4781-b2fb-c53b417ee83c\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[serviceAccount:\n    annotations:\n        iam.gke.io/gcp-service-account: <GCP SA Email>]]></ac:plain-text-body></ac:structured-macro></li></ul><p>For Example:<br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"e7072e23-ddc8-44fe-af48-e82ab2c1baa4\"><ac:plain-text-body><![CDATA[serviceAccount:\n    enabled: true\n    annotations:\n        iam.gke.io/gcp-service-account: sa-demand-prd-wallet-web@meesho-demand-prd-0622.iam.gserviceaccount.com]]></ac:plain-text-body></ac:structured-macro><ul><li><p>For support related to job and WI, contact <ac:link><ri:user ri:account-id=\"61c95b057c6f980070d8c4ec\" /></ac:link> <br />For more info - <a href=\"https://docs.google.com/document/d/1Fv7lkLJeiNrZVvxgH80tzx4t5ggCa_u466TlcAlf5Kw/edit\">Docs</a>: </p></li></ul></ac:rich-text-body></ac:structured-macro><p /><ul><li><p><strong>For AWS: </strong>(S3, KMS etc.)</p><ul><li><p>We have put setup for IAM roles for anywhere in the pods directly, so things should continue working seamlessly.</p></li><li><p>If you still face issues, ensure with the help of DevOps POCs that the IAM Role For Anywhere role has the services required by the DevOps teams.</p></li></ul></li></ul><p />",
        "representation": "storage",
        "word_count": 1100
      },
      "version": {
        "number": 21,
        "when": "2024-07-23T04:43:32.205Z",
        "by": "Mahak Jain"
      },
      "labels": [
        "i7redsw512a-"
      ]
    },
    {
      "id": "3087532075",
      "title": "Search Oncall Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3087532075",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h3>ES CPU choked</h3><p>Do this iteratively and measure impact of each.</p><ul><li><p>stop gst partial indexer - <a href=\"https://prod-ops-argocd.meesho.com/applications/prd-text-search-partial-catalogs-elastic-indexer?resource=\">service</a></p></li><li><p>stop non gst partial indexer - <a href=\"https://prod-ops-argocd.meesho.com/applications/prd-ts-partial-catalogs-elastic-indexer-non-gst?resource=\">service</a></p></li><li><p>stop gst main indexer - <a href=\"https://prod-ops-argocd.meesho.com/applications/prd-text-search-catalogs-elastic-indexer?resource=\">service</a></p></li><li><p>stop non gst main indexer - <a href=\"https://prod-ops-argocd.meesho.com/applications/prd-text-search-catalogs-elastic-indexer-non-gst?resource=\">service</a></p></li><li><p>increase redis ttl at qwest/iacg - whichever handles the ES requests at that time. <strong>Check for redis storage before increasing this</strong>.</p></li></ul><h3>What to do if indexers are not working ?</h3><ul><li><p>check pod logs - this is usually an issue with pod going OOM</p></li><li><p>restart the service from UI and check if consumption starts</p></li><li><p>if issue still persists, decrease poll size and concurrency, in that order</p></li><li><p>do NOT restart multiple times continuously, this will cause consumers to be in rebalancing state and consumption won&rsquo;t start.</p></li><li><p>on restarting, if there is a lag buildup, main indexers will autoscale and indexing rate on ES will increase. Limit the max count if traffic is high/ es is getting choked</p></li></ul><h3>What to do if OOS has increased ?</h3><ul><li><p>check lag in main indexers</p></li><li><p>for indexer issue &rarr; see `What to do if indexers are not working?` section</p></li><li><p>check if catalog feed dump cron ran fine the previous day</p></li></ul><h3>What to do if qdrant is impacted ?</h3><ul><li><p>Start with scaling ES / fallback.</p></li><li><p>Report the issue to MLP team.</p></li><li><p>Circuit breaker will trigger and stop calling IACG, so before this happens make sure ES got scaled.</p></li></ul><h3>What to do if circuits are not closed automatically ?</h3><ul><li><p>Go to following path on zookeeper <code>/config/{rx-feed-aggregation/search-orchestrator/or whatever is the app name}/cb-overrides/</code></p></li><li><p>Inside <code>serviceCBStateOverrideMap</code>, add the service name with CB state as the value, Eg:</p><ul><li><p>searchIOP &rarr; RESET_CB</p></li></ul></li><li><p>The following values can be used as CB states</p><ul><li><p>FORCE_OPEN_CB, FORCE_CLOSE_CB or RESET_CB</p></li></ul></li><li><p>Once the service name, cb state mapping is added, change the value of <code>enableCBOverride</code> to <code>true</code>. </p></li><li><p>a readMe file is also present in the same path on how to do this.</p></li></ul><h3>What to do if catalog cron dump has failed ?</h3><ul><li><p>check if we have data or not in redis as ttl is 2 days and take decision accordingly to run the cron or not</p></li><li><p>check why the cron has failed and start fixing it</p></li><li><p>steps to follow If we want to run the cron at any cost </p><ul><li><p>change the schedule here to trigger the run for the time you wanted and change back the trigger to normal time after done - <a href=\"https://prod-ops-argocd.meesho.com/applications/prd-offline-cg-scheduler?node=batch%2FCronJob%2Fprd-offline-cg-scheduler%2Fprd-offline-cg-scheduler-searchfeeddump%2F0\">link</a></p></li><li><p>before triggering the cron see if the dependencies like Text search ES, qwest-int and spp-int can take load or not and scale accordingly or change the speed of the cron</p></li><li><p>refer this to configure the speed of cron - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Configuring the Processing Speed of Catalog Dump Cron\" ri:version-at-save=\"1\" /><ac:link-body>Configuring the Processing Speed of Catalog Dump Cron</ac:link-body></ac:link> </p></li></ul></li></ul>",
        "representation": "storage",
        "word_count": 422
      },
      "version": {
        "number": 6,
        "when": "2023-12-06T06:15:08.867Z",
        "by": "Gajulapalli Sai Dheeraj"
      },
      "labels": []
    },
    {
      "id": "3085797030",
      "title": "Meecom Oncall Runbook: 27 Nov to 04 Dec, 2023",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3085797030",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>On Call : <ac:link><ri:user ri:account-id=\"60e5328ac0ea290069c4488d\" /></ac:link> </p><p>On Call Dashboard: <a href=\"https://meesho.atlassian.net/jira/software/c/projects/HER/boards/318\"><u>Habit, Engagement and Resurrection</u></a></p><p /><table data-table-width=\"1032\" data-layout=\"default\" ac:local-id=\"76c60629-ae2e-4736-b1d1-7538e431915d\"><colgroup><col style=\"width: 48.0px;\" /><col style=\"width: 243.0px;\" /><col style=\"width: 127.0px;\" /><col style=\"width: 354.0px;\" /><col style=\"width: 163.0px;\" /><col style=\"width: 97.0px;\" /></colgroup><tbody><tr><th><p>&nbsp;</p></th><th><p><strong>Alert</strong></p></th><th><p><strong>Alert date</strong></p></th><th><p><strong>Alert report</strong></p></th><th><p><strong>Action</strong></p></th><th><p>&nbsp;</p></th></tr><tr><td><p>1</p></td><td><p>In app campaigns were not processed</p></td><td><p>28th Nov</p></td><td><p>There was typo in Aws region.So files were not getting downloaded thats why campaigns stuck in scheduled state. 2 in app campaigns affected by this.</p></td><td><p>fixed the typo</p></td><td><p /></td></tr><tr><td><p>2</p></td><td><p>Drop in PN sent count</p></td><td><p>28th Nov</p></td><td><p>Got the alert for PN sent count drop at 11:00 Pm. The reason for this is day before yesterday we did a deployment for mq migration keeping consumers latest but with different group id. And it was picking the events from earliest ao that number of PNs sent was little high which leads to drop in PN sent count next day as compared to previous day. </p></td><td><p>Changed the group id as same and keep listen from earliest.</p></td><td><p /></td></tr><tr><td><p>3</p></td><td><p>Whatsapp Login issue</p></td><td><p>29th Nov</p></td><td><p>This was raised by ios team on 29th November. Bust it started occurring since 10th Nov. For ios users whatsapp logon was not working and we were also not getting any error from them. We found that issue was with pricing.<br />Discussion thread : <a href=\"https://meesho.slack.com/archives/C02NJ834K8C/p1701064626718119\" data-card-appearance=\"inline\">https://meesho.slack.com/archives/C02NJ834K8C/p1701064626718119</a> </p></td><td><p>After clearing the payment it got sorted.</p></td><td><p>&nbsp;</p></td></tr><tr><td><p>4</p></td><td><p>5xx in all the apis on communicator</p></td><td><p>30th Nov 01:10 AM</p></td><td><p>It happened due to an incorrect update in the zookeeper config(topic-&gt; mqId map is kept in zookeeper, there was a missing&nbsp; `:` while updating the same)<br />This change in config is required for Mq deployment.</p></td><td><p>&nbsp;Mitigated after updating zookeeper.</p></td><td><p>&nbsp;</p></td></tr><tr><td><p>5</p></td><td><p>Surge in 4xx in send apis</p></td><td><p>30th Nov </p></td><td><p>Was getting Duplicate Request ids. It happened exactly for one day when the issue happened before night. May be upstream services were retrying. <br /></p></td><td><p>It got mitigated by its own in one day.</p></td><td><p>&nbsp;</p></td></tr><tr><td><p>6</p></td><td><p>pull fy/clp process failure</p></td><td><p>30th Nov</p></td><td><p><code>cas.pull_notification.meta_data</code> events needs to be pushed to communicator kafka, but while migrating to mq we pushed it to shared kafka which leads to this issue.</p></td><td><p>Changing the broker fixed this.</p></td><td><p /></td></tr><tr><td><p>7</p></td><td><p>5xx in notification store</p></td><td><p>Intermittent</p></td><td><p /></td><td><p>Currently threshold is very low, we can increase the threshold.</p></td><td><p /></td></tr><tr><td><p>8</p></td><td><p>Gupshup sms report consumer failure</p></td><td><p>1st dec, 12:30 AM</p></td><td><p>Reason for this is while migrating the mq gateway team changed the message by mistake for the gupshup.sms_callback_report which leads to failure in consumer. When we deployed our change we found out the issue that message is wrong.</p></td><td><p>Reverting gateway changes fixed it.</p></td><td><p /></td></tr><tr><td><p>9</p></td><td><p>Alerts</p></td><td><p>&nbsp;</p></td><td><p><a href=\"https://meesho.pagerduty.com/incidents/Q0CE982FROJO7N?utm_campaign=channel&amp;utm_source=slack\"><u>prd-cas-catalog-prefetch-consumer</u></a> memory limit breach 80%. </p><p>fcm-latency breach alert</p></td><td><p>Action for fcm-latency</p><p>monitor the lag in deliver consumers.</p><p>If we are getting multiple alert and nothing is breaking then we can silent this.</p></td><td><p>&nbsp;</p></td></tr></tbody></table><p>PD Alert Report : <a href=\"https://meesho.pagerduty.com/reports#system?since=2023-11-27T00%3A00%3A00&amp;until=2023-12-03T23%3A59%3A59&amp;grouped_by%5B%5D=service&amp;grouped_by%5B%5D=weekly&amp;time_zone=Asia%2FKolkata\">https://meesho.pagerduty.com/reports#system?since=2023-11-27T00%3A00%3A00&amp;until=2023-12-03T23%3A59%3A59&amp;grouped_by[]=service&amp;grouped_by[]=weekly&amp;time_zone=Asia%2FKolkata</a></p>",
        "representation": "storage",
        "word_count": 424
      },
      "version": {
        "number": 4,
        "when": "2023-12-04T11:57:49.743Z",
        "by": "Bipul Kumar"
      },
      "labels": []
    },
    {
      "id": "3035332618",
      "title": "Mega Blockbuster Sale Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3035332618",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p><strong>On-call Roster: </strong><a href=\"https://docs.google.com/spreadsheets/d/1-N4H1Og6rxAxlI94vlBuiNWEJXjG0v56x6Ta8KHAKHU/edit#gid=0\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1-N4H1Og6rxAxlI94vlBuiNWEJXjG0v56x6Ta8KHAKHU/edit#gid=0</a> </p><h2><u>Dashboard Links</u></h2><p><strong>System Monitoring Dashboards: </strong><a href=\"https://grafana.meesho.com/d/kSwUPj_Vz/oncall-montioring-consolidated?orgId=1&amp;from=now-6h&amp;to=now&amp;refresh=5m\">https://grafana.meesho.com/d/kSwUPj_Vz/oncall-montioring-consolidated?orgId=1&amp;from=now-6h&amp;to=now&amp;refresh=5m</a></p><p><strong>Business Metrics Monitoring Dashboards: </strong><a href=\"https://grafana.meesho.com/d/f84rnmu4k/oncall-business-monitoring-consolidated?orgId=1&amp;from=now-1h&amp;to=now&amp;refresh=5m\">https://grafana.meesho.com/d/f84rnmu4k/oncall-business-monitoring-consolidated?orgId=1&amp;from=now-1h&amp;to=now&amp;refresh=5m</a></p><p><strong>Monetisation Superset Dashboard:</strong> <a href=\"https://di-prd-superset.meesho.com/superset/dashboard/104/?native_filters_key=ZUWTg6jNLzUkX-gvPlAlX-2eYRMImQx9GhwxuCTlsTpIVPAZ7ZSlk_zYyUCt-sSO\">https://di-prd-superset.meesho.com/superset/dashboard/104/?native_filters_key=ZUWTg6jNLzUkX-gvPlAlX-2eYRMImQx9GhwxuCTlsTpIVPAZ7ZSlk_zYyUCt-sSO</a></p><p><strong>Supplier Ads Serving Flow Dashboard: </strong><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1\">https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1</a></p><p><strong>System Uptime Dashboard: </strong><a href=\"https://grafana.meesho.com/d/w72igLl4k/system-uptime-stateless?orgId=1&amp;var-telegraf_service=advertisement-reco&amp;var-cluster=prd-advertisement-reco_prd-advertisement-reco-primary_80&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.99&amp;var-latency_percentile_period=5m&amp;var-latency_percentile_threshold=0.999&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d&amp;var-uri=All&amp;from=now-7d&amp;to=now\">https://grafana.meesho.com/d/w72igLl4k/system-uptime-stateless?orgId=1&amp;var-telegraf_service=advertisement-reco&amp;var-cluster=prd-advertisement-reco_prd-advertisement-reco-primary_80&amp;var-generic_period=15s&amp;var-min_error_count_rate=10&amp;var-error_count_rate_percent_threshold=0.01&amp;var-rate_period=1m&amp;var-latency_percentile=0.99&amp;var-latency_percentile_period=5m&amp;var-latency_percentile_threshold=0.999&amp;var-latency_percentile_threshold_period=24h&amp;var-latency_percentile_threshold_min_over_time=7d&amp;var-uri=All&amp;from=now-7d&amp;to=now</a></p><p><strong>Live Ad Catalogs on VSS Dashboard: </strong><a href=\"https://grafana.meesho.com/d/VfcGv7aVk/vss-platform?orgId=1&amp;refresh=30s&amp;viewPanel=25&amp;from=now-2d&amp;to=now\">https://grafana.meesho.com/d/VfcGv7aVk/vss-platform?orgId=1&amp;refresh=30s&amp;viewPanel=25&amp;from=now-2d&amp;to=now</a></p><p><strong>Other Ads Miscellaneous Dashboards: </strong><a href=\"https://grafana.meesho.com/dashboards/f/C4rVZQyMk/advertisement\">https://grafana.meesho.com/dashboards/f/C4rVZQyMk/advertisement</a></p><p /><h2><u>Redis monitoring:</u></h2><p><a href=\"https://grafana.meesho.com/d/jZWX54Nnkdup/redis-bdb-dashboard-new?orgId=1&amp;from=now-2d&amp;to=now&amp;var-cluster=c21107.ap-seast-1-mz.ec2.cloud.rlrcp.com&amp;var-bdb=11198894&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation\">https://grafana.meesho.com/d/jZWX54Nnkdup/redis-bdb-dashboard-new?orgId=1&amp;from=now-2d&amp;to=now&amp;var-cluster=c21107.ap-seast-1-mz.ec2.cloud.rlrcp.com&amp;var-bdb=11198894&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation</a></p><p>Go to above dashboard and change BDB for dashboards of different Redis clusters:</p><ul><li><p>11220460- Advertisement Serving Redis</p></li><li><p>11883529- Advertisement User Session Redis</p></li><li><p>11198894- Advertisement Views Redis</p></li></ul><p><strong>For serving feed cache go to </strong><a href=\"https://grafana.meesho.com/d/neSOwn7Vk/redis-bdb-dashboard-dbe-team?orgId=1&amp;from=now-2d&amp;to=now&amp;var-cluster=c27514.ap-seast-1-mz.ec2.cloud.rlrcp.com&amp;var-bdb=11928047&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation&amp;refresh=5s\"><strong>this</strong></a><strong> link</strong></p><p /><h2><u>Oncall Runbook:</u></h2><p>Following are the links for frequently occurring issue on supply or demand for quick resolution of any ongoing issue-</p><p><ac:link><ri:page ri:content-title=\"Supply Oncall Runbook\" ri:version-at-save=\"14\" /><ac:link-body>Supply Oncall Runbook</ac:link-body></ac:link></p><p><ac:link><ri:page ri:content-title=\"Demand Runbook\" ri:version-at-save=\"3\" /><ac:link-body>Demand Oncall Runbook</ac:link-body></ac:link></p><p /><h2><u>Opening/Closing of CB:</u></h2><p>Link to understand how to forcefully open or close CB:</p><p><a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3030089787/Force+open+closing+the+CB?atlOrigin=eyJpIjoiYTEwYmFlNWU0MjRhNDcyMmE3NGRhYjY0Y2E0NTEyOWIiLCJwIjoiY29uZmx1ZW5jZS1jaGF0cy1pbnQifQ\">Force open/closing the CB</a></p><h2><u>Kafka Operations on MQ:</u></h2><p>For any operations on Kakfa like enabling/disabling of consumer etc go through the following doc</p><p><ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Ads Kafka MSK To Confluent Migration\" ri:version-at-save=\"7\" /><ac:link-body>Ads Kafka MSK To Confluent Migration</ac:link-body></ac:link>  </p><h2><u>Log monitoring:</u></h2><h3>Coralogix</h3><p><a href=\"https://meesho.app.coralogixsg.com/#/query-new/logs?id=49jy9Qna0Ys&amp;page=0\">Coralogix</a></p><ul><li><p>Navigate to Corlogix and then Explore tab &rarr; Logs</p></li><li><p>Search your application in the application filter. For eg &ldquo;prd-advertisement-reco&rdquo;</p></li><li><p>Write the Log line you want to search for in the &ldquo;Search Logs&rdquo; Panel<br />Note: Coralogix only hosts Error logs for the past 2 days. For other log levels and for past days please use &ldquo;Athena&rdquo;</p></li></ul><h2><u>Pagerduty Open Incidents on team:</u></h2><p><a href=\"https://meesho.pagerduty.com/incidents\">https://meesho.pagerduty.com/incidents</a></p><h2><u>Ringmaster(to make changes related to ASG, pod count, etc):</u></h2><p><a href=\"https://ringmaster.meesho.com/applications/home\">https://ringmaster.meesho.com/applications/home</a></p>",
        "representation": "storage",
        "word_count": 224
      },
      "version": {
        "number": 5,
        "when": "2023-10-05T09:34:04.313Z",
        "by": "Anupam Rai"
      },
      "labels": []
    },
    {
      "id": "3035004963",
      "title": "Demand Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3035004963",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>Product Demand dashboard: <a href=\"https://metabase-presto.meesho.com/dashboard/4891-sale-hourly-trend-monetisation\">https://metabase-presto.meesho.com/dashboard/4891-sale-hourly-trend-monetisation</a><br /><a href=\"https://metabase-presto.meesho.com/question/1032-cla-ad-revenue-dod-comparison-by-hour\">https://metabase-presto.meesho.com/question/1032-cla-ad-revenue-dod-comparison-by-hour</a></p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"9fb03e4b-6ad2-422a-a538-2a4ca54ff162\"><colgroup><col style=\"width: 224.0px;\" /><col style=\"width: 396.0px;\" /><col style=\"width: 140.0px;\" /></colgroup><tbody><tr><th><p><strong>Issue</strong></p></th><th><p><strong>Resolution</strong></p></th><th><p><strong>POC</strong></p></th></tr><tr><td><p>Latency in ads-admin APIs<br />Board: <a href=\"https://grafana.meesho.com/d/q59JEw3Gk/adserver-admin?orgId=1&amp;refresh=5s&amp;from=1696485975176&amp;to=1696489575177&amp;viewPanel=2\">https://grafana.meesho.com/d/q59JEw3Gk/adserver-admin?orgId=1&amp;refresh=5s&amp;from=1696485975176&amp;to=1696489575177&amp;viewPanel=2</a></p></td><td><ol start=\"1\"><li><p><a href=\"https://grafana.meesho.com/d/q59JEw3Gk/adserver-admin?orgId=1&amp;refresh=5s&amp;from=1696486078695&amp;to=1696489678696&amp;viewPanel=19\">Taxonomy APIs latent</a></p></li></ol></td><td><p>@sg-oncall </p></td></tr><tr><td><p>Increase in data size of the scheduler due to which the scheduler starts lagging</p></td><td><ol start=\"1\"><li><p>Check for particular scheduler failure on <a href=\"https://cronitor.io/app/groups/supplier-ads-p0?env=production&amp;time=7d\">Cronitor</a>. If we are getting success of the scheduler or not.</p></li><li><p> Check for size in crease in Presto table associated with the scheduler eg: <br /><code>select created_at, count(*) from gold.smart_campaigns_age_table group by created_at</code></p></li><li><p>Check for the reason for the increased size with the product team</p></li></ol></td><td><p><ac:link><ri:user ri:account-id=\"61cd5e56bce5e00069ec947a\" /></ac:link> <ac:link><ri:user ri:account-id=\"629a004bcd636500697de4eb\" /></ac:link> </p></td></tr><tr><td><p>Error on Coralogix in the schedulers<br />SUPPLIER_CAMPAIGN_CONFIG_V3 CAMPAIGN_MANAGEMENT_RECO SMART_CAMPAIGN_AUTO_CATALOG CAMPAIGN_AGE_METRIC CAMPAIGN_CATALOG_BUDGET_FETCH SMART_CAMPAIGN_BUDGET_PRESTO SUPPLIER_CATALOG_RECOMMENDATIONS FETCH_BANNER_RECOMMENDATION FETCH_BANNER_RECOMMENDATION_CONFIG VIEW_GUARANTEE_CONFIG SUPPLIER_MMA_DETAILS_SYNC</p><p><br /></p></td><td><ol start=\"1\"><li><p>Check for the error in Coralogix</p></li><li><p>If it is the data mismatch issue, connect with product team</p></li></ol></td><td><p><ac:link><ri:user ri:account-id=\"61cd5e56bce5e00069ec947a\" /></ac:link> <ac:link><ri:user ri:account-id=\"629a004bcd636500697de4eb\" /></ac:link> </p></td></tr><tr><td><p>Issue: If there is an issue with CATALOG_METRIC_RECOMMENDATION and it does not run for some days. In the next run it is possible to get older data. </p></td><td><p>Move the offset in this particular scheduler before the next run to the current time. SO that it picks the latest data.</p></td><td><p /></td></tr><tr><td><p>Slave CPU reaching 100%</p><p /></td><td><ol start=\"1\"><li><p><a href=\"https://meesho.slack.com/archives/C020VHBCNVA/p1687848742879939\">Kill the long running queries on slave as a temporary fix</a></p></li></ol></td><td><p>db-oncall</p></td></tr><tr><td><p>CTR Alerts during sale</p></td><td><p>Since views will be way higher on ads during sale,  CTR is expected to drop during sale day so we might need to adjust the threshold accordingly</p><p>Alert links -</p><p><a href=\"https://grafana.meesho.com/alerting/eab551b7-c0d7-40dd-bf06-ec992391fe2b/edit?returnTo=%2Fd%2Filisep27z%2Fads-alerts%3Ftab%3Dalert%26orgId%3D1%26refresh%3D5s%26editPanel%3D30\">overall CTR</a> </p><p><a href=\"https://grafana.meesho.com/alerting/adc09779-19c7-40fa-840e-86592097f871/edit?returnTo=%2Fd%2Filisep27z%2Fads-alerts%3Ftab%3Dalert%26orgId%3D1%26refresh%3D5s%26editPanel%3D39\">CLP</a></p><p><a href=\"https://grafana.meesho.com/alerting/f17e96b4-22ec-4f75-b647-7846a3a10d32/edit?returnTo=%2Fd%2Filisep27z%2Fads-alerts%3Ftab%3Dalert%26orgId%3D1%26refresh%3D5s%26editPanel%3D38\">FY</a></p><p><a href=\"https://grafana.meesho.com/alerting/e573e069-fe79-4239-aca4-beb84db43552/edit?returnTo=%2Fd%2Filisep27z%2Fads-alerts%3Ftab%3Dalert%26orgId%3D1%26refresh%3D5s%26editPanel%3D41\">SEARCH</a></p><p><a href=\"https://grafana.meesho.com/alerting/cd3dc959-7024-44ce-adf7-9ba07a2a4bae/edit?returnTo=%2Fd%2Filisep27z%2Fads-alerts%3Ftab%3Dalert%26orgId%3D1%26refresh%3D5s%26editPanel%3D40\">Collection</a></p><p>CTR is clicks/views.</p><p>Things to check when CTR drops - </p><ol start=\"1\"><li><p>lag in click events consumers in CPS (we consume clicks from ingestion kafka and then publish to internal kafka. check lag in both the topics)</p></li><li><p>If no lag is present, then check for issues at supply side which might be impacting ad-clicks</p></li></ol></td><td><p /></td></tr><tr><td><p>OOS Increase</p></td><td><p>Check If there is high lag in rti pipeline, topic : marvel.storefront_validated_catalogs (<a href=\"https://grafana.meesho.com/d/kSwUPj_Vz/oncall-montioring-consolidated?orgId=1&amp;viewPanel=314\">https://grafana.meesho.com/d/kSwUPj_Vz/oncall-montioring-consolidated?orgId=1&amp;viewPanel=314</a>), If yes, then for time being we can manually run the scheduler that updates catalog valid flag via taxonomy (please inform taxonomy for some burst traffic).<br />Use following curl :</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"59cd3a6c-5d80-425a-b8f0-ab2ca5680cd5\"><ac:plain-text-body><![CDATA[curl --location 'http://advertisement-admin-scheduler.supl.internal.meesho.co/api/v1/scheduler/start' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: hduza7tbhg' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--data '{\n\t\"scheduler_type\": \"CATALOG_ATTRIBUTES\",\n\t\"start_time\":\"0.0.0\"\n}']]></ac:plain-text-body></ac:structured-macro><p>If the lag is okay in this pipeline then we can check if rate of invalidation has increased (<a href=\"https://grafana.meesho.com/d/Z7goPuP7z/oncall-monitoring?orgId=1&amp;refresh=5s&amp;viewPanel=59\">https://grafana.meesho.com/d/Z7goPuP7z/oncall-monitoring?orgId=1&amp;refresh=5s&amp;viewPanel=59</a>), If yes the this means more catalog on platform are going OOS.</p></td><td><p /></td></tr></tbody></table>",
        "representation": "storage",
        "word_count": 393
      },
      "version": {
        "number": 10,
        "when": "2023-10-05T12:19:21.726Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3033858066",
      "title": "AB Service Sale Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3033858066",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h1 style=\"text-align: center;\"><ac:structured-macro ac:name=\"anchor\" ac:schema-version=\"1\" ac:local-id=\"8799aaae-ea93-459b-adcc-6d6999428237\" ac:macro-id=\"ac0c139e-8f7c-43b4-8312-a5d529bd3ba0\"><ac:parameter ac:name=\"\">_6jksmnx543nq</ac:parameter></ac:structured-macro>AB Service Sale Runbook</h1><p /><h2><ac:structured-macro ac:name=\"anchor\" ac:schema-version=\"1\" ac:local-id=\"8fccc9d4-f3ba-4fbe-81bf-0dfcd6d0db2d\" ac:macro-id=\"1adaf13b-452e-4656-ab2c-7e14fc2ce6d8\"><ac:parameter ac:name=\"\">_5cg4x9sw2kn5</ac:parameter></ac:structured-macro>Monitoring Links:</h2><p /><p>AB service:<a href=\"https://grafana.meesho.com/d/SZkD4lpGk/ab?orgId=1&amp;from=now-2d&amp;to=now\"><span style=\"color: rgb(17,85,204);\"><u>AB Service Grafana</u></span></a><br />AB Custom Metrics: <a href=\"https://grafana.meesho.com/d/ihaPJCQ4k/ab-custom-metrics?orgId=1&amp;from=now-7d&amp;to=now\"><span style=\"color: rgb(17,85,204);\"><u>AB Custom Metrics</u></span></a><br /><br /></p><p>Audience Redis: <a href=\"https://grafana.meesho.com/d/jZWX54Nnk/bdb-dashboard?orgId=1&amp;var-cluster=c25779.ap-seast-1-mz.ec2.cloud.rlrcp.com&amp;var-bdb=11741098&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation&amp;from=now-2d&amp;to=now\"><span style=\"color: rgb(17,85,204);\"><u>Audience Redis</u></span></a></p><p>AB Hbase: <a href=\"https://grafana.meesho.com/d/etTsDBbVk/hbase-metrics?orgId=1&amp;from=now-2d&amp;to=now&amp;var-cluster=bac-p-ab-hbase&amp;var-namespace=ab&amp;var-table=user_audience_map&amp;var-region=All&amp;var-master_instance=172.31.20.29:9274&amp;var-instance=172.31.28.55:9274\"><span style=\"color: rgb(17,85,204);\"><u>AB HBase</u></span></a></p><p>AB Scylla: <a href=\"https://grafana.meesho.com/d/overview-5-1/overview?orgId=1&amp;from=now-6h&amp;to=now&amp;var-cluster=bac-p-ab-v1-scylladb&amp;var-by=cluster&amp;var-node=All&amp;var-shard=All&amp;var-mount_point=%2Fvar%2Flib%2Fscylla&amp;var-func=sum&amp;var-dash_version=5-1&amp;var-all_scyllas_versions=All&amp;var-scylla_version=5.1&amp;var-monitoring_version=4.2.1&amp;refresh=30s\"><span style=\"color: rgb(17,85,204);\"><u>AB Scylla</u></span></a></p><p><br /></p><p>EKS Infra: <a href=\"https://grafana.meesho.com/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&amp;var-service=prd-ab-worker-update&amp;var-cluster=kube-state-metrics-p-dataplatform-cluster&amp;var-namespace=prd-ab-worker-update&amp;var-Node=All&amp;var-Pod=All&amp;var-statefulset=All&amp;var-deployment=All&amp;from=now-24h&amp;to=now&amp;var-host_ip=All&amp;var-copy_of_cluster=kube-state-metrics-p-dataplatform-cluster&amp;var-copy_of_namespace=prd-ab-worker-update\"><span style=\"color: rgb(17,85,204);\"><u>EKS Infra</u></span></a></p><p>JMX Metrics: <a href=\"https://grafana.meesho.com/d/anshAi8pmz/jmx-metrics?orgId=1&amp;var-cluster=p-dataplatform-cluster&amp;var-namespace=prd-ab-worker-update&amp;var-pod=prd-ab-worker-update-549bb75f8c-zkwhb&amp;var-instance=172.27.164.249:8880&amp;from=now-7d&amp;to=now\"><span style=\"color: rgb(17,85,204);\"><u>JMX Metrics</u></span></a></p><p>Kafka MSK: <a href=\"https://grafana.meesho.com/d/JloyTAIn/msk?orgId=1&amp;var-Cluster=shared-kafk&amp;var-GroupID=entityAudienceHBaseMigrationConsumerV1&amp;var-Topic_test=All&amp;var-Topic=All&amp;from=now-1h&amp;to=now\"><span style=\"color: rgb(17,85,204);\"><u>Kafka MSK</u></span></a></p><p>Deployment Jenkins Link: <a href=\"http://cicd-prod.meeshoint.in/job/ab-service-ci/job/master/\">http://cicd-prod.meeshoint.in/job/ab-service-ci/job/master/</a></p><h2>Cloud Watch Monitoring Links:<br /></h2><ul><li><p><a href=\"https://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-1#alarmsV2:alarm/audienceEntityFetchUpdateConsumer_MaxOffsetLag?~(search~'audienceEntityFetch)\">audienceEntityFetchUpdateConsumer_MaxOffsetLag</a></p></li><li><p><a href=\"https://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-1#alarmsV2:alarm/audienceEntityFetchCreateConsumer_MaxOffsetLag?~(search~'audienceEntityFetch)\">audienceEntityFetchCreateConsumer_MaxOffsetLag</a></p></li><li><p><a href=\"https://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-1#alarmsV2:alarm/audienceEntityFetchCreateConsumer_SumOffsetLag?~(search~'audienceEntityFetch)\">audienceEntityFetchCreateConsumer_SumOffsetLag</a></p></li><li><p><a href=\"https://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-1#alarmsV2:alarm/audienceEntityFetchUpdateConsumer_SumOffsetLag?~(search~'audienceEntityFetch)\">audienceEntityFetchUpdateConsumer_SumOffsetLag</a></p></li></ul><p /><p /><h2><ac:structured-macro ac:name=\"anchor\" ac:schema-version=\"1\" ac:local-id=\"ee34716d-d673-43ad-9596-58074972d54e\" ac:macro-id=\"abeb9e5d-728f-4c99-9b05-5e5a053fb741\"><ac:parameter ac:name=\"\">_mysdo4cpfox5</ac:parameter></ac:structured-macro>Experimentation System:</h2><h4><ac:structured-macro ac:name=\"anchor\" ac:schema-version=\"1\" ac:local-id=\"1288ab11-f511-49d3-9af2-4395f097493b\" ac:macro-id=\"700f4509-96ee-4b70-804f-650216a531ee\"><ac:parameter ac:name=\"\">_s5yhj3czr3rw</ac:parameter></ac:structured-macro>Scenario: P99 Latency Spike</h4><p><span style=\"color: rgb(102,102,102);\">Solution:</span><br />If APIs are based on User to audience lookup:</p><ol start=\"1\"><li><p>Check <a href=\"https://grafana.meesho.com/d/jZWX54Nnk/bdb-dashboard?orgId=1&amp;var-cluster=c25779.ap-seast-1-mz.ec2.cloud.rlrcp.com&amp;var-bdb=11741098&amp;var-status=&amp;var-newStatus=active&amp;var-aggregation=$__auto_interval_aggregation&amp;from=now-2d&amp;to=now\"><span style=\"color: rgb(17,85,204);\"><u>Audience Redis</u></span></a> and look at evictions and used memory</p></li><li><p>HBase should be under increased load because of Redis failures.</p></li><li><p>Enable Circuit breaker from <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString%2Fredis-service-breaker\"><span style=\"color: rgb(17,85,204);\"><u>ZK config for AB service</u></span></a><span style=\"color: rgb(17,85,204);\"><u>  (</u></span><a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2F\"><span style=\"color: rgb(17,85,204);\"><u>http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2F</u></span></a><span style=\"color: rgb(17,85,204);\"><u>)</u></span></p><ol start=\"1\"><li><p>Change value of property <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FenableCBOverride\">enableCBOverride</a> from false to true. (This flag enables manual overide)</p></li><li><p>Go to <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString\">serviceCBStateOverrideMapString</a> &gt; <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString%2Fhbase-service-breaker%2F\"><u>hbase-service-breaker</u></a> and change value from DISABLED to FORCED_OPEN</p></li><li><p>Go to <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString\">serviceCBStateOverrideMapString</a> &gt; <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString%2Fredis-service-breaker\">redis-service-breaker</a> and change value from DISABLED to FORCED_OPEN</p></li></ol></li><li><p>To disable circuit breaker again once things are stable :</p><ol start=\"1\"><li><p>Go to <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString\">serviceCBStateOverrideMapString</a> &gt; <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString%2Fhbase-service-breaker%2F\"><u>hbase-service-breaker</u></a> and change value from FORCED_OPEN to DISABLED </p></li><li><p>Go to <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString\">serviceCBStateOverrideMapString</a> &gt; <a href=\"http://zk-web-config.meeshoint.in/node?path=%2Fconfig%2Fab-service%2Fcb-overrides%2FserviceCBStateOverrideMapString%2Fredis-service-breaker\">redis-service-breaker</a> and change value from FORCED_OPEN to DISABLED </p></li></ol></li></ol><p><span style=\"color: rgb(102,102,102);\">Impact of above solution:</span></p><p>With circuit breaker enabled on Redis, all request will get the default response of &ldquo;&rdquo; from Redis. This means the given user_id is not part of any audience. Hence no user will be seeing any Experiment treatment and also will not be seeing any widget groups.</p><p /><p /><p /><p /><h2><ac:structured-macro ac:name=\"anchor\" ac:schema-version=\"1\" ac:local-id=\"9f0bc9f9-0f98-4bc5-a490-e2dcc58127c4\" ac:macro-id=\"b34bc368-21dc-4f26-82da-80978e9c7787\"><ac:parameter ac:name=\"\">_dan06br7fiwq</ac:parameter></ac:structured-macro>Audience System:</h2><p><span style=\"color: rgb(102,102,102);\">Scenario</span>: Prioritizing an Audience</p><p>In case of PNs and similar cases, users might be creating an audience a few hours before the sale starts (should be 24 hours). We must prioritize refresh OR creation of these audiences over others.<br /></p><p><strong>Method-1</strong></p><p><strong>Important curls for prioritising/refreshing automatic audiences:Prioritising a Manual/Automatic Audience which is already in queue:</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c3d38c82-5fc9-40e9-a9d4-a4719aa9892a\"><ac:plain-text-body><![CDATA[curl --location --request POST 'http://bac-p-ab-worker.meeshoint.in/v1/audience/prioritise/18304' \\\n--header 'Authorization: 7Sk6T3eEHrTIt2P' \\\n--header 'Content-Type: application/json']]></ac:plain-text-body></ac:structured-macro><p><strong>Hard Refresh Automatic Audience:</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"25c1b81b-ed2f-4935-8fba-3e8bf38e5c7f\"><ac:plain-text-body><![CDATA[curl --location 'http://172.27.189.19:8010/v1/audience/hard_refresh' \\\n--header 'Authorization: 7Sk6T3eEHrTIt2P' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"audience_ids\": [110148]\n}']]></ac:plain-text-body></ac:structured-macro><p>Note: Please make sure to run the <strong>Hard Refresh Automatic Audience </strong>curl onto update worker ip only, with port 8010. Do not run hard refresh curl on create worker or anywhere else.</p><p><strong>Method-2</strong></p><p><br /><span style=\"color: rgb(102,102,102);\">Steps:</span></p><ol start=\"1\"><li><p>Determine where audience is currently:</p><ol start=\"1\"><li><p>Table audience_refresh_stages contains this info. Use following query to figure out the stage of refresh: </p></li></ol></li></ol><p style=\"margin-left: 60.0px;\">select * from audience_refresh_stages</p><p style=\"margin-left: 60.0px;\">where audience_id=91484</p><p style=\"margin-left: 60.0px;\">order by id desc</p><p style=\"margin-left: 60.0px;\">limit 1</p><p>If there is no entry if it is newly created, we can skip the kafka queue and hit following CURL API:</p><p style=\"margin-left: 90.0px;\">curl --location --request POST 'http://172.27.7.90:8010/v1/audience/hard_refresh' \\</p><p style=\"margin-left: 90.0px;\">--header 'Authorization: 7Sk6T3eEHrTIt2P' \\</p><p style=\"margin-left: 90.0px;\">--header 'Content-Type: application/json' \\</p><p style=\"margin-left: 90.0px;\">--data-raw '{</p><p style=\"margin-left: 90.0px;\">    &quot;audience_ids&quot;: [&lt;audience_ids comma separated&gt;]</p><p style=\"margin-left: 90.0px;\">}'</p><p /><ol start=\"1\"><li><p>If entry is present and :</p><ol start=\"1\"><li><p>If stage &lt; 4, it is not present in the redis queue yet.</p></li><li><p>If stage = 4, it is in redis queue, yet to be picked up</p></li><li><p>If stage &gt; 4, then the audience is being refreshed by an async worker.</p></li></ol></li><li><p>If stage &lt; 4:</p><ol start=\"1\"><li><p>You can do step 1.b</p></li></ol></li><li><p>If stage=4</p><ol start=\"1\"><li><p>Login to redis cluster using redis-cli -c -h &lt;host&gt; -p 18527 -a &lt;auth_key&gt;</p></li><li><p>Run command : lrange {0}.entity_larger_audience_update 0 -1<br />You need to replace queue name with one of the following according to update vs create and audience_size &gt; or &lt; 10M:</p><ol start=\"1\"><li><p>{0}.entity_audience_update</p></li><li><p>{0}.entity_larger_audience_update</p></li><li><p>{0}.entity_audience_create</p></li><li><p>{0}.entity_larger_audience_create</p></li></ol></li></ol></li></ol><p style=\"margin-left: 90.0px;\">Above command will display the complete queue and you need to find the audience ID you want to prioiritize and copy the whole value.<br />In case there are multiple entries, take the latest one (from the top of the list).</p><p>Run following command to put it to the head of the queue:</p><p style=\"margin-left: 90.0px;\">RPUSH {0}.entity_audience_update &quot;{\\&quot;audienceId\\&quot;:21686,\\&quot;s3ObjectId\\&quot;:{\\&quot;bucket\\&quot;:\\&quot;meesho-supply-v2\\&quot;,\\&quot;key\\&quot;:\\&quot;audiences/audience_21686.csv\\&quot;,\\&quot;versionId\\&quot;:\\&quot;Z0bcIdiy0sEG9ch.b60CTAgpy_fOYirn\\&quot;}}&quot;<br />Replace value copied from above in this command</p><p style=\"margin-left: 90.0px;\" /><p>Run following command to remove the older entry from the queue:<br />LREM {0}.entity_audience_update 1 &quot;{\\&quot;audienceId\\&quot;:21686,\\&quot;s3ObjectId\\&quot;:{\\&quot;bucket\\&quot;:\\&quot;meesho-supply-v2\\&quot;,\\&quot;key\\&quot;:\\&quot;audiences/audience_21686.csv\\&quot;,\\&quot;versionId\\&quot;:\\&quot;Z0bcIdiy0sEG9ch.b60CTAgpy_fOYirn\\&quot;}}&quot;<br />Replace value copied from above in this command</p><ol start=\"1\"><li><p>If stage&gt;4</p><ol start=\"1\"><li><p>Refresh is already in progress.</p></li><li><p>See <a href=\"https://prod-ops-argocd.meesho.com/applications/prd-ab-worker-update?node=%2FPod%2Fprd-ab-worker-update%2Fprd-ab-worker-update-6f559d4db-w59nt%2F0&amp;tab=logs\"><span style=\"color: rgb(17,85,204);\"><u>logs</u></span></a> and search for that audience ID.</p></li><li><p>If we figure out that audience is not being processed (because of restart OR abrupt termination), then re-trigger audience from step 1.b</p></li></ol></li></ol><p />",
        "representation": "storage",
        "word_count": 726
      },
      "version": {
        "number": 5,
        "when": "2023-11-10T15:17:11.737Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3033792543",
      "title": "Supply Oncall Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3033792543",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<h3>Recommendations</h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"140ab9b8-6239-4ef6-93d1-adb0555364a6\"><ac:plain-text-body><![CDATA[rx-fa-recommendations → advertisement-reco → pdp-iop → vss    (PERSONALISED)\nrx-fa-recommendations → advertisement-reco → Hbase            (DEFAULT)       (Disabled for Sale)\nrx-fa-recommendations → advertisement-reco → elastic-search   (SECONDARY)     (Disabled for Sale)]]></ac:plain-text-body></ac:structured-macro><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"de74393f-088e-4569-ad3f-01db5eb9f5ee\"><colgroup><col style=\"width: 224.0px;\" /><col style=\"width: 396.0px;\" /><col style=\"width: 140.0px;\" /></colgroup><tbody><tr><th><p><strong>Issue</strong></p></th><th><p><strong>Flow</strong></p></th><th><p><strong>POC</strong></p></th></tr><tr><td><p>Feed Aggregator Short Circuit</p></td><td><ol start=\"1\"><li><p><a href=\"https://grafana.meesho.com/d/f164e1ea-db23-45f5-81dd-837612a2e786/rx-fa-recommendations?orgId=1&amp;refresh=1m\">FA downstream p99</a> latency and error rate spike.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1#:~:text=12%20panels)-,advertisement,-%2Dreco\">Ads envoy proxy p99</a> latency and error rate spike.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1#:~:text=12%20panels)-,advertisement,-%2Dreco\">Ads api p99 latency</a> and error rate spike.</p></li></ol><p>If only condition 1 is true &rarr; Contact shopping-platform-oncall, reco-oncall<br />if only condition 1 &amp; 2 are true &rarr; Contact devops team<br />if all conditions are true &rarr; Debug ads application. </p></td><td><p>shopping-platform-oncall,</p><p>reco-oncall,</p><p>Ravi Prakash</p></td></tr><tr><td><p>Serving API Latency Increase</p></td><td><ul><li><p>Check API component wise latency in API Profile.</p></li><li><p><a href=\"https://grafana.meesho.com/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1\">Check PDP IOP p99 latency</a> (perceived and actual).</p></li></ul><p>If PDP-IOP perceived latency increased, contact devops-oncall<br />If PDP-IOP actual latency increase, contact reco-oncall</p><p>CB should open automatically in case if latency increases at PDP-IOP. Check Downstream Failure panel in oncall monitoring for any spike (it should reflect the <code>CB CallNotPermitted</code> exception). Other way to check is to search for <code>CB CallNotPermitted</code> in coralogix for advertisement-reco.</p><p>To close any downstream CB forcefully please follow <a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3030089787?atlOrigin=eyJpIjoiYTEwYmFlNWU0MjRhNDcyMmE3NGRhYjY0Y2E0NTEyOWIiLCJwIjoiY29uZmx1ZW5jZS1jaGF0cy1pbnQifQ\" data-card-appearance=\"inline\">https://meesho.atlassian.net/wiki/spaces/EW/pages/3030089787?atlOrigin=eyJpIjoiYTEwYmFlNWU0MjRhNDcyMmE3NGRhYjY0Y2E0NTEyOWIiLCJwIjoiY29uZmx1ZW5jZS1jaGF0cy1pbnQifQ</a> </p><p>We fallback to batch mode feed if the CB is open</p><p>If Redis, HBase latency increase, contact db-oncall</p></td><td><p>reco-oncall, </p><p>ranking-oncall,</p><p>Dhaval Parmar, Bhawya Lohia,</p><p>Lakshay Gaba</p></td></tr><tr><td><p>Serving Error Increase</p></td><td><ul><li><p>Check Coralogix logs for Exceptions or Errors.</p></li><li><p><a href=\"https://grafana.meesho.com/d/fa4f1c4c-5d6c-4f68-b479-9483ac0e48c1/all-about-pdp-iop?orgId=1\">Check PDP IOP</a> error metrics.</p></li></ul><p>If errors from PDP-IOP increase, contact reco-oncall.</p></td><td><p>reco-oncall, </p><p>ranking-oncall,</p><p>Dhaval Parmar, Bhawya Lohia</p></td></tr><tr><td><p>Revenue Dip</p></td><td><ul><li><p>Check for revenue dip for 1d, 2d and 1w and confirm if all metrics are dipping.</p></li><li><p>Check if FA has short-circuited ads.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1&amp;viewPanel=100&amp;from=1696318153536&amp;to=1696490953536\">Check if Requests Served with zero catalogs have increased</a>, if yes &rarr; Check PDP-IOP error metrics, if increased, contact reco-oncall (<strong><ac:inline-comment-marker ac:ref=\"20617e37-7695-45b1-a20e-996b517b3933\">This metric spikes a lot and is the most critical to monitor, there will be a direct revenue impact associated with the same. Please open CB forcefully if it persists for a longer time</ac:inline-comment-marker></strong><ac:inline-comment-marker ac:ref=\"20617e37-7695-45b1-a20e-996b517b3933\">)</ac:inline-comment-marker></p></li><li><p>Check if catalogs filtering ratio has increased, if yes &rarr; Check if Active catalog count has dropped, if yes &rarr; contact demand-product to confirm drop. If no &rarr; Check if PDP-IOP Qudrant count is matching Ad active catalog count, if no contact reco-oncall, mlp-oncall</p></li><li><p>Check if there is any lag in the click ingestion pipeline in CPS.</p></li><li><p>Check if any DS feed is missing from the regular schedule, if yes &rarr; check any failures in the spark job else contact ds-oncall.</p></li><li><p>Check if dip started after recent ingestion of catalog_segment_mapping or user_segment_mapping_auto_cpc ds feeds ,if so it will affect all real estates, confirm dip in other real estates&rarr; contact ds-oncall/ds-model-owner</p></li></ul></td><td><p /></td></tr></tbody></table><h3>PDP Widget</h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"55363087-eaa8-4887-b199-c1800a57ce0b\"><ac:plain-text-body><![CDATA[widget-aggregator -> advertisement-pdp-widget -> HBase (reco feed)\n                                              -> Redis (cached child catalogs after filering)\n                                              -> ES (Child catalog metadata)\n                                              -> Prdouct Aggregator (Parent catalog metadata)]]></ac:plain-text-body></ac:structured-macro><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"5f10442d-982b-4c2f-9da6-4c599c3b0212\"><tbody><tr><th><p><strong>Issue</strong></p></th><th><p><strong>Flow</strong></p></th><th><p><strong>POC</strong></p></th></tr><tr><td><p>Widget Aggregator Short circuit</p></td><td><ol start=\"1\"><li><p><a href=\"https://grafana.meesho.com/d/NZSETdr7k1/widget-aggregator?orgId=1\">Widget Aggregator Dashboard</a> - Check for p99, CB open, failure rate metrics</p></li><li><p><a href=\"https://grafana.meesho.com/goto/7oyiIiMSg?orgId=1\">Ads envoy</a> RPS, Latency, Error rate </p></li><li><p><a href=\"https://grafana.meesho.com/goto/GXXj4iGSg?orgId=1\">Flow wise dashboard</a> - check RPS, Latency, error rate</p></li></ol><p>If only (1) is true reach out product-feed team<br />If (1) and (2) true, then perceived latency, errors is higher than actual latency - contact devops</p><p>If (1), (2), (3) true then debug from ads side </p></td><td><p>@product-feed-oncall</p><p>Sameer Verma, Chaitany Ch</p></td></tr><tr><td><p>Serving API latency increase</p></td><td><ol start=\"1\"><li><p>Check for spike in RPS, pod CPU</p></li><li><p>Check API component wise latency in API Profile.</p></li><li><p>If Redis, HBase latency is higher - Check the iops and CPU - contact devops</p></li></ol></td><td><p>@display-ads-oncall</p><p>Shubham, Neehar</p></td></tr><tr><td><p>Serving error increase</p></td><td><p>Check Coralogix logs for Exceptions or Errors.</p></td><td><p /></td></tr><tr><td><p>Revenue dip</p></td><td><ul><li><p>Check for revenue dip for 1d, 2d and 1w and confirm if all metrics are dipping.</p></li><li><p>Check if widget aggregator has short-circuited ads.</p></li><li><p><a href=\"https://grafana.meesho.com/goto/4u3iHiGSg?orgId=1\">Check if Requests Served with zero catalogs have increased</a>, if yes &rarr; check for the filtering breakdown metrics</p></li><li><p>Check if <a href=\"https://grafana.meesho.com/goto/ZRpVNmGIR?orgId=1\">catalogs filtering</a> reason breakdown</p><ul><li><p>guardrails</p></li><li><p>invalid catalogs</p></li><li><p>catalogs not available in HBase</p></li></ul></li><li><p>Check if there is any lag in the click ingestion pipeline in CPS.</p></li><li><p>Check if any DS feed is missing from the regular schedule, if yes &rarr; check any failures in the spark job else contact ds-oncall.</p></li></ul></td><td><p /></td></tr></tbody></table><p /><p><strong>For You</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"56a54a37-d58e-445f-9eda-f57e5b6e6fc7\"><ac:plain-text-body><![CDATA[rx-fa-collections → advertisement-fy → vision service (gfr service)  (PERSONALISED)\nrx-fa-collections → advertisement-fy → elastic-search  (DEFAULT)]]></ac:plain-text-body></ac:structured-macro><p /><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"2fd4b8fd-c2d2-4880-b124-1b0368b8893e\"><colgroup><col style=\"width: 224.0px;\" /><col style=\"width: 396.0px;\" /><col style=\"width: 140.0px;\" /></colgroup><tbody><tr><th><p><strong>Issue</strong></p></th><th><p><strong>Flow</strong></p></th><th><p><strong>POC</strong></p></th></tr><tr><td><p>Feed Aggregator Short Circuit</p></td><td><ol start=\"1\"><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1\">FA downstream p99</a> latency and error rate spike.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1#:~:text=9%20panels)-,advertisement,-%2Dfy\">Ads envoy proxy p99</a> latency and error rate spike.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1#:~:text=tenant%3D%27ad%27%7D%5B1m%5D))-,advertisement,-%2Dfy\">Ads api p99 latency</a> and error rate spike.</p></li></ol><p>If only condition 1 is true &rarr; Contact shopping-platform-oncall, ranking-oncall<br />if only condition 1 &amp; 2 are true &rarr; Contact devops team<br />if all conditions are true &rarr; Debug ads application. </p></td><td><p>shopping-platform-oncall,</p><p>ranking-oncall, Ravi Prakash</p></td></tr><tr><td><p>Serving API Latency Increase</p></td><td><ul><li><p>Check API component wise latency in API Profile.</p></li></ul><p>(getFYCatalogs method represents downstream vision api call in api profile)</p><p>If<a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1&amp;viewPanel=99\"> vision service actual latency</a> increase, contact ranking-oncall</p><p>If <a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1&amp;editPanel=59\">vision service perceived latency</a> increased, contact devops-oncall<br />If ES(fetchMatchingAds method in api profile represents ES call), Redis, HBase latency increase, contact db-oncall</p></td><td><p>ranking-oncall</p></td></tr><tr><td><p>Serving Error Increase</p></td><td><ul><li><p>Check Coralogix logs for Exceptions or Errors.</p></li></ul><p>If errors related to vision/gfr call increase, contact ranking-oncall else debug ads application.</p></td><td><p>ranking-oncall</p></td></tr><tr><td><p>Revenue Dip</p></td><td><ul><li><p>Check for revenue dip for 1d, 2d and 1w and confirm if all metrics are dipping.</p></li><li><p>Check if FA has short-circuited ads. If yes check the tech metrics like p99 and serving 5xx and debug.</p></li><li><p>Check if dip started after recent ingestion of catalog_segment_mapping or user_segment_mapping_auto_cpc ds feeds ,if so it will affect all real estates, confirm dip in other real estates&rarr; contact ds-oncall/ds-model-owner.<br />This is accompanied with drop in cpc.</p></li><li><p>Check if we are serving enough catalogs and relevant catalogs. For this multiple metrics can be checked:<br />1. Requests Served with zero catalogs <br />2. max catalogs shown.<br />3. increase in iterations per request<br />4. change in contribution of personalised and default feeds.</p></li><li><p>In continuation to above, if we are not serving enough good catalogs, check if catalogs filtering ratio has increased, if yes &rarr; Check if Active catalog count has dropped, if yes &rarr; contact demand-product to confirm drop. If no &rarr; contact  ranking-oncall to check for any issue on their side.</p></li><li><p>Check if there is any lag in the click ingestion pipeline in CPS.</p></li></ul></td><td><p /></td></tr></tbody></table><h3>Text Search</h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"77fb4b48-c53a-4bf6-bad7-4c0c8d84d074\"><ac:plain-text-body><![CDATA[Rx Feed Aggregator → advertisement-text-search → search orchestrator → qwest → model-proxy (Head & Torso Query) \nRx Feed Aggregator → advertisement-text-search → search orchestrator → search IACG → model-proxy (Tail Query)]]></ac:plain-text-body></ac:structured-macro><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"5afd8202-b68f-4eb9-895e-e5390c59f478\"><colgroup><col style=\"width: 186.0px;\" /><col style=\"width: 330.0px;\" /><col style=\"width: 104.0px;\" /><col style=\"width: 140.0px;\" /></colgroup><tbody><tr><th><p><strong>Issue</strong></p></th><th><p><strong>Flow</strong></p></th><th><p><strong>POC</strong></p></th><th><p><strong>Channel</strong></p></th></tr><tr><td><p>Feed Aggregator Short Circuit</p></td><td><ol start=\"1\"><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1\">FA downstream p99</a> latency and error rate spike.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1#:~:text=9%20panels)-,advertisement,-%2Dfy\">Ads envoy proxy p99</a> latency and error rate spike.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1#:~:text=tenant%3D%27ad%27%7D%5B1m%5D))-,advertisement,-%2Dfy\">Ads api p99 latency</a> and error rate spike.</p></li></ol><p>If only condition 1 is true &rarr; Contact search-oncall.<br />if only condition 1 &amp; 2 are true &rarr; Contact devops team.<br />if all conditions are true &rarr; Debug ads application. </p></td><td><p>search-oncall</p></td><td><p>search-tech</p></td></tr><tr><td><p>Serving API Latency Increase</p></td><td><ul><li><p>Check API component wise latency in API Profile.</p></li><li><p><a href=\"https://grafana.meesho.com/d/ro_lHt_Vz/search-orchestrator?orgId=1&amp;refresh=5m&amp;from=now-1h&amp;to=now&amp;var-pod=All&amp;var-tenant=ad&amp;var-path=%2Fv1%2Fsearch%2Ftext\">Check Search Orchestrator p99 latency</a> (perceived and actual).</p></li></ul><p>If Search Orchestrator perceived latency increased, check if there is any connection pool related error in coralogix otherwise contact devops-oncall.<br />If Search Orchestrator actual latency increase, contact search-oncall<br />If Redis, HBase latency increase, contact db-oncall</p><p>To close any downstream CB forcefully please follow <a href=\"https://meesho.atlassian.net/wiki/spaces/EW/pages/3030089787?atlOrigin=eyJpIjoiYTEwYmFlNWU0MjRhNDcyMmE3NGRhYjY0Y2E0NTEyOWIiLCJwIjoiY29uZmx1ZW5jZS1jaGF0cy1pbnQifQ\" data-card-appearance=\"inline\">https://meesho.atlassian.net/wiki/spaces/EW/pages/3030089787?atlOrigin=eyJpIjoiYTEwYmFlNWU0MjRhNDcyMmE3NGRhYjY0Y2E0NTEyOWIiLCJwIjoiY29uZmx1ZW5jZS1jaGF0cy1pbnQifQ</a> </p></td><td><p>search-oncall</p><p>Vipin Gupta</p><p>Rajat Goyal</p></td><td><p>search-tech</p></td></tr><tr><td><p>Serving Error Increase</p></td><td><ul><li><p>Check Coralogix logs for Exceptions or Errors.</p></li><li><p><a href=\"https://grafana.meesho.com/d/ro_lHt_Vz/search-orchestrator?orgId=1&amp;refresh=5m&amp;from=now-1h&amp;to=now&amp;var-pod=All&amp;var-tenant=ad&amp;var-path=%2Fv1%2Fsearch%2Ftext&amp;viewPanel=12\">Check Search Orchestrator</a> error metrics.</p></li></ul><p>If errors from Search Orchestrator increase, contact search-oncall.</p></td><td><p>search-oncall</p></td><td><p>search-tech</p></td></tr><tr><td><p>Revenue Dip</p></td><td><ul><li><p>Check for revenue dip for 1d, 2d and 1w and confirm if all metrics are dipping.</p></li><li><p>Check if FA has short-circuited ads. If yes check the tech metrics like p99 and serving 5xx and debug.</p></li><li><p>Check if catalogs filtering ratio has increased, if yes &rarr; Check if Active catalog count has dropped, if yes &rarr; contact demand-product to confirm drop. If no &rarr; check the lag in Kafka topic for indexing in ES.</p></li><li><p>Check if there is drop in text search CPC. if yes &rarr;  contact Vishnu, Sajal. </p></li><li><p>Check if there is any lag in the click ingestion pipeline in CPS.</p></li></ul></td><td><p /></td><td><p /></td></tr></tbody></table><h3>CLP/Collections</h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d79e8cfb-d9f6-416a-b13f-4bc73ebb724a\"><ac:plain-text-body><![CDATA[rx-fa-collections → advertisement → Redis/Hbase          (PERSONALISED)\nrx-fa-collections → advertisement → elastic-search       (DEFAULT)       \nrx-fa-collections → advertisement → elastic-search       (SECONDARY)     (Disabled for Sale)]]></ac:plain-text-body></ac:structured-macro><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"a9117c0c-a19b-432b-8a63-c403a12343e5\"><colgroup><col style=\"width: 224.0px;\" /><col style=\"width: 396.0px;\" /><col style=\"width: 140.0px;\" /></colgroup><tbody><tr><th><p><strong>Issue</strong></p></th><th><p><strong>Flow</strong></p></th><th><p><strong>POC</strong></p></th></tr><tr><td><p>Feed Aggregator Short Circuit</p></td><td><ol start=\"1\"><li><p><a href=\"https://grafana.meesho.com/d/b3fabe0c-2975-4bb3-bd20-df13e7e5a058/rx-fa-collections?orgId=1&amp;from=1696462388228&amp;to=1696505588228&amp;viewPanel=23\">FA downstream p99</a> latency and error rate spike.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1&amp;from=1696483916253&amp;to=1696505516253&amp;viewPanel=65\">Ads envoy proxy p99</a> latency and error rate spike.</p></li><li><p><a href=\"https://grafana.meesho.com/d/c17244f3-7c33-43a6-82fd-00a770fdd79b/flow-wise-dashboard?orgId=1&amp;from=1696484062826&amp;to=1696505662826&amp;viewPanel=65\">Ads api p99 latency</a> and error rate spike.</p></li></ol><p>If only condition 1 is true &rarr; Contact shopping-platform-oncall<br />if only condition 1 &amp; 2 are true &rarr; Contact devops team<br />if all conditions are true &rarr; Debug ads application. </p></td><td><p>shopping-platform-oncall,</p><p>Ravi Prakash</p></td></tr><tr><td><p>Serving API Latency Increase</p></td><td><ul><li><p>Check API component wise latency in API Profile.</p></li><li><p>Check collections Hbase latency/CPU</p></li></ul><p>If Redis, HBase latency increase, contact db-oncall</p></td><td><p>Lakshay Gaba</p><p>Anupam Rai</p></td></tr><tr><td><p>Serving Error Increase</p></td><td><ul><li><p>Check Coralogix logs for Exceptions or Errors.</p></li><li><p>Check if the latest feed table exists in the Hbase, and feed is ingested correctly</p></li><li><p>Relevant feed tables for clp/coll - <strong>clp_collection_new</strong> and <strong>user_segment_mapping</strong></p></li><li><p>Check if there are any latest feed related non-2xx errors, in such case go to the <strong>ds_feeds</strong> MySQL table and mark the latest feed as FAILED. In that case, we will be temporarily serving from older feed until the issue is fixed</p></li></ul></td><td><p /></td></tr><tr><td><p>Revenue Dip</p></td><td><ul><li><p>Check for revenue dip for 1d, 2d and 1w and confirm if all metrics are dipping</p></li><li><p>Check if FA has short-circuited ads</p></li><li><p><a href=\"https://grafana.meesho.com/d/Z7goPuP7z/oncall-monitoring?orgId=1&amp;refresh=5s&amp;from=1696335177846&amp;to=1696507977846&amp;viewPanel=13\">Check if there is any lag in feed ingestion</a>. In that case, refer to <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"DS feed ingestion failures\" ri:version-at-save=\"2\" /><ac:link-body>DS feed ingestion failures</ac:link-body></ac:link> for the resolution</p></li><li><p><a href=\"https://grafana.meesho.com/d/Z7goPuP7z/oncall-monitoring?orgId=1&amp;refresh=5s&amp;from=1696334978370&amp;to=1696507778370&amp;viewPanel=19\">Check the feed source personalisation ratio</a>, if there is a drop in personalisation it means we are not serving relevant catalogs</p></li><li><p>Check if catalogs filtering ratio has increased, if yes &rarr; Check if Active catalog count has dropped, if yes &rarr; contact demand-product to confirm drop</p></li><li><p>Check if there is any lag in the click ingestion pipeline in CPS.</p></li><li><p>Check if dip started after recent ingestion of catalog_segment_mapping or user_segment_mapping_auto_cpc ds feeds ,if so it will affect all real estates, confirm dip in other real estates&rarr; contact ds-oncall/ds-model-owner</p></li></ul></td><td><p /></td></tr></tbody></table>",
        "representation": "storage",
        "word_count": 1595
      },
      "version": {
        "number": 16,
        "when": "2023-10-05T15:31:43.639Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3031334916",
      "title": "Loyalty sale runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3031334916",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>Deployables: </p><p><strong>prd-loyalty-earn-engine-eks </strong>&rarr; This is responsible for serving traffic coming from Price aggregator for CLP&rsquo;s. </p><p><strong>prd-loyalty-earn-engine-internal-eks</strong> &rarr; Serves calls for PDP, cart. </p><p><strong>prd-loyalty-earn-engine-cron</strong> &rarr; There are 3 crons running in loyalty namely, </p><ul><li><p>coin-reconciliation cron - runs at 3:45AM once everyday. Completes at 6:22AM</p></li><li><p>product-id refresh cron -  runs at 2:00 AM once everyday. Completes by 2:43 AM</p></li><li><p>ProductSupplierOptInReconciliation cron - runs at 3:30AM once everyday. Completes by 3:35AM</p><p /></li></ul><p><strong>prd-loyalty-earn-engine-consumer</strong> &rarr; We have 4 consumers running : OrderCancelled, OrderReturned, Sub-Order Non-returnable, and Order-confirmed-v3. </p><p /><p><em><strong>Failure points</strong></em> : </p><ul><li><p><strong>Serving flow</strong> -</p><ol start=\"1\"><li><p> We currently have no retries for AB call which is needed to get holdout group for loyalty and a socket timeout of 70ms, ideally it won&rsquo;t cause any issue even if ab service goes latent, as timeout set in config service is 180ms. Even if we see upstream circuit breaking loyalty in #loyalty-service-alerts channel, we need not redeploy loyalty as the impact will be just that user won&rsquo;t be seeing loyalty on app, user will still be able to place the orders. </p></li><li><p>If you see timeouts in price-aggregator or sf and redis command timeouts in #loyalty-service-alerts and in loyalty service logs. We are seeing this behaviour during service startup most of the times, and redis-timeouts settles down pretty soon. No action needed for this during sale. If redis goes down for a longer duration<ac:inline-comment-marker ac:ref=\"0a306ea3-d3af-4e44-8a50-6d26e25e86c2\">, current timeout set is 1000ms which we will reduce tomorrow to a very less value(50ms)</ac:inline-comment-marker> as we don&rsquo;t have a circuit breaker here. So it will not choke all the api&rsquo;s.</p></li></ol></li><li><p><strong>Cron</strong> - </p><ul><li><p>We have cronitor alerts for cron coming in #growth-alerts slack channel. Potential point of failure is overlapping of crons, currently we are using default ThreadPoolTaskScheduler which allows single cron at a time,  we have 3 crons running at night, if they overlap we get an alert in #growth-alerts channel(<ac:inline-comment-marker ac:ref=\"d78f3f87-f68b-4db6-b30e-41512fffc826\">confirm for pagerduty here</ac:inline-comment-marker>). Re-run the failed cron using the api&rsquo;s : </p><ul><li><p>For coin reconciliation : </p><ul><li><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"0f018f85-b4ab-4d6d-a9b8-14c9121359c2\"><ac:plain-text-body><![CDATA[curl --location --request POST 'loyalty-earn-engine-cron.prd.internal.meeshotest.in/api/1.0/loyalty/coin/credit' \\\n--header 'Authentication: HwQ58k67Gb&f' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"start_time\": \"2023-05-31T00:00:00.00\",   // LocalDateTime.now().minusDays(12);\n    \"end_time\": \"2023-06-16T00:00:00.00\". // start time - 12 days \n}]]></ac:plain-text-body></ac:structured-macro></li></ul></li><li><p>For ProductSupplierOptInReconciliation : </p><ul><li><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"1d944c6f-1967-41eb-af6f-9a66f2aa5b7f\"><ac:plain-text-body><![CDATA[curl --location --request POST 'loyalty-earn-engine-cron.prd.internal.meeshotest.in/api/1.0/loyalty/product-id-sets' \\\n--header 'Authentication: HwQ58k67Gb&f' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'Content-Type: application/json']]></ac:plain-text-body></ac:structured-macro></li></ul></li><li><p>For product-id refresh cron : </p><ul><li><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"03146066-3116-4304-bd36-5a3cd4fe9e25\"><ac:plain-text-body><![CDATA[curl --location --request POST 'loyalty-earn-engine-cron.prd.internal.meeshotest.in/api/1.0/loyalty/product-id-sets' \\\n--header 'Authentication: HwQ58k67Gb&f' \\\n--header 'MEESHO-ISO-COUNTRY-CODE: IN' \\\n--header 'Content-Type: application/json']]></ac:plain-text-body></ac:structured-macro><p /></li></ul></li></ul></li><li><p>If loyalty products on app are not visible then run both product-id refresh cron and ProductSupplierOptInReconciliation cron. Crons are idempotent, so no harm in running them again.</p></li><li><p>No need to re-run coin-reconciliation cron if it fails during the sale. There is wallet service dependency here but even if it fails, loyalty-backend will re-run for failed cases after sale.</p></li></ul></li><li><p><strong>Consumer :  </strong></p><ul><li><p>We have a dependency on supply-api in consumers. If that goes latent, we will start getting slack alerts in #loyalty-service-alerts channel(<ac:inline-comment-marker ac:ref=\"edbb6399-35c7-4067-b233-f476b0620f0f\">confirm for pagerduty alerts</ac:inline-comment-marker>) and lag will increase for the consumer group as we have an infinite number of retries from DLQ(<ac:inline-comment-marker ac:ref=\"d2d72afe-ef75-4545-b72c-3b4155a5b5f2\">check for PG alert</ac:inline-comment-marker>). We need not do anything here, rps to downstream supply-api is less, so they won&rsquo;t be troubled by this to get their service up again.</p></li></ul></li></ul><p /><p />",
        "representation": "storage",
        "word_count": 534
      },
      "version": {
        "number": 3,
        "when": "2023-10-10T11:48:12.405Z",
        "by": "Former user (Deleted)"
      },
      "labels": []
    },
    {
      "id": "3030908983",
      "title": "Communicator Sale Runbook",
      "type": "page",
      "url": "https://meesho.atlassian.net/wiki/spaces/EW/pages/3030908983",
      "space": {
        "key": "EW",
        "name": "Engineering Wiki"
      },
      "content": {
        "body": "<p>Communicator codebase has multiple clusters, each having a use case specific to channels.</p><ul><li><p>Notification</p></li><li><p>SMS</p></li><li><p>Email</p></li><li><p>WhatsApp</p></li><li><p>InApp Popup</p></li><li><p>Web Popup</p></li></ul><p> Additionally, clusters for certain use cases have bifurcation on the basis of whether they are Automated or Manual (Campaign).<br /><ac:link><ri:page ri:content-title=\"Service Readiness Check - Communicator Service\" ri:version-at-save=\"50\" /><ac:link-body>Granular details around workflows can be referenced in the attached document.</ac:link-body></ac:link></p><h3>Important Monitoring Links for Reference</h3><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"95df3a35-7d66-42ee-ac82-1bad768e8c71\"><ac:parameter ac:name=\"title\">REFERENCE LIST OF LINKS</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"7a0f0c0d-1141-4302-9931-ba21da67a454\"><ac:parameter ac:name=\"language\">typescript</ac:parameter><ac:plain-text-body><![CDATA[Service:\n- https://grafana.meesho.com/d/IiSru8Vnz/communicator?orgId=1&from=now-24h&to=now\n- https://grafana.meesho.com/d/2I_RCyM7k/telegraf-services-dashboard?orgId=1&refresh=5s&var-service=communicator&from=now-15m&to=now\n\nHBase:\n- TableName / RPS Reference: http://ip-172-31-20-28.ap-southeast-1.compute.internal:16010/master-status\n- Load Across Nodes: http://ec2-52-221-253-135.ap-southeast-1.compute.amazonaws.com/ganglia/?r=hour&cs=&ce=&c=j-1U9D44JOFOXPK&h=&tab=m&vn=&hide-hf=false&m=load_one&sh=1&z=small&hc=4&host_regex=&max_graphs=0&s=by+name\n- Other Hbase Metrics: https://grafana.meesho.com/d/etTsDBbVk/hbase-metrics?orgId=1&from=now-3h&to=now&var-cluster=bac-p-communicator-mm-hbase&var-namespace=communicator&var-table=fcm_records_v2&var-region=All&var-master_instance=172.31.20.28:9274&var-instance=All&refresh=5s\n\nRDS:\n- https://app.redislabs.com/#/databases/11073858/subscription/1714157/view-bdb/configuration\n\nEKS_INFRA:\n- https://grafana.meesho.com/d/by1lhRO4k/eks-infra-monitoring-dashboard?orgId=1&var-service=prd-communicator-consumer-for-you&var-cluster=kube-state-metrics-p-demand-cluster&var-namespace=prd-communicator-consumer-for-you&var-Node=All&var-Pod=All&var-statefulset=All&var-deployment=All&var-host_ip=All&var-copy_of_cluster=kube-state-metrics-p-demand-cluster&var-copy_of_namespace=prd-communicator-consumer-for-you&from=now-3h&to=now&refresh=1m\n\nArgoCD:\n- TO BE UPDATED]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><h3><br /><ac:link><ri:page ri:content-title=\"User-Communication On-call SOPs\" ri:version-at-save=\"8\" /><ac:link-body>For Day-to-Day functional issues, we have our on-call run book for references</ac:link-body></ac:link><br /><br />Notification Frequently Occurring Problems and their Fixes</h3><table data-table-width=\"982\" data-layout=\"default\" ac:local-id=\"5f03ce33-35c8-467a-b162-bf8c967ff6f9\"><colgroup><col style=\"width: 306.0px;\" /><col style=\"width: 389.0px;\" /><col style=\"width: 287.0px;\" /></colgroup><tbody><tr><th><p><strong>Problem Description</strong></p></th><th><p><strong>Action Items</strong></p></th><th><p><strong>Remark</strong></p></th></tr><tr><td><p>Lag in Kafka Delivery Topics, delaying delivery of PNs dropping business metrics.<br /><br /></p></td><td><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"98c74516-b366-4f95-9b9e-3d3694a1d6dc\"><ac:parameter ac:name=\"title\">Detailed Action Items</ac:parameter><ac:rich-text-body><p>1. <a href=\"https://grafana.meesho.com/d/IiSru8Vnz/communicator?orgId=1&amp;from=now-24h&amp;to=now&amp;viewPanel=59\">Check the latency of FCM</a>, expected to be below 10sec, degradation in latency of FCM over a broader time range reduces the delivery rate. It remains a temporary degradation over a time period, we compensate it by increasing the throughput, we have rate limit which helps in limiting the max number of PNs we send.<br /><br />2. Basis on impacted cluster, we change the Poll/Semaphore for handling existing requests present in Kafka lag, and increases the number of Pods (keeping in mind the partition of topic) to handle the incoming requests.</p><p>3. </p></ac:rich-text-body></ac:structured-macro></td><td><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"700aa7d0-993a-4199-82e2-7b3aeceebabb\"><ac:parameter ac:name=\"title\">Kafka links</ac:parameter><ac:rich-text-body><p>1. <a href=\"https://grafana.meesho.com/d/JloyTAIn/msk?orgId=1&amp;var-Cluster=bac-p-communicato.kzdh2z&amp;var-GroupID=All&amp;var-Topic_test=All&amp;var-Topic=All&amp;refresh=10s&amp;from=now-3h&amp;to=now\">MSK</a> </p><p>2. Mq dashboard : </p><p>url : <a href=\"http://172.31.39.174:3000/\">http://172.31.39.174:3000/</a></p><p>demand auth token : <strong>59nDaEHZHS7RT4zUznfT</strong></p><p>Api auth token (cluster wise): get from vault (<code>API_AUTH_TOKEN</code>)</p><p>3. <a href=\"https://docs.google.com/spreadsheets/d/1K-8VQxIsdFQPL1GtK5mL4IUyU3qfo48CwGXnCNDMonU/edit#gid=0\">Mq onboarded consumers</a> (P1)</p><p>4. <a href=\"https://docs.google.com/spreadsheets/d/1hw9OUpKl72by8gbak1HTe_pX8fIhrAh__BcduLp_8dk/edit#gid=0\">Mq onboarded consumers ( P2)</a></p><p /></ac:rich-text-body></ac:structured-macro><ul><li><p><a href=\"https://console.cloud.google.com/apis/api/fcm.googleapis.com/quotas?project=meesho-supply\">FCM console </a></p></li><li><p><a href=\"https://kafka-ui.internal.meesho.co/ui/clusters/bac-p-communicator-kafka-3/consumer-groups\">Kafka UI</a></p></li><li><p><a href=\"https://grafana.meesho.com/d/IiSru8Vnz/communicator?orgId=1&amp;viewPanel=59\">Notification downstream latency</a></p></li></ul></td></tr><tr><td><p>Increase in fcm_token deletion activity, leading to increased FCM <strong>Token_Not_Found </strong>error, will be a potential error of unknowingly deleting the fcm_tokens</p></td><td><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"abd8b3ac-5c0c-4097-81ee-b8a766357a36\"><ac:parameter ac:name=\"title\">Detailed Action Item</ac:parameter><ac:rich-text-body><p>1. Reference the Grafana dashboards for the impact.</p><p><a href=\"https://grafana.meesho.com/d/IiSru8Vnz/communicator?orgId=1&amp;from=now-6h&amp;to=now&amp;viewPanel=99\">Token Update Traffic Dashboard</a></p><p><a href=\"https://grafana.meesho.com/d/IiSru8Vnz/communicator?orgId=1&amp;from=now-6h&amp;to=now&amp;viewPanel=98\">Token Not Found in FY-CLP Dashboard</a></p><p>we have Slack and PD alerts integrated on both of these.<br />2. Retrieve the data from presto tables namely<br />    - table_name to be updated<br />    we can reach out to <a href=\"mailto:prabhu.om@meesho.com\"><u>prabhu.om</u></a> from Data Platform team.<br />3. Reference the curls and sample CSV format from remark section.</p></ac:rich-text-body></ac:structured-macro></td><td><ul><li><p>Backfill FCM token</p></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d2e3b56c-7e4d-4caf-87e8-31bbc5ee2aa2\"><ac:plain-text-body><![CDATA[curl --location --request POST 'http://communicator-consumer.meeshoint.in/api/v1/communicator/notification/backfill-fcm-tokens' \\\n--form 'file=@\"<path_to_the_csv_file>\"']]></ac:plain-text-body></ac:structured-macro><p>Sample csv : <ac:structured-macro ac:name=\"view-file\" ac:schema-version=\"1\" ac:macro-id=\"2c82d5ce-0706-4b99-8bf5-4b3162168758\"><ac:parameter ac:name=\"name\"><ri:attachment ri:filename=\"fcm_backfill_sample.csv\" ri:version-at-save=\"1\" /></ac:parameter></ac:structured-macro> </p><ul><li><p>Backfill anonymous FCM token</p></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"56c1f098-f2d8-46ca-8ae1-0caf55f7ea62\"><ac:plain-text-body><![CDATA[curl --location --request POST 'http://communicator-consumer.meeshoint.in/api/v1/anonymous/internal/communicator/notification/backfill-fcm-tokens' \\\n--form 'file=@\"<path_to_the_csv_file>\"']]></ac:plain-text-body></ac:structured-macro><p>Sample CSV : <ac:structured-macro ac:name=\"view-file\" ac:schema-version=\"1\" ac:macro-id=\"bf2cfe38-bd20-4f2b-84e3-7f425ee7df70\"><ac:parameter ac:name=\"name\"><ri:attachment ri:filename=\"anonym_fcm_backfill.csv\" ri:version-at-save=\"1\" /></ac:parameter></ac:structured-macro> </p></td></tr><tr><td><p>For the following issues:</p><ol start=\"1\"><li><p>Campaign suspend</p></li><li><p>Cas cron based apis</p></li><li><p>Issues in templateName, campaignName, campaign not getting scheduled because of invalid input</p></li><li><p>Upload new notification templates</p></li></ol></td><td><p><ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"User-Communication On-call SOPs\" ri:version-at-save=\"8\" /><ac:link-body>User-Communication On-call SOPs</ac:link-body></ac:link> </p></td><td><p /></td></tr></tbody></table><p /><h3>SMS Frequently Occurring Problems and their Fixes</h3><table data-table-width=\"980\" data-layout=\"default\" ac:local-id=\"edda9414-ed94-4b4c-8d83-ba874379e574\"><colgroup><col style=\"width: 304.0px;\" /><col style=\"width: 348.0px;\" /><col style=\"width: 326.0px;\" /></colgroup><tbody><tr><th><p><strong>Problem Description</strong></p></th><th><p><strong>Action Items</strong></p></th><th><p><strong>Remark</strong></p></th></tr><tr><td><p>SMS Template Not Approved</p></td><td><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"22f341e1-e8a0-4d24-b8c3-42367b14f95d\"><ac:parameter ac:name=\"title\">Detailed Action Item</ac:parameter><ac:rich-text-body><p>1. Verify the status of sms template from the callbacks received from 3 party providers from presto table: <br />silver.msg_communicator__sms_callback_delivery_reports</p><p>2. Check if it is happening for all providers from above table. If yes , try to replace the template with similar active template if present</p><p>3. In the meantime, we can connect with <ac:link><ri:user ri:account-id=\"61b80aedacc926006aef75ed\" /></ac:link> for revalidation of templates </p><p>4. For critical sms templates use-cases, i.e , User and supplier OTP login flows, in the recent past , we have mitigated these failures with replacing the running templates with ones working with a slightly different content due to urgency of the situation . For the sale period , we have got some templates already approved as fallback. <ac:link><ri:user ri:account-id=\"61b80aedacc926006aef75ed\" /></ac:link> </p></ac:rich-text-body></ac:structured-macro><p /></td><td><p /></td></tr><tr><td><p /></td><td><p /></td><td><p /></td></tr></tbody></table><p /><h3>Email/WhatsApp Frequently Occurring Problems and their Fixes</h3><table data-table-width=\"994\" data-layout=\"default\" ac:local-id=\"30932e4f-73ee-462f-b2db-f50bf1820add\"><tbody><tr><th><p><strong>Problem Description</strong></p></th><th><p><strong>Action Items</strong></p></th><th><p><strong>Remark</strong></p></th></tr><tr><td><p>Whatsapp sender mismatch : Sending will fail</p></td><td><p>Check the sender mapped to template. Sender name is present in comms zk and name to number mapping is present in RDS table<code>communicator_service.whatsapp_sender_metadata</code></p></td><td><p /></td></tr><tr><td><p>Lag in Kafka Delivery Topics, delaying delivery of Whatsapp/Email msgs.</p></td><td><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"3adf5d33-f234-4dd2-9244-3c35aef7baec\"><ac:parameter ac:name=\"title\">Detailed action plan</ac:parameter><ac:rich-text-body><p>1. Check if Third party providers latency has increased or other template or providers specific metrics using this <a href=\"https://grafana.meesho.com/d/IiSru8Vnz/communicator?orgId=1&amp;refresh=10s&amp;from=now-30d&amp;to=now#:~:text=6%20panels)-,Email,-Email%20Downstream%20p99\">dashboard</a>. </p><p>2. Basis on impacted cluster, we change the Poll/Semaphore for handling existing requests present in Kafka lag, and increases the number of Pods (keeping in mind the partition of topic) to handle the incoming requests.</p></ac:rich-text-body></ac:structured-macro><p /><p /></td><td><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"437ee829-b7d1-4e87-8d72-8ea2f0524033\"><ac:parameter ac:name=\"title\">Dashboards and details</ac:parameter><ac:rich-text-body><p>Clusters: </p><p>1. <a href=\"https://prod-ops-argocd.meesho.com/applications/prd-communicator-consumer-email-whatsapp?resource=\">PRD-Communicator-consumer-email-whatsapp </a></p><p>2. <a href=\"https://prod-ops-argocd.meesho.com/applications/prd-communicator-consumer-email-whatsapp-campaign?resource=\">PRD-Communicator-consumer-email-whatsapp-campaign</a></p><p>Vault: </p><p>1. <a href=\"https://prod-ops-vault.meesho.com/ui/vault/secrets/meesho/show/prd/dmnd/comms/communicator-consumer-email-whatsapp\">communicator-consumer-email-whatsapp</a></p><p>2. <a href=\"https://prod-ops-vault.meesho.com/ui/vault/secrets/meesho/show/prd/dmnd/comms/communicator-consumer-email-whatsapp-campaign\">communicator-consumer-email-whatsapp-campaign</a></p><p>Config to change concurrency: </p><p>WHATSAPP_DELIVERY_LOW_CAMPAIGN_CONSUMER_CONCURRENCY </p><p>WHATSAPP_DELIVERY_LOW_CONSUMER_CONCURRENCY</p><p>Dashboard to monitor lags:  <a href=\"https://grafana.meesho.com/d/JloyTAIn/msk?orgId=1&amp;from=now-30d&amp;to=now&amp;var-Cluster=bac-p-communicato.kzdh2z&amp;var-GroupID=All&amp;var-Topic_test=All&amp;refresh=10s&amp;var-Topic=communicator.email.delivery.campaign.low_priority&amp;var-Topic=communicator.email.delivery.low_priority_v4&amp;var-Topic=communicator.whatsapp.delivery.campaign.low_priority&amp;var-Topic=communicator.whatsapp.delivery.low_priority_v3\">here</a></p></ac:rich-text-body></ac:structured-macro><p /></td></tr></tbody></table><p />",
        "representation": "storage",
        "word_count": 763
      },
      "version": {
        "number": 32,
        "when": "2023-10-03T09:30:58.166Z",
        "by": "Rohit Ranjan"
      },
      "labels": []
    }
  ]
}