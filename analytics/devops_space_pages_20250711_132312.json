{
  "metadata": {
    "fetched_at": "2025-07-11T13:23:12.366387",
    "space_key": "DEVOPS",
    "space_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/",
    "total_pages": 100,
    "runbook_pages": 1
  },
  "pages": [
    {
      "id": "1605769",
      "title": "DevOps",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/1605769",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/overview",
      "created": "2018-10-29T12:29:58.031Z",
      "content": "<ac:layout><ac:layout-section ac:type=\"single\"><ac:layout-cell><ac:macro ac:name=\"tip\"><ac:parameter ac:name=\"title\">Welcome to your new documentation space!</ac:parameter><ac:rich-text-body><p>This is the home page for your documentation space within Confluence. Documentation spaces are great for keeping technical documentation organised and up to date.</p></ac:rich-text-body></ac:macro><ac:macro ac:name=\"panel\"><ac:parameter ac:name=\"title\">Next you might want to:</ac:parameter><ac:rich-text-body><ac:task-list><ac:task><ac:task-id>1</ac:task-id><ac:task-status>complete</ac:task-status><ac:task-body><strong>Customise the home page</strong>&nbsp;-&nbsp;Click &quot;Edit&quot; to start editing your home page</ac:task-body></ac:task><ac:task><ac:task-id>2</ac:task-id><ac:task-status>complete</ac:task-status><ac:task-body><strong>Check out our sample pages</strong>&nbsp;-&nbsp;Browse the sample pages in the sidebar for layout ideas</ac:task-body></ac:task><ac:task><ac:task-id>3</ac:task-id><ac:task-status>complete</ac:task-status><ac:task-body><strong>Create additional pages</strong>&nbsp;-&nbsp;Click &quot;Create&quot; and choose &quot;Blank Page&quot; to get started</ac:task-body></ac:task><ac:task><ac:task-id>4</ac:task-id><ac:task-status>complete</ac:task-status><ac:task-body><strong>Manage permissions</strong>&nbsp;-&nbsp;Click &quot;Space Tools&quot; and select &quot;Permissions&quot; in the sidebar to manage what users see</ac:task-body></ac:task></ac:task-list></ac:rich-text-body></ac:macro></ac:layout-cell></ac:layout-section><ac:layout-section ac:type=\"three_equal\"><ac:layout-cell><h2>Search this documentation</h2><p><ac:macro ac:name=\"livesearch\"><ac:parameter ac:name=\"spaceKey\">DEVOPS</ac:parameter></ac:macro></p><h2>Popular Topics</h2><p><ac:macro ac:name=\"popular-labels\"><ac:parameter ac:name=\"count\">10</ac:parameter><ac:parameter ac:name=\"spaceKey\">DEVOPS</ac:parameter></ac:macro></p></ac:layout-cell><ac:layout-cell><h2>Featured Pages</h2><p><ac:structured-macro ac:name=\"contentbylabel\"><ac:parameter ac:name=\"spaces\">DEVOPS</ac:parameter><ac:parameter ac:name=\"showLabels\">false</ac:parameter><ac:parameter ac:name=\"sort\">title</ac:parameter><ac:parameter ac:name=\"labels\">featured</ac:parameter><ac:parameter ac:name=\"showSpace\">false</ac:parameter><ac:parameter ac:name=\"type\">page</ac:parameter></ac:structured-macro></p></ac:layout-cell><ac:layout-cell><h2>Recently Updated Pages</h2><p><ac:macro ac:name=\"recently-updated\"><ac:parameter ac:name=\"max\">5</ac:parameter><ac:parameter ac:name=\"hideHeading\">true</ac:parameter><ac:parameter ac:name=\"theme\">concise</ac:parameter><ac:parameter ac:name=\"types\">page</ac:parameter></ac:macro></p></ac:layout-cell></ac:layout-section></ac:layout>",
      "approach_used": "endpoint_2",
      "word_count": 109
    },
    {
      "id": "1671174",
      "title": "JIRA reports",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/1671174",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/1671174/JIRA+reports",
      "created": "2018-09-14T10:39:46.182Z",
      "content": "<p style=\"text-align: right;\"><ac:structured-macro ac:name=\"create-from-template\" ac:schema-version=\"1\" ac:macro-id=\"6ed87ff3-d270-47b9-863a-d939e1e45ccc\"><ac:parameter ac:name=\"spaceKey\"><ri:space ri:space-key=\"DEVOPS\" /></ac:parameter><ac:parameter ac:name=\"blueprintModuleCompleteKey\">com.atlassian.confluence.plugins.confluence-software-blueprints:jirareports-blueprint</ac:parameter><ac:parameter ac:name=\"templateName\">com.atlassian.confluence.plugins.confluence-software-blueprints:jirareports-blueprint</ac:parameter><ac:parameter ac:name=\"buttonLabel\">Add JIRA Report</ac:parameter></ac:structured-macro></p><p><ac:structured-macro ac:name=\"detailssummary\" ac:schema-version=\"2\" ac:macro-id=\"36adb74a-f348-48f5-b5cb-596f7bf29f6f\"><ac:parameter ac:name=\"firstcolumn\">Title</ac:parameter><ac:parameter ac:name=\"headings\">Date,Issues,Status</ac:parameter><ac:parameter ac:name=\"pageSize\">100</ac:parameter><ac:parameter ac:name=\"label\">jirareport</ac:parameter><ac:parameter ac:name=\"cql\">label = &quot;jirareport&quot; and space = currentSpace()</ac:parameter></ac:structured-macro></p>",
      "approach_used": "endpoint_2",
      "word_count": 28
    },
    {
      "id": "1769481",
      "title": "How to Articles DevOps",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/1769481",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/1769481/How+to+Articles+DevOps",
      "created": "2018-11-09T15:40:09.488Z",
      "content": "<p style=\"text-align: left;\">1,&nbsp;<ac:link><ri:page ri:content-title=\"FreeIPA Server and Client Configuration\" ri:version-at-save=\"4\" /><ac:plain-text-link-body><![CDATA[FreeIPA Server and Client Configuration ]]></ac:plain-text-link-body></ac:link></p><p style=\"text-align: left;\">2,&nbsp;<ac:link><ri:page ri:content-title=\"How to Setup Kong Service in Ubuntu\" ri:version-at-save=\"4\" /></ac:link></p><p style=\"text-align: left;\">3,&nbsp;<ac:link><ri:page ri:content-title=\"How to Setup and Configure OpenVpn Server in Ubuntu\" ri:version-at-save=\"4\" /></ac:link></p><p style=\"text-align: left;\">4,&nbsp;<ac:link><ri:page ri:content-title=\"How to Integrate FreeIPA (LDAP) with Jenkins for an Access Control\" ri:version-at-save=\"2\" /></ac:link></p><p style=\"text-align: left;\"><br /></p><p style=\"text-align: left;\"><br /></p><p style=\"text-align: left;\"><br /></p><p style=\"text-align: left;\"><br /></p><p><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 67
    },
    {
      "id": "1835010",
      "title": "DevOps Board",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/1835010",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/1835010/DevOps+Board",
      "created": "2018-10-29T19:17:40.577Z",
      "content": "<p class=\"auto-cursor-target\"><br /></p><p><ac:structured-macro ac:name=\"gadget\" ac:schema-version=\"1\" ac:macro-id=\"839e173b-36bb-4d89-9f50-f6fb5981e736\"><ac:parameter ac:name=\"isConfigured\">true</ac:parameter><ac:parameter ac:name=\"preferences\">rapidViewId=5&amp;showRapidViewName=true&amp;quickFilters=none&amp;showQuickFilterNames=true&amp;isConfigured=true&amp;refresh=15</ac:parameter><ac:parameter ac:name=\"rapidViewId\">5</ac:parameter><ac:parameter ac:name=\"quickFilters\">none</ac:parameter><ac:parameter ac:name=\"showRapidViewName\">true</ac:parameter><ac:parameter ac:name=\"width\">1000</ac:parameter><ac:parameter ac:name=\"refresh\">15</ac:parameter><ac:parameter ac:name=\"showQuickFilterNames\">true</ac:parameter><ac:parameter ac:name=\"id\">jirareport</ac:parameter><ac:parameter ac:name=\"url\">https://meesho.atlassian.net/rest/gadgets/1.0/g/com.pyxis.greenhopper.jira:greenhopper-gadget-rapid-view/gadgets/greenhopper-rapid-view.xml</ac:parameter></ac:structured-macro></p><h2>Summary</h2><p>Day to Day Tasks&nbsp;@Meesho.</p><h2>Important highlights from this release</h2><ol><li>Monitoring Part</li><li>Alerting/Notifications Part</li><li>Day To Day Tasks Adhoc</li><li>POC Part and New Feature Update in our existing Infra</li><li>Migration Related Tasks/Stories wrt AWS/GCP from Non-India Region to Mumbai Region in India under Both Cloud Providers.</li></ol>",
      "approach_used": "endpoint_2",
      "word_count": 54
    },
    {
      "id": "2260993",
      "title": "FreeIPA Server and Client Configuration",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2260993",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2260993/FreeIPA+Server+and+Client+Configuration",
      "created": "2018-10-03T05:54:49.137Z",
      "content": "<p><br /></p><p><span>FreeIPA tool manages Linux users and client hosts in your realm from one central location with CLI, Web UI or RPC access. </span></p><p><span>You can enable Single SignOn authentication for all your systems, services and applications.</span></p><p><br /></p><p class=\"graf graf--p graf-after--p\">In this Document, we are taking you through the Installation part of FreeIPA Server/Client on Ubuntu 16.04. It includes the following sections:</p><ul class=\"postList\"><li class=\"graf graf--li graf-after--p\">​​FreeIPA Server Installation</li><li class=\"graf graf--li graf-after--li\">FreeIPA Client Installation</li></ul><h2>Step-by-step guide</h2><p><br /></p><p><span>​​</span><strong class=\"markup--strong markup--p-strong\">Prerequisites</strong><span>:</span></p><p><br /></p><p><strong>FreeIPA server configuration:</strong></p><ol><li>Launch a new machine where you want to setup FreeIPA server</li><li>Attache Elastic IP to the new launched EC2 instance</li><li>Create route 53 entry with DNS name &quot;ipa.meesho.com ==&gt; publicIP/PrivateIP(devops-p-ldap-01b)&quot;</li><li>Configure hostname as &quot;<a href=\"http://ipa.meesho.com\">ipa.meesho.com</a>&quot; in FreeIPA server machine.</li><li>Edit below line in<strong> /etc/hosts</strong> file where 172.*.*.* is the private IP of FreeIpa server:</li></ol><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;172.31.0.110 <a href=\"http://ipa.meesho.com\">ipa.meesho.com</a></p><p>&nbsp; &nbsp; &nbsp; 6. Add below line in <strong>/etc/hostname</strong> file:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ipa.meesho.com</p><p>&nbsp; &nbsp; &nbsp; 7. Run below command in terminal to configure hostname:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$ <strong>hostname&nbsp;<a href=\"http://ipa.meesho.com\">ipa.meesho.com</a></strong></p><p>&nbsp; &nbsp; &nbsp; 8. Open below ports in FreeIpa server machine:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TCP: 80 443 389 636 88 464 22</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; UDP: 88 464 123</p><p>&nbsp; &nbsp; &nbsp; 9. Below are service details on ports:</p><p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;389 ldap<br />&nbsp; &nbsp; &nbsp; &nbsp; 636 ldaps ldap over ssl<br />&nbsp; &nbsp; &nbsp; &nbsp; 88 kerberos authentication system<br />&nbsp; &nbsp; &nbsp; &nbsp; 123 NTP Network Time Protocol(used for time synchronization)<br />&nbsp; &nbsp; &nbsp; &nbsp; 464 Kerberos Change/Set password<br />&nbsp; &nbsp; &nbsp; &nbsp; 22 ssh service</p><p>&nbsp; &nbsp; &nbsp;10.&nbsp;<span>Execute the following commands to install FreeIPA package.</span></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$ <strong>apt-get update</strong></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$&nbsp;<strong>apt-get install freeipa-server freeipa-server-dns</strong></p><p>&nbsp; &nbsp; &nbsp;11.&nbsp;<span>Install FreeIPA:</span></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$&nbsp;<strong>ipa-server-install</strong> -U --hostname={{ server_hostname }} --domain={{ domain }} --realm={{ realm }} --ds-password={{ directory_manager_password }} --admin-password={{ principal_user_password }}</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values&nbsp; which are inside&nbsp;{{ }} can be changed, below are the values that we used during configuration:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;freeipa_server_version: 4.3.1<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;freeipa_server_dns_version: 4.3.1<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;server_alias: ipa<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;server_hostname: <a href=\"http://ipa.meesho.com\">ipa.meesho.com</a><br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;domain: <a href=\"http://meesho.com\">meesho.com</a><br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;realm: MEESHO.COM<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;principal_user: admin<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;principal_user_password: M*****@*****6<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;directory_manager_password: M*****@*****6</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p>&nbsp; &nbsp; &nbsp;12. Configure proxy pass in apache conf&nbsp; file.</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;file name:&nbsp;&nbsp;<strong>/etc/apache2/sites-available/<a href=\"http://ipa.meesho.com\">ipa.meesho.com</a>.conf</strong></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ProxyPass https://{{ server_hostname }} https://{{ server_hostname }}/ipa/ui/</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;All the request which are coming to <a href=\"https://ipa.meesho.com\">https://ipa.meesho.com</a>&nbsp;===&gt;&nbsp;&nbsp;<a href=\"https://ipa.meesho.com\">https://ipa.meesho.com</a>/ipa/ui/</p><p>&nbsp; &nbsp; &nbsp;13- After making changes in apache conf, restart apahe service</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;$ <strong>server&nbsp;apache2 restart</strong>&nbsp; &nbsp; &nbsp;</p><p>&nbsp; &nbsp; &nbsp;</p><p>&nbsp; &nbsp; &nbsp;14. SSH into FreeIpa server and run below command to check status of the services:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; $ <strong>ipactl status</strong></p><p>&nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp;15. Command to i<span style=\"color: rgb(34,34,34);\">nitialize a Kerberos token for the admin user</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp; $ <strong>kinit admin</strong></span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp; enter the password for admin user that we configured earlier</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;16. Command to check created tokens:</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp; $&nbsp;<strong><span style=\"color: rgb(0,0,0);\">klist</span></strong></span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;17. Command to find user in FreeIpa server:</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp; $ <strong>ipa user-find admin</strong></span></p><p><br /></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;18. After successful installation open the url&nbsp;<a href=\"https://ipa.meesho.com\">https://ipa.meesho.com</a>&nbsp;and login by using&nbsp;principal_user and&nbsp;principal_user_password.</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<ac:image ac:border=\"true\" ac:width=\"600\"><ri:attachment ri:filename=\"Screen Shot 2018-09-16 at 5.33.15 PM.png\" ri:version-at-save=\"1\" /></ac:image> </span></p><p><br /></p><p><span style=\"color: rgb(34,34,34);\"><strong>FreeIPA Client Configuration:</strong></span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; 1. <strong>SSH</strong> into machine where you want to configure IPA client</span></p><p><br /></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; 2. Add below line in <strong>/etc/hosts:</strong></span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp;private_ip hostname</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp;ex:&nbsp;172.31.X.X devops-p-ldap-client-01b</span></p><p><br /></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;3. Add below line in <strong>/etc/hostname</strong> file:</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp;devops-p-ldap-client-01b</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; &nbsp;<strong>Note</strong>: step 2 and 3 only needed if hostname is not configured.</span></p><p><br /></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;4. Install FreeIPA client:</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp; $<strong>&nbsp;apt install freeipa-client=4.3.1</strong></span></p><p><br /></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;5. Allow member of admin group to become sudo:</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;add below line in file &quot;<strong>/etc/sudoers.d/freeipa-admins</strong>&quot;</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;&quot;%admins ALL=(ALL:ALL) ALL&quot;</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;6. Add below line in file &quot;<strong>/etc/pam.d/common-session</strong>&quot; for Enable auto creation of LDAP user folders:</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;&nbsp;session required <a href=\"http://pam_mkhomedir.so\">pam_mkhomedir.so</a></span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;7.&nbsp; Add new prefix meesho.com in IPA client</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;so new client name would be &quot;hostname.meesho.com&quot;</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;8. run below command to configure IPA client</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;&nbsp;ipa-client-install -U --domain={{ domain }} --hostname={{ client_hostname }} --server={{ server_hostname }} --realm={{ realm }} --password={{ principal_user_password }} --principal={{ principal_user }} --mkhomedir</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;values&nbsp; which are inside&nbsp;{{ }} can be changed, below are the values that we used during configuration:</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; freeipa_client_version: 4.3.1<br />&nbsp; &nbsp; server_alias: ipa<br />&nbsp; &nbsp; server_hostname: <a href=\"http://ipa.meesho.com\">ipa.meesho.com</a><br />&nbsp; &nbsp; domain: <a href=\"http://meesho.com\">meesho.com</a><br />&nbsp; &nbsp; realm: MEESHO.COM<br />&nbsp; &nbsp; principal_user: admin<br />&nbsp; &nbsp; principal_user_password: M*****@*****6<br />&nbsp; &nbsp; directory_manager_password:&nbsp;<span style=\"color: rgb(34,34,34);\">M*****@*****6</span></span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp;</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp;9.&nbsp;Set ChallengeResponseAuthentication to yes</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;file name:&nbsp;/etc/ssh/sshd_config</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;edit below line:</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;ChallengeResponseAuthentication yes</span></p><p><br /></p><p><span style=\"color: rgb(34,34,34);\">10. Restart SSH service</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp;$ service sshd restart</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; &nbsp; &nbsp;&nbsp;</span></p><p><span style=\"color: rgb(34,34,34);\">11. After successful run verify if FreeIPA client is reflecting in host section of &quot;<a href=\"https://ipa.meesho.com/ipa/ui/#/e/host/search\">https://ipa.meesho.com/ipa/ui/#/e/host/search</a>&quot;</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp;<ac:image ac:border=\"true\" ac:height=\"165\" ac:width=\"600\"><ri:attachment ri:filename=\"Screen Shot 2018-09-16 at 5.54.37 PM.png\" ri:version-at-save=\"1\" /></ac:image>&nbsp;</span></p><p><span style=\"color: rgb(34,34,34);\">12. Now login to FreeIPA sever and create user for testing</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp;&nbsp;ipa user-add pankaj.kumar --first=pankaj --last=kumar --email=pankaj.kumar@<a href=\"http://meesho.com\">meesho.com</a> --password</span></p><p><span style=\"color: rgb(34,34,34);\">13- After successful user creation try to login from your local to FreeIPA client machine</span></p><p><span style=\"color: rgb(34,34,34);\">&nbsp; ssh pankaj.kumar@<a style=\"text-decoration: none;\" href=\"https://ap-southeast-1.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-1#Addresses:search=52.76.162.76;sort=publicIp\">52.76.162.76</a></span></p><p><br /></p><p><strong><span style=\"color: rgb(34,34,34);\">Important Commands:</span></strong></p><p><span style=\"color: rgb(34,34,34);\">add user:</span></p><p><span style=\"color: rgb(34,34,34);\">ipa user-add jmutai --first=pankaj --last=kumar --email=pankaj.kumar@meesho.com --password</span></p><p><span style=\"color: rgb(34,34,34);\">find user:</span></p><p><span style=\"color: rgb(34,34,34);\">ipa user-find pankaj.kumar</span></p><p><span style=\"color: rgb(34,34,34);\">delete user:</span></p><p><span style=\"color: rgb(34,34,34);\">ipa user-del pankaj.kumar</span></p><p>command to create new group:</p><p>ipa group-add bar --desc &quot;this is an example group&quot;</p><p>command to assign user to group:</p><p>ipa group-add-member bar --users=foo</p><p><br /></p><p><strong>Importnat Configuration file:</strong></p><p>/etc/ipa/default.conf</p><p><br /></p><p><strong>Common error:</strong></p><p>1-&nbsp; TCP: 80, 88, 389\\n UDP: 88 (at least one of TCP/UDP ports 88 has to be open)\\nAlso note that following ports are necessary for ipa-client working properly after enrollment:\\n TCP: 464\\n UDP: 464, 123 (if NTP enabled)\\nInstallation failed. Rolling back changes.\\nIPA client is not configured on this system.&quot;, &quot;stderr_lines&quot;: [&quot;Skip <a href=\"http://ipa.meesho.com\">ipa.meesho.com</a>: cannot verify if this is an IPA server&quot;, &quot;Failed to verify that <a href=\"http://ipa.meesho.com\">ipa.meesho.com</a> is an IPA Server.&quot;, &quot;This may mean that the remote server is not up or is not reachable due to network or firewall settings.&quot;, &quot;Please make sure the following ports are opened in the firewall settings:&quot;, &quot; TCP: 80, 88, 389&quot;, &quot; UDP: 88 (at least one of TCP/UDP ports 88 has to be open)&quot;, &quot;Also note that following ports are necessary for ipa-client working properly after enrollment:&quot;, &quot; TCP: 464&quot;, &quot; UDP: 464, 123 (if NTP enabled)&quot;, &quot;Installation failed. Rolling back changes.&quot;, &quot;IPA client is not configured on this system.&quot;], &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: []}</p><p><strong>Solution: </strong>Don't forget to open mentioned ports:</p><p>tcp: 80, 88, 389, 464</p><p>udp: 464, 123&nbsp;</p><p><br /></p><p><strong>References:</strong></p><p><a href=\"https://computingforgeeks.com/how-to-install-and-configure-freeipa-server-on-ubuntu-18-04-ubuntu-16-04/\">https://computingforgeeks.com/how-to-install-and-configure-freeipa-server-on-ubuntu-18-04-ubuntu-16-04/</a><br /><a href=\"https://computingforgeeks.com/how-to-install-and-configure-freeipa-server-on-ubuntu-18-04-ubuntu-16-04/\">https://computingforgeeks.com/how-to-install-and-configure-freeipa-server-on-ubuntu-18-04-ubuntu-16-04/</a><br /><a href=\"https://www.freeipa.org/page/Main_Page\">https://www.freeipa.org/page/Main_Page</a><br /><a href=\"https://blog.powerupcloud.com/freeipa-server-and-client-installation-on-ubuntu-16-04-part-i-d7ec186975e5\">https://blog.powerupcloud.com/freeipa-server-and-client-installation-on-ubuntu-16-04-part-i-d7ec186975e5</a></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p><p><br /></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp;</p><p><br /></p><p><br /></p><p class=\"auto-cursor-target\"><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 1184
    },
    {
      "id": "3932170",
      "title": "Best Practices Links",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3932170",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/3932170/Best+Practices+Links",
      "created": "2018-09-20T14:53:55.436Z",
      "content": "<p><br /></p><p>1, Best Practices Link for AWS Redshift -</p><p><ac:link><ri:blog-post ri:content-title=\"Best Practices Guide For AWS RedShift\" ri:posting-day=\"2018/09/20\" ri:version-at-save=\"2\" /></ac:link>&nbsp;</p>",
      "approach_used": "endpoint_2",
      "word_count": 18
    },
    {
      "id": "6651908",
      "title": "How to Setup and Configure OpenVpn Server in Ubuntu",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/6651908",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/6651908/How+to+Setup+and+Configure+OpenVpn+Server+in+Ubuntu",
      "created": "2018-10-31T11:41:04.702Z",
      "content": "<p>OpenVPN is a full-featured open source Secure Socket Layer (SSL) VPN solution that accommodates a wide range of configurations.&nbsp;</p><h2><strong>Step-by-step guide</strong></h2><h2>Prerequisites</h2><p>To configure OpenVPN, we will need access to an Ubuntu 16.04 server.</p><p><br /></p><p><strong>Step 1: Install OpenVPN</strong></p><p><br /></p><p>update your server's package index and install openvpn and</p><p>$ sudo apt-get update<br />$ sudo apt-get install openvpn easy-rsa</p><p><br /></p><p><strong>Step 2:&nbsp;Create CA(Certificate Authority) directory</strong></p><p><span style=\"color: rgb(0,0,0);\">OpenVPN is an TLS/SSL VPN. This means that it utilizes certificates in order to encrypt traffic between the server and clients. </span></p><p><span style=\"color: rgb(0,0,0);\">In order to issue trusted certificates, we will need to set up our own simple certificate authority (CA).</span></p><p><br /></p><p>$ make-cadir ~/openvpn-ca</p><p>move into newly created directory</p><p>$ cd ~/openvpn-ca</p><p><br /></p><p><strong>Step 3:&nbsp;Configure CA(Certificate Authority) vars</strong></p><p><span style=\"color: rgb(0,0,0);\">To configure the values our CA will use, we need to edit the&nbsp;</span><code>vars</code><span style=\"color: rgb(0,0,0);\">&nbsp;file within the directory. Open that file now in your text editor:</span></p><p>update below variable in file: ~/openvpn-ca/vars</p><p><br /></p><p>vi ~/openvpn-ca/vars</p><pre class=\"code-pre\"><code>. . .\n\nexport KEY_COUNTRY=&quot;IN&quot;<br />export KEY_PROVINCE=&quot;KA&quot;<br />export KEY_CITY=&quot;Bengaluru&quot;<br />export KEY_ORG=&quot;Meesho&quot;<br />export KEY_EMAIL=&quot;pankaj.kumar@<a href=\"http://meesho.com\">meesho.com</a>&quot;<br />export KEY_OU=&quot;Community&quot;<br />export KEY_NAME=&quot;vpnserver&quot;\n\n. . .<br /><br /></code></pre><p><strong>Step 4:&nbsp;Build certificate authority</strong></p><p>Now, we can use the variables we set and the easy-rsa utilities to build our certificate authority.</p><p>Ensure you are in your CA directory, and then source the vars file you just edited:</p><p><br /></p><p>$ cd ~/openvpn-ca<br />$ source vars</p><p>You should see the following if it was sourced correctly:</p><p>NOTE: If you run ./clean-all, I will be doing a rm -rf on /root/openvpn-ca/keys</p><p>clean the directory:</p><p>$ ./clean-all</p><p>build our root CA by typing:</p><p>$ ./build-ca</p><p>press ENTER through the prompts to confirm the selections:</p><pre class=\"code-pre\"><br /></pre><div class=\"secondary-code-label\" title=\"Output\">Output</div><pre class=\"code-pre\"><code>Generating a 2048 bit RSA private key\n..........................................................................................+++\n...............................+++\nwriting new private key to 'ca.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [IN]:\nState or Province Name (full name) [KA]:\nLocality Name (eg, city) [Bengaluru]:\nOrganization Name (eg, company) [Meesho]:\nOrganizational Unit Name (eg, section) [<code>Community</code>]: Common Name (eg, your name or your server's hostname) [Meesho CA]: Name [server]: Email Address [pankaj.kumar@meesho.com:</code></pre><p><br /></p><p><span style=\"color: rgb(0,0,0);\">We now have a CA that can be used to create the rest of the files we need.</span></p><p><br /></p><p><strong>Step 5 :&nbsp;Create the Server Certificate, Key, and Encryption Files</strong></p><p><br />Note: If you choose a name other than server here, you will have to adjust some of the instructions below. For instance, when copying the generated</p><p>files to the /etc/openvpn directory, you will have to substitute the correct names. You will also have to modify the /etc/openvpn/server.conf file later to</p><p>point to the correct .crt and .key files.</p><p>command to generate OpenVPN server certificate and key pair:</p><p>$ ./build-key-server server</p><p>where server is the same name i used in &quot;KEY_NAME&quot;.</p><p>press enter if it prompting for user input.</p><p><br /></p><p>generate a strong Diffie-Hellman keys to use during key exchange by typing:</p><p>$ ./build-dh</p><p><br />generate an HMAC signature to strengthen the server's TLS integrity verification capabilities:</p><p>$ openvpn --genkey --secret keys/ta.key</p><p><br /></p><p><strong>Step 6:&nbsp;Generate a Client Certificate and Key Pair</strong></p><p>Next, we can generate a client certificate and key pair. Although this can be done on the client machine and then signed by the server/CA for security</p><p>purposes, for this guide we will generate the signed key on the server for the sake of simplicity.</p><p>We will generate a single client key/certificate for this guide, but if you have more than one client, you can repeat this process as many times as you'd like.</p><p>Pass in a unique value to the script for each client.</p><p>Below are the command you need to follow</p><p>$cd ~/openvpn-ca<br />$source vars<br />$./build-key client1</p><p>where client1 can be any name. for me it's Pankaj</p><p><br /></p><p><strong>Step 7:&nbsp;Configure the OpenVPN Service</strong></p><p><br /></p><p>copy all generated key and cert files to /etc/openvpn</p><p><br />$ cd ~/openvpn-ca/keys<br />$ cp ca.crt server.crt server.key ta.key dh2048.pem /etc/openvpn</p><p><br />Next, we need to copy and unzip a sample OpenVPN configuration file into configuration directory so that we can use it as a basis for our setup:</p><p>gunzip -c /usr/share/doc/openvpn/examples/sample-config-files/server.conf.gz | sudo tee /etc/openvpn/vpnserver.conf</p><p><br />Now that our files are in place, we can modify the server configuration file:</p><p>vim /etc/openvpn/server.conf</p><p><br />a)First, find the HMAC section by looking for the tls-auth directive. Remove the &quot;;&quot; to uncomment the tls-auth line. Below this, add the key-direction parameter set to &quot;0&quot;:</p><p>tls-auth ta.key 0 # This file is secret<br />key-direction</p><p>b) uncomment the cipher AES-128-CBC line</p><p>cipher AES-128-CBC</p><p>c) Below this, add an auth line to select the HMAC message digest algorithm. For this, SHA256 is a good choice:</p><p>auth SHA256</p><p>d) find the user and group settings and remove the &quot;;&quot; at the beginning of to uncomment those lines:</p><p>user nobody<br />group nogroup</p><p><br /></p><p>Adjust the UFW Rules to Masquerade Client Connections</p><p><br /></p><p>Before we open the firewall configuration file to add masquerading, we need to find the public network interface of our machine. To do this, type:</p><p>ip route | grep default</p><p><br />e) configure dhcp-option</p><p><br /></p><p><strong>Step 8: Adjust the Server Networking Configuration</strong></p><p>Allow IP Forwarding<br />First, we need to allow the server to forward traffic. This is fairly essential to the functionality we want our VPN server to provide.</p><p>We can adjust this setting by modifying the /etc/sysctl.conf file:</p><p>$ sudo vi /etc/sysctl.conf</p><p>uncomment below line:</p><p>net.ipv4.ip_forward=1</p><p>To read the file and adjust the values for the current session, type:</p><p>$ sudo sysctl -p</p><p><br />Adjust the UFW Rules to Masquerade Client Connections</p><p>Before we open the firewall configuration file to add masquerading, we need to find the public network interface of our machine. To do this, type:</p><p>$ ip route | grep default</p><p>get the interface. for me it's &quot;eth0&quot;</p><p><br />Now add below lines in file: /etc/ufw/before.rules</p><p><br /></p><p># START OPENVPN RULES<br /># NAT table rules<br />*nat<br />:POSTROUTING ACCEPT [0:0] <br /># Allow traffic from OpenVPN client to eth0 (change to the interface you discovered!)<br />-A POSTROUTING -s 10.8.0.0/8 -o eth0 -j MASQUERADE<br />COMMIT<br /># END OPENVPN RULES</p><p><br /></p><p>Open /etc/default/ufw file</p><p><br /></p><p>vi /etc/default/ufw</p><p>inside, find the DEFAULT_FORWARD_POLICY directive. We will change the value from DROP to ACCEPT:</p><p>DEFAULT_FORWARD_POLICY=&quot;ACCEPT&quot;</p><p>Save and close the file when you are finished.</p><p><br /></p><p><strong>Step 9: Start and Enable the OpenVPN Service</strong></p><p>We need to start the OpenVPN server by specifying our configuration file name as an instance variable after the systemd unit file name.</p><p>Our configuration file for our server is called /etc/openvpn/server.conf, so we will add @server to end of our unit file when calling it:</p><p>$ service start openvpn@server</p><p>You can also check that the OpenVPN tun0 interface is available by typing:</p><p>$ ip addr show tun0</p><p><br />output should look like this:</p><p>3: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 100<br />link/none<br />inet 10.8.0.1 peer 10.8.0.2/32 scope global tun0<br />valid_lft forever preferred_lft forever</p><p><br />If everything went well, enable the service so that it starts automatically at boot:</p><p>sudo systemctl enable openvpn@server</p><p><br /></p><p><strong>Step 10: Create Client Configuration Infrastructure</strong></p><p>Creating the Client Config Directory Structure</p><p>$ mkdir -p ~/client-configs/files</p><p>lock down permissions on our inner directory:</p><p>$ chmod 700 ~/client-configs/files</p><p>Creating a Base Configuration</p><p>Next, let's copy an example client configuration into our directory to use as our base configuration:</p><p>$ cp /usr/share/doc/openvpn/examples/sample-config-files/client.conf ~/client-configs/base.conf</p><p><br />Open this new file in your text editor:</p><p>$ vim ~/client-configs/base.conf</p><p>First, locate the remote directive. This points the client to our OpenVPN server address. This should be the public IP address of your OpenVPN server. If you changed the port that the OpenVPN server is listening on, change 9511 to the port you selected:</p><p>&quot;remote&nbsp;<a style=\"text-decoration: none;\" href=\"https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Addresses:search=13.233.0.199;sort=publicIp\">13.233.0.199</a>&nbsp;9511&quot;</p><p><br />Be sure that the protocol matches the value you are using in the server configuration:</p><p>&quot;proto udp&quot;</p><p><br />Next, uncomment the user and group directives by removing the &quot;;&quot;:</p><p>user nobody<br />group nogroup</p><p><br />Find the directives that set the ca, cert, and key. Comment out these directives since we will be adding the certs and keys within the file itself:</p><p>#ca ca.crt<br />#cert client.crt<br />#key client.key</p><p><br />Mirror the cipher and auth settings that we set in the /etc/openvpn/server.conf file:</p><p>cipher AES-128-CBC<br />auth SHA256</p><p><br /></p><p>Next, add the key-direction directive somewhere in the file. This must be set to &quot;1&quot; to work with the server:</p><p>key-direction 1</p><p><br />Save the file when you are finished.</p><p><br />Creating a Configuration Generation Script</p><p>Next, we will create a simple script to compile our base configuration with the relevant certificate, key, and encryption files. This will place the generated configuration in the ~/client-configs/files directory.</p><p>Create and open a file called make_config.sh within the ~/client-configs directory:</p><p><br />$ vim ~/client-configs/make_config.sh</p><p>Inside, paste the following script:</p><p><br />#!/bin/bash</p><p># First argument: Client identifier</p><p>KEY_DIR=~/openvpn-ca/keys<br />OUTPUT_DIR=~/client-configs/files<br />BASE_CONFIG=~/client-configs/base.conf</p><p>cat ${BASE_CONFIG} \\<br />&lt;(echo -e '&lt;ca&gt;') \\<br />${KEY_DIR}/ca.crt \\<br />&lt;(echo -e '&lt;/ca&gt;\\n&lt;cert&gt;') \\<br />${KEY_DIR}/${1}.crt \\<br />&lt;(echo -e '&lt;/cert&gt;\\n&lt;key&gt;') \\<br />${KEY_DIR}/${1}.key \\<br />&lt;(echo -e '&lt;/key&gt;\\n&lt;tls-auth&gt;') \\<br />${KEY_DIR}/ta.key \\<br />&lt;(echo -e '&lt;/tls-auth&gt;') \\<br />&gt; ${OUTPUT_DIR}/${1}.ovpn</p><p><br /></p><p>Save and close the file when you are finished.</p><p>Mark the file as executable by typing:</p><p>$chmod 700 ~/client-configs/make_config.sh</p><p><br /></p><p><strong>Step 11: Generate Client Configurations</strong></p><p><br />We already created a client certificate and key called pankaj.crt and pankaj.key respectively by running the ./build-key client1 command in step 6. We can generate a config for these credentials by moving into our ~/client-configs directory and using the script we made:</p><p>$ cd ~/client-configs</p><p>$ ./make_config.sh pankaj</p><p><br />If everything went well, we should have a client1.ovpn file in our ~/client-configs/files directory:</p><p>$ ls ~/client-configs/files</p><p>Output<br />pankaj.ovpn</p><p><br /></p><p>Transferring Configuration to Client Devices</p><p><br />sftp -i ~/.ssh/devops-prod-mumbai.pem <a href=\"mailto:ubuntu@35.154.156.85\">ubuntu@</a><a style=\"text-decoration: none;\" href=\"https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Addresses:search=13.233.0.199;sort=publicIp\">13.233.0.199</a>:client-configs/files/pankaj.ovpn ~/</p><p><br /></p><p><strong>Step 12: Install the Client Configuration</strong></p><p><br /></p><p><br />Now, we'll discuss how to install a client VPN profile on OS X.</p><p><br /></p><p>OS X</p><p>Installing</p><p>Tunnelblick is a free, open source OpenVPN client for Mac OS X. You can download the latest disk image from the Tunnelblick Downloads page.</p><p>Double-click the downloaded .dmg file and follow the prompts to install.</p><p>Towards the end of the installation process, Tunnelblick will ask if you have any configuration files. It can be easier to answer No and let Tunnelblick finish.</p><p>Open a Finder window and double-click pankaj.ovpn. Tunnelblick will install the client profile. Administrative privileges are required.</p><p><br /></p><p>Connecting</p><p><br /></p><p>Launch Tunnelblick by double-clicking Tunnelblick in the Applications folder. Once Tunnelblick has been launched, there will be a Tunnelblick icon in the</p><p>menu bar at the top right of the screen for controlling connections. Click on the icon, and then the Connect menu item to initiate the VPN connection. Select the client1 connection.</p><p><br /></p><p><br /></p><p><strong>Step 13: Test Your VPN Connection</strong></p><p><br />Once everything is installed, a simple check confirms everything is working properly. Without having a VPN connection enabled, open a browser and go to DNSLeakTest.</p><p>The site will return the IP address assigned by your internet service provider and as you appear to the rest of the world. To check your DNS settings through</p><p>the same website, click on Extended Test and it will tell you which DNS servers you are using.</p><p>Now connect the OpenVPN client to your Droplet's VPN and refresh the browser. The completely different IP address of your VPN server should now appear.</p><p>That is now how you appear to the world. Again, DNSLeakTest's Extended Test will check your DNS settings and confirm you are now using the DNS resolvers pushed by your VPN.</p><p><br /></p><p><br /></p><p><strong>Step 14: Revoking Client Certificates</strong></p><p>Occasionally, you may need to revoke a client certificate to prevent further access to the OpenVPN server.</p><p>To do so, enter your CA directory and re-source the vars file:</p><p>$ cd ~/openvpn-ca</p><p>$ source vars</p><p>Next, call the revoke-full command using the client name that you wish to revoke:</p><p>$ ./revoke-full client3<br />This will show some output, ending in error 23. This is normal and the process should have successfully generated the necessary revocation information, which is stored in a file called crl.pem within the keys subdirectory.</p><p>Transfer this file to the /etc/openvpn configuration directory:</p><p>$ sudo cp ~/openvpn-ca/keys/crl.pem /etc/openvpn<br />Next, open the OpenVPN server configuration file:</p><p>$ sudo nano /etc/openvpn/server.conf<br />At the bottom of the file, add the crl-verify option, so that the OpenVPN server checks the certificate revocation list that we've created each time a connection attempt is made:</p><p>$ /etc/openvpn/server.conf</p><p>$ crl-verify crl.pem</p><p>Save and close the file.</p><p>Finally, restart OpenVPN to implement the certificate revocation:</p><p>$ sudo service restart openvpn@server</p><p>The client should now longer be able to successfully connect to the server using the old credential.</p><p>To revoke additional clients, follow this process:</p><p>Generate a new certificate revocation list by sourcing the vars file in the ~/openvpn-ca directory and then calling the revoke-full script on the client name.</p><p>Copy the new certificate revocation list to the /etc/openvpn directory to overwrite the old list.</p><p>Restart the OpenVPN service.</p><p>This process can be used to revoke any certificates that you've previously issued for your server.</p><p><br /></p><p><br /></p><p><strong>Conclusion</strong></p><p><br />We are now securely traversing the internet protecting your identity, location, and traffic from snoopers and censors.</p><p>To configure more clients, you only need to follow steps 6, and 11-13 for each additional device. To revoke access to clients, follow step 14.</p><p><br /></p><p><strong>Issue faced while configuring open VPN:</strong></p><p>1- if we are enabling ufw then port 1195 or 9511 what we configure need to allowed</p><p>2- port no. 22 should also allowed</p><p>commands:</p><ul class=\"prefixed\"><li class=\"line\">sudo ufw allow <span class=\"highlight\" style=\"color: rgb(233,72,73);\">9511</span>/<span class=\"highlight\" style=\"color: rgb(233,72,73);\">udp</span></li><li class=\"line\">sudo ufw allow OpenSSH</li><li class=\"line\"><span style=\"color: rgb(58,58,58);\">sudo ufw enable</span></li></ul><p><br /></p><p><br /></p><p><span style=\"color: rgb(58,58,58);\"><strong>Note</strong>: We already have script to create and remove client certificate.&nbsp;</span></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 2150
    },
    {
      "id": "16547858",
      "title": "How to Setup Kong Service in Ubuntu",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/16547858",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/16547858/How+to+Setup+Kong+Service+in+Ubuntu",
      "created": "2018-10-31T11:39:24.290Z",
      "content": "<p><br />For KONG DOCS Reference go to<span>&nbsp;</span><a class=\"external-link\" style=\"text-decoration: none;\" href=\"https://docs.konghq.com/\" rel=\"nofollow\">https://docs.konghq.com/</a></p><p><br /></p><p><strong>1-Prerequisites</strong></p><p><br />$ sudo apt-get update<br />$ sudo apt-get install openssl libpcre3 procps perl</p><p><br /></p><p><strong>2-Download Kong Deb file and Install</strong></p><p><br />$ wget<span>&nbsp;</span><a class=\"external-link\" style=\"text-decoration: none;\" href=\"https://bintray.com/kong/kong-community-edition-deb/download_file?file_path=dists/kong-community-edition-0.14.0.xenial.all.deb\" rel=\"nofollow\">https://bintray.com/kong/kong-community-edition-deb/download_file?file_path=dists/kong-community-edition-0.14.0.xenial.all.deb</a><span>&nbsp;</span>-O kong.deb<br />$ sudo dpkg -i kong.deb</p><p><br /></p><p><strong>3-Install Postgres</strong></p><p>$ sudo apt-get update<br />$ sudo apt-get install postgresql-client</p><p><br /></p><p><br /></p><p><strong>4-Connect to postgres RDS and perform below operations</strong></p><p><strong>Create kong user</strong><br />CREATE USER kong; CREATE DATABASE kong OWNER kong; ALTER USER kong WITH password 'kong';</p><p><strong>Update Kong Config</strong><br />Update required DB settings in Kong Conf file by uncommenting lines for POSTGRES host, port, user are database and add kong to pg_password line in the below file $ sudo vi /etc/kong/kong.conf.default</p><p><strong>Run Kong Migrations</strong><br />$ sudo kong migrations up -c /etc/kong/kong.conf.default</p><p><strong>Make changes in Kong Postgres Database</strong><br />$ ALTER TABLE keyauth_credentials ADD COLUMN ak_id varchar(50); $ ALTER TABLE keyauth_credentials DROP CONSTRAINT keyauth_credentials_consumer_id_fkey;</p><p><br /></p><p><strong>5- Update DB details in conf file:&nbsp;/etc/kong/kong.conf.default</strong></p><p>pg_host =<span>&nbsp;</span><a class=\"external-link\" style=\"text-decoration: none;\" href=\"http://platform-p-kong.meeshoint.in/\" rel=\"nofollow\">platform-p-kong.meeshoint.in</a><span>&nbsp;</span># The PostgreSQL host to connect to.<br />pg_port = 5432 # The port to connect to.<br />pg_user = **** # The username to authenticate if required.<br />pg_password =<span>&nbsp;</span><strong>*</strong>*&nbsp;# The password to authenticate if required.<br />pg_database = kong # The database name to connect to.</p><p><br /></p><p><strong>6-Update Max Open File Desciptors</strong><br /><a class=\"external-link\" style=\"text-decoration: none;\" href=\"https://underyx.me/2015/05/18/raising-the-maximum-number-of-file-descriptors\" rel=\"nofollow\">https://underyx.me/2015/05/18/raising-the-maximum-number-of-file-descriptors</a></p><p><br /></p><p><strong>7-Installing Custom Request-Hijack Plugin in Kong</strong></p><p><br />Clone<span>&nbsp;</span><a class=\"external-link\" style=\"text-decoration: none;\" href=\"https://github.com/Meesho/supply_scripts.git\" rel=\"nofollow\">https://github.com/Meesho/supply_scripts.git</a><span>&nbsp;</span>and cd into the folder and run below commands</p><p>$ sudo luarocks make<br />If you are getting error like sh: 1: zip: not found Error: Failed packing Run this Command $ sudo apt-get install zip unzip</p><p>$ sudo luarocks pack kong-plugin-request-hijack 0.1.0-1<br />$ sudo luarocks install kong-plugin-request-hijack-0.1.0-1.all.rock<br />Restart Kong $ sudo KONG_CUSTOM_PLUGINS=request-hijack kong restart --vv -c /etc/kong/kong.conf.default</p><p><br /></p><p><strong>8-Start Kong</strong></p><p>$sudo KONG_CUSTOM_PLUGINS=request-hijack kong start -c /etc/kong/kong.conf.default -vv</p><p><br /></p><p><strong>9-Creating Service in Kong (Note: Save All JSON Output of Each Command)</strong></p><p>Replcace the HOST IP with the respective Server IP</p><p>$ curl -i -X POST<span>&nbsp;</span><a class=\"external-link\" style=\"text-decoration: none;\" href=\"http://localhost:8001/services/\" rel=\"nofollow\">http://localhost:8001/services/</a><span>&nbsp;</span>-H 'Content-Type: application/json' -d '{&quot;name&quot;:&quot;Meesho-API-Server-2&quot;, &quot;protocol&quot;: &quot;http&quot;, &quot;host&quot;: &quot;HOST-IP&gt;&quot;, &quot;retries&quot;: 3}'</p><p><br /></p><p><strong>10-Creating a Router for Meesho API Service</strong></p><p>Replcace the Service ID gotten in the previous step</p><p>$ curl -i -X POST<span>&nbsp;</span><a class=\"external-link\" style=\"text-decoration: none;\" href=\"http://localhost:8001/routes/\" rel=\"nofollow\">http://localhost:8001/routes/</a><span>&nbsp;</span>-H 'Content-Type: application/json' -d '{&quot;service&quot;:{&quot;id&quot;:&quot;&lt;SERVICE-ID&gt;&quot;}, &quot;hosts&quot;:[&quot;*.com&quot;], &quot;preserve_host&quot;: true, &quot;paths&quot;: [&quot;/&quot;]}'</p><p><br /></p><p><strong>11-Adding Up OAuth Plugin in Kong</strong></p><p>Add OAuth Plugin for Prod Service $ curl -X POST<span>&nbsp;</span><a style=\"text-decoration: none;\" rel=\"nofollow\">http://localhost:8001/services/&lt;SERVICE-ID&gt;/plugins</a><span>&nbsp;</span>-H 'content-type: application/x-www-form-urlencoded' -d 'name=oauth2&amp;config.enable_authorization_code=true&amp;config.accept_http_if_already_terminated=true'</p><p><br /></p><p><strong>12-Create Consumer for OAuth Plugin</strong></p><p>$ curl -i -X POST<span>&nbsp;</span><a class=\"external-link\" style=\"text-decoration: none;\" href=\"http://localhost:8001/consumers/\" rel=\"nofollow\">http://localhost:8001/consumers/</a><span>&nbsp;</span>-H 'Content-Type: application/json' -d '{&quot;username&quot;:&quot;MeeshoUser&quot;}'</p><p><strong>Create an Application for a Given Consumer</strong></p><p>$ curl -i -X POST<span>&nbsp;</span><a style=\"text-decoration: none;\" rel=\"nofollow\">http://localhost:8001/consumers/&lt;CONSUMER-ID&gt;/oauth2</a><span>&nbsp;</span>-H 'Content-Type: application/json' -d '{&quot;name&quot;:&quot;Meesho-Prod-App&quot;, &quot;redirect_uri&quot;:&quot;<a class=\"external-link\" style=\"text-decoration: none;\" href=\"http://localhost:3000/oauth_callback\" rel=\"nofollow\">http://localhost:3000/oauth_callback</a>&quot;}'</p><p><br /></p><p><strong>13-Extra things to be added to Kong</strong></p><p><br />$ cd /usr/local/share/lua/5.1/kong/templates/ $ sudo vim ngnix_kong.conf</p><p><strong>note:</strong> Search for location keyword and ADD before that</p><p>location = /app-version-code {<br />internal;<br />proxy_pass http://&lt;MACHINE-IP&gt;/api/1.0/kong/app-version-code;<br />}</p><p><br />location = /app-upgrade-notification {<br />internal;<br />proxy_pass http://&lt;MACHINE-IP&gt;/api/1.0/kong/app-upgrade-notification;<br />}</p>",
      "approach_used": "endpoint_2",
      "word_count": 455
    },
    {
      "id": "17301543",
      "title": "How to Integrate FreeIPA (LDAP) with Jenkins for an Access Control",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/17301543",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/17301543/How+to+Integrate+FreeIPA+LDAP+with+Jenkins+for+an+Access+Control",
      "created": "2018-11-09T15:26:23.096Z",
      "content": "<h2>Problem</h2><p><span style=\"text-decoration: none;\">The point of setting up freeIPA for an intranet is to enable single-sign-on (SSO) for all the internal services that requires authentication and authorization.<br /><span style=\"text-decoration: none;\">FreeIPA can serve as a&nbsp;LDAP authentication and authorization provider to integrate&nbsp;with most of today's&nbsp;reputable server&nbsp;software. Ex: Jenkins and so on..</span><br /></span></p><h2>Solution</h2><p>Here two way modifications is required -<br />1, First on the FreeIPA Server Side, we need to create LDAP account for Jenkins to access the LDAP data. Create a file called &quot;jenkins_ldap&quot;<br />2, Second on the Jenkins UI Console Page to update FreeIPA details.</p><h2>#, First on the FreeIPA Server Side</h2><p>1, Create a user file in ldif format, move to this directory -&nbsp;<br />&nbsp;#&nbsp;/usr/share/ipa/<br /><br /></p><p><span>#&nbsp;root@<a href=\"http://ipa/usr/share/ipa\">ipa:/usr/share/ipa#</a> cat jenkins_ldap.ldif&nbsp;</span></p><p><span>dn: uid=jenkins_ldap,cn=sysaccounts,cn=etc,dc=meesho,dc=com</span></p><p><span>changetype: add</span></p><p><span>objectclass: account</span></p><p><span>objectclass: simplesecurityobject</span></p><p><span>uid: jenkins_ldap</span></p><p><span>userPassword: some_secure_password_and_not_any_random_pass</span></p><p><span>passwordExpirationTime: 20380119031407Z</span></p><p><span>nsIdleTimeout: 0</span></p><p><span><br /><strong>Note:</strong> As the user has been created already and&nbsp;hence the cat is done, for new setup you need to create it first and then run ldapmodify command.Ex: show below.<br /></span></p><p><span>#&nbsp;vim&nbsp;<span>jenkins_ldap.ldif</span><br />#&nbsp;ldapmodify -h <a href=\"http://ipa.meesho.com\">ipa.meesho.com</a> -p 389 -x -D &quot;cn=Directory Manager&quot; -W -f jenkins_ldap.ldif<br /><br /></span></p><h2>#, Second on the Jenkins UI Console&nbsp;</h2><p>1,&nbsp;<span style=\"text-decoration: none;\">Go to&nbsp;</span><em>Manage Jenkins</em><span style=\"text-decoration: none;\">&nbsp;-&gt;&nbsp;</span><em>Configure Global Security</em><span style=\"text-decoration: none;\">&nbsp;-&gt;&nbsp;</span><em>Security Realm</em><span style=\"text-decoration: none;\">, and choose&nbsp;</span><em>LDAP</em><span><span><em>, and set the following:</em></span><br /><span><em>a, Server:</em></span><br /><span><em>b,&nbsp;</em></span><span style=\"color: rgb(51,51,51);text-decoration: none;\">root DN:</span><br /><span><em>c,&nbsp;</em></span><span style=\"color: rgb(51,51,51);text-decoration: none;\">User search base:&nbsp;</span><br /><span><em>d,&nbsp;</em></span><span style=\"color: rgb(51,51,51);text-decoration: none;\">User search filter:&nbsp;</span><br /><span><em>e,&nbsp;</em></span><span><em>Group membership &gt;&nbsp;</em><span style=\"color: rgb(51,51,51);text-decoration: none;\">Group membership filter:</span><br /><em>f,&nbsp;</em><span><em>Manager DN:</em><br /><em>g,&nbsp;</em><span><em>Manager Password:&nbsp;<br /></em></span></span></span></span><span><span><span><span>h, Apply and Save.<br /><br /><strong style=\"color: rgb(51,51,51);font-style: italic;text-decoration: none;\">Note:</strong><em>&nbsp;Details of the above is mentioned here in&nbsp;snapshot.</em><br /></span></span></span></span></p><p><ac:image><ri:attachment ri:filename=\"image2018-11-9_20-39-36.png\" ri:version-at-save=\"1\" /></ac:image></p><p><span><br /><br /></span></p><p><br /></p><p><span><span style=\"text-decoration: none;\">Then click on<span>&nbsp;</span></span><em style=\"text-decoration: none;\">Test LDAP settings&nbsp;</em><span style=\"text-decoration: none;\">and try login with an account, if results are all green, authentication is configured.&nbsp;<span style=\"text-decoration: none;\">Once saved, one has to login to Jenkins with the SSO account of freeIPA, but that's the point, isn't it.&nbsp;</span></span><br /><br /><br /><br /></span>Referenced Link -</p><p>1,&nbsp;<a href=\"https://watchmysys.com/blog/2014/09/freeipa-ldap-and-jenkins/\">https://watchmysys.com/blog/2014/09/freeipa-ldap-and-jenkins/</a></p><p><br /></p><p><br /></p><p><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 303
    },
    {
      "id": "17399811",
      "title": "DevOps Process for any New Infra Requirement",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/17399811",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/17399811/DevOps+Process+for+any+New+Infra+Requirement",
      "created": "2020-03-10T07:25:04.989Z",
      "content": "<p class=\"auto-cursor-target\"><br /></p><ac:structured-macro ac:name=\"details\" ac:schema-version=\"1\" ac:macro-id=\"7461f04a-7e38-432f-89ac-50a566d5050f\"><ac:parameter ac:name=\"id\">add</ac:parameter><ac:rich-text-body><p class=\"auto-cursor-target\"><br /></p><table class=\"wrapped\"><colgroup><col /><col /></colgroup><tbody><tr><th>POD Name</th><td>Team Name</td></tr><tr><th>Jira Ticket Link</th><td>Link for&nbsp;Jira Ticket</td></tr><tr><th>Delivery Time Frame</th><td><div class=\"content-wrapper\">Will be updated once approved from the concerned owners.</div></td></tr><tr><th>Approved By</th><td><div class=\"content-wrapper\"><p><ac:link><ri:user ri:userkey=\"8a7f808a6adefb47016adfac96a500fb\" /></ac:link></p></div></td></tr><tr><th>Approved By</th><td><div class=\"content-wrapper\"><ac:link><ri:user ri:userkey=\"8a7f808565cb41b70165cdabac17002d\" /></ac:link></div></td></tr><tr><th>Lead Developer Name</th><td>Lead Developer Name</td></tr><tr><th><p>Project Owner Name</p></th><td>Project Owner Name</td></tr></tbody></table><p class=\"auto-cursor-target\"><br /></p></ac:rich-text-body></ac:structured-macro><h2>Goals :</h2><ul><li>This is required to simplify and understand the request appropriately.</li></ul><h2>Background and strategic fit :</h2><ul><li>T<span style=\"color: rgb(34,34,34);text-decoration: none;\">o clearly understand the requirements and to update on the stipulated time frame.</span></li></ul><h2>Requirements :</h2><ol><li style=\"text-decoration: none;\">Application or Service Name or Team Name for which this Infra is required ? (&nbsp;POD/Team Owner Contact Person).</li><li style=\"text-decoration: none;\">Type of Machine (Instance Type) : (Ex: m3.medium etc..)</li><li style=\"text-decoration: none;\">Cloud Provider Name (AWS or GCP) ? (If in AWS then which AWS Account (SandBox or Prod)) ?</li><li style=\"text-decoration: none;\">What other machines will connect to the new machine and on which ports it will connect to ?</li><li style=\"text-decoration: none;\">If Public Access for the machine then why do we need this to in Public (Internet Facing) ?</li><li style=\"text-decoration: none;\">Domain Name for the concerned machines ?</li><li style=\"text-decoration: none;\">If the machines/cluster will be Internal then do update (Yes or No) ?</li><li style=\"text-decoration: none;\">Machines/Cluster needs to be load balanced and if then details we need here (Ex: Port Based | Path Based etc) ?</li><li style=\"text-decoration: none;\">Monitoring Required for Prod other then basic system checks (cpu | memory | cron | keepalive | disk), then do update here with all such details in a bit brief ?<br /><span style=\"color: rgb(34,34,34);text-decoration: none;\">What would be the storage/disk size ?</span></li><li style=\"text-decoration: none;\"><span style=\"color: rgb(34,34,34);text-decoration: none;\">For Production Services we need to avoid any Single Point of Failure and hence we need to avoid single machines for Critical Business services.</span></li><li><span style=\"color: rgb(34,34,34);\">Health Check Path API for&nbsp;<span>checking</span>&nbsp;the Machine(s) response&nbsp;under the LB.</span></li></ol><h2>IMP Note :</h2><p>DevOps related process for any of the new infrastructure related&nbsp;projects, which will have the approvals from the concerned Team Member and post approval things will be aligned to change or update or to deploy in our Infrastructure.<br /><br />Application Level Update Needs to be updated by the concerned tech lead (project owner).</p><ul><li style=\"text-decoration: none;\">Infra Level of an Approval -</li></ul><div style=\"text-decoration: none;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;a, Jegadesh</div><div style=\"text-decoration: none;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;b, Pankaj (RDS, Search, Redis)</div><p><br /></p><p><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 372
    },
    {
      "id": "17432633",
      "title": "DevOps-Dash",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/17432633",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/17432633/DevOps-Dash",
      "created": "2018-10-30T10:28:18.533Z",
      "content": "<p class=\"auto-cursor-target\"><br /></p><ac:structured-macro ac:name=\"details\" ac:schema-version=\"1\" ac:macro-id=\"2dce3ff6-244b-4211-bd7f-f0f80500456f\"><ac:parameter ac:name=\"label\">jirareport</ac:parameter><ac:rich-text-body><p class=\"auto-cursor-target\"><br /></p><table class=\"wrapped\"><colgroup><col /><col /></colgroup><tbody><tr><th>Date</th><td>Oct 30, 2018</td></tr><tr><th>Issues</th><td><div class=\"content-wrapper\"><ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"20cc548b-82f5-47b7-a7d7-d08f4f0757d4\"><ac:parameter ac:name=\"server\">System JIRA</ac:parameter><ac:parameter ac:name=\"jqlQuery\">project = DEVOPS</ac:parameter><ac:parameter ac:name=\"count\">true</ac:parameter><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter></ac:structured-macro></div></td></tr><tr><th>Status</th><td><div class=\"content-wrapper\"><ac:structured-macro ac:name=\"status\" ac:schema-version=\"1\" ac:macro-id=\"44870ce4-a922-44fb-ac49-496ea45fbbd9\"><ac:parameter ac:name=\"colour\">Green</ac:parameter><ac:parameter ac:name=\"title\">Green</ac:parameter></ac:structured-macro></div></td></tr></tbody></table><p class=\"auto-cursor-target\"><br /></p></ac:rich-text-body></ac:structured-macro><h2>DevOps Report summary</h2><p><ac:placeholder>Insert release summary text here.</ac:placeholder></p><h3>Overall status for<ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"d81f1a10-c405-4893-86f7-d6e2d14b91a8\"><ac:parameter ac:name=\"server\">System JIRA</ac:parameter><ac:parameter ac:name=\"jqlQuery\">project = DEVOPS</ac:parameter><ac:parameter ac:name=\"count\">true</ac:parameter><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter></ac:structured-macro></h3><p><ac:structured-macro ac:name=\"jirachart\" ac:schema-version=\"1\" ac:macro-id=\"2aca1783-5f09-4b2e-bb5a-93942d8dc897\"><ac:parameter ac:name=\"border\">true</ac:parameter><ac:parameter ac:name=\"server\">System JIRA</ac:parameter><ac:parameter ac:name=\"jql\">project = DEVOPS</ac:parameter><ac:parameter ac:name=\"statType\">statuses</ac:parameter><ac:parameter ac:name=\"chartType\">pie</ac:parameter><ac:parameter ac:name=\"width\" /><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter></ac:structured-macro></p><h3>Priority</h3><p><ac:structured-macro ac:name=\"jirachart\" ac:schema-version=\"1\" ac:macro-id=\"5b14df30-f0b9-44ce-a143-4127856696c8\"><ac:parameter ac:name=\"border\">true</ac:parameter><ac:parameter ac:name=\"server\">System JIRA</ac:parameter><ac:parameter ac:name=\"jql\">project = DEVOPS</ac:parameter><ac:parameter ac:name=\"statType\">priorities</ac:parameter><ac:parameter ac:name=\"chartType\">pie</ac:parameter><ac:parameter ac:name=\"width\" /><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter></ac:structured-macro></p><h3>Component</h3><p><ac:structured-macro ac:name=\"jirachart\" ac:schema-version=\"1\" ac:macro-id=\"c110d86f-2634-42cc-94ea-223fe8f2fe36\"><ac:parameter ac:name=\"border\">true</ac:parameter><ac:parameter ac:name=\"server\">System JIRA</ac:parameter><ac:parameter ac:name=\"jql\">project = DEVOPS</ac:parameter><ac:parameter ac:name=\"statType\">components</ac:parameter><ac:parameter ac:name=\"chartType\">pie</ac:parameter><ac:parameter ac:name=\"width\" /><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter></ac:structured-macro></p><h3>Issue Type</h3><p><ac:structured-macro ac:name=\"jirachart\" ac:schema-version=\"1\" ac:macro-id=\"fdf96dd9-eb62-4e0a-b46a-69997184db60\"><ac:parameter ac:name=\"border\">true</ac:parameter><ac:parameter ac:name=\"server\">System JIRA</ac:parameter><ac:parameter ac:name=\"jql\">project = DEVOPS</ac:parameter><ac:parameter ac:name=\"statType\">issuetype</ac:parameter><ac:parameter ac:name=\"chartType\">pie</ac:parameter><ac:parameter ac:name=\"width\" /><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter></ac:structured-macro></p>",
      "approach_used": "endpoint_2",
      "word_count": 108
    },
    {
      "id": "17563716",
      "title": "Troubleshooting articles",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/17563716",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/17563716/Troubleshooting+articles",
      "created": "2018-10-31T11:44:11.828Z",
      "content": "<p style=\"text-align: right;\"><ac:macro ac:name=\"create-from-template\"><ac:parameter ac:name=\"contentBlueprintId\">2676a303-5005-4c1e-8b04-7dfda85b7544</ac:parameter><ac:parameter ac:name=\"blueprintModuleCompleteKey\">com.atlassian.confluence.plugins.confluence-knowledge-base:kb-troubleshooting-article-blueprint</ac:parameter><ac:parameter ac:name=\"createButtonLabel\">Add troubleshooting article</ac:parameter></ac:macro></p><hr /><p><ac:macro ac:name=\"content-report-table\"><ac:parameter ac:name=\"contentBlueprintId\">2676a303-5005-4c1e-8b04-7dfda85b7544</ac:parameter><ac:parameter ac:name=\"blueprintModuleCompleteKey\">com.atlassian.confluence.plugins.confluence-knowledge-base:kb-troubleshooting-article-blueprint</ac:parameter><ac:parameter ac:name=\"analyticsKey\">kb-troubleshooting-article</ac:parameter><ac:parameter ac:name=\"blankDescription\">Provide solutions for commonly encountered problems.</ac:parameter><ac:parameter ac:name=\"blankTitle\">Troubleshooting article</ac:parameter><ac:parameter ac:name=\"spaces\">DEVOPS</ac:parameter><ac:parameter ac:name=\"createButtonLabel\">Add troubleshooting article</ac:parameter><ac:parameter ac:name=\"labels\">kb-troubleshooting-article</ac:parameter></ac:macro></p>",
      "approach_used": "endpoint_2",
      "word_count": 27
    },
    {
      "id": "40337412",
      "title": "Monitoring Services",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/40337412",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/40337412/Monitoring+Services",
      "created": "2019-02-20T19:29:39.098Z",
      "content": "<h2>Problem</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"dc1279d1-b861-478b-a029-cb89c8c83867\"><ac:rich-text-body>If any issues comes under Sensu wrt CPU | Memory | Disk | Keepalive, then what you guy's need to do, please update the Steps here -</ac:rich-text-body></ac:structured-macro><h2>Solution for CPU Issue</h2><p>First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;Check the private ip address of the alert from Sensu dashboard accordingly.<br />Singapore:&nbsp;&nbsp;<a href=\"http://sensu.meesho.com:9090/#/clients\">http://sensu.meesho.com:9090/#/clients</a><br />Mumbai:&nbsp;<a href=\"http://sensu.meesho.co:9090/#/clients\">http://sensu.meesho.co:9090/#/clients</a><br />On sensu Dashboard you will be quickly able to see the &quot;History&quot; on config of alert to understand the pattern of an alert.&nbsp;<br /><br /></li><li><p class=\"auto-cursor-target\">Go to EC2 Search on AWS console &rarr; <a href=\"https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Home\">https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Home</a>:<br />Copy the client Private IP address from sensu Dashboard and search in Respective(Singapore/Mumbai) Ec2 instances search to get more details about the Host machine. We can quickly monitor the current CPU usage of the host and parallelly take ONLY the PRIVATE IP address of the machine on AWS monitor details and Login in to Bash using simple ssh username@ipaddress (if it is Mumbai)<br /><br /></p></li><li><p class=\"auto-cursor-target\">If its Singapore region, you would be required to login to&nbsp;bastion Server&nbsp; first and from Bastion server(54.169.65.159) ,We have to ssh to Singapore servers as shown below.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"dd5ae289-66a2-4fa0-96cd-6d67470ad07d\"><ac:plain-text-body><![CDATA[1. ssh username@hostipaddress (mumbai Region)\n2. ssh username@54.169.65.159 (bastion Server)\n3. ssh -i key-pair username@ipaddress (singapore Region)\n4. ps -eo pmem,pcpu,pid,args | tail -n +2 | sort -rnk 2 | head -20]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p></li><li>The above commands will help to analyse the current CPU usage and process responsible for hike. So, paste the details of process and current usage of the alert to responsible Engineers.<br />Contact sheet will be available:&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1he8GxwKL9UTpli2xBoGuSIm0q0Vk7Mf4oKq66j8nA4o/edit#gid=0\">https://docs.google.com/spreadsheets/d/1he8GxwKL9UTpli2xBoGuSIm0q0Vk7Mf4oKq66j8nA4o/edit#gid=0</a>.&nbsp;<br /><br />Screenshots:<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 6.00.09 PM.png\" ri:version-at-save=\"2\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 6.04.04 PM.png\" ri:version-at-save=\"2\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 3.55.16 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /><br /></li></ol><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"061e7b3c-b5b7-49a7-9e3c-f332fb347e7e\"><ac:rich-text-body>Make sure, we are NOT using PUBLIC IP ADDRESS.<br />DO NOT Use any&nbsp; commands other than the instructed one's on SENSU dashboard.</ac:rich-text-body></ac:structured-macro><h2>Solution for Memory Issue</h2><p>First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;Check the client name of the alert from dashboard accordingly.<br />Singapore:&nbsp;&nbsp;<a href=\"http://sensu.meesho.com:9090/#/clients\">http://sensu.meesho.com:9090/#/clients</a><br />Mumbai:&nbsp;<a href=\"http://sensu.meesho.co:9090/#/clients\">http://sensu.meesho.co:9090/#/clients</a><br />On Dashboard you will be quickly able to see the &quot;History&quot; on config of alert to understand the pattern of alert.&nbsp;<br /><br /></li><li><p class=\"auto-cursor-target\">Go to EC2 Search on AWS console &rarr; <a href=\"https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Home\">https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Home</a>:<br />Copy the&nbsp;client Private IP address from sensu Dashboard and search in Respective(Singapore/Mumbai) Ec2 instances search to get more details about the Host machine. We can quickly monitor the current CPU usage of the host and parallelly take ONLY the PRIVATE IP address of the machine on AWS monitor details and Login in to Bash using simple ssh username@ipaddress (if it is Mumbai).</p></li><li><p class=\"auto-cursor-target\">If its Singapore region, you would be required to login to&nbsp;bastion Server&nbsp; first and from Bastion server(54.169.65.159)&nbsp; we have to ssh to Singapore servers as shown below.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"1d055844-f6d2-49b5-954b-6b18d345f09f\"><ac:plain-text-body><![CDATA[1. ssh username@hostipaddress (mumbai Region)\n2. ssh username@54.169.65.159 (bastion Server)\n3. ssh -i key-pair username@ipaddress (singapore Region)\n4. ps -eo pmem,pcpu,pid,args | tail -n +2 | sort -rnk 2 | head -20\n5. free -mh]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p></li><li>The above commands will help to analyse the current Memory usage and processes responsible for consuming memory. So, paste the details of process and current usage of the alert to responsible Engineers.<br />Contact sheet will be available:&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1he8GxwKL9UTpli2xBoGuSIm0q0Vk7Mf4oKq66j8nA4o/edit#gid=0\">https://docs.google.com/spreadsheets/d/1he8GxwKL9UTpli2xBoGuSIm0q0Vk7Mf4oKq66j8nA4o/edit#gid=0</a>. Find screen shots for reference<br /><br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 6.04.04 PM.png\" ri:version-at-save=\"2\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 6.00.09 PM.png\" ri:version-at-save=\"2\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 4.18.19 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /><br /><br /></li></ol><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"9c07bb52-b142-4744-9b19-e01b93d42cce\"><ac:rich-text-body><p>Make sure, we are not using PUBLIC IP ADDRESS.<br />DO NOT Use any&nbsp; commands other than the instructed one's on SENSU dashboard.<br />Frontend ALERTS will be from MUMBAI only backend services are deployed in Singapore, make sure to select right regions all time.</p></ac:rich-text-body></ac:structured-macro><h2>Solution for Disk Space Issue&nbsp;</h2><p>First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;Check the client name of the alert from dashboard accordingly.<br />Singapore:&nbsp;&nbsp;<a href=\"http://sensu.meesho.com:9090/#/clients\">http://sensu.meesho.com:9090/#/clients</a><br />Mumbai:&nbsp;<a href=\"http://sensu.meesho.co:9090/#/clients\">http://sensu.meesho.co:9090/#/clients</a><br />On Dashboard you will be quickly able to see the &quot;History&quot; on config of alert to understand the pattern of alert.&nbsp;<br /><br /></li><li><p class=\"auto-cursor-target\">Go to EC2 Search on AWS console &rarr; <a href=\"https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Home\">https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Home</a>:<br />Copy the&nbsp;client Private IP address from sensu Dashboard and search in Respective(Singapore/Mumbai) Ec2 instances search to get more details about the Host machine. We can quickly monitor the current CPU usage of the host and parallelly take ONLY the PRIVATE IP address of the machine on AWS monitor details and Login in to Bash using simple ssh username@ipaddress (if it is Mumbai).</p></li><li><p class=\"auto-cursor-target\">If its Singapore region, you would be required to login to&nbsp;bastion Server&nbsp; first and from Bastion server(54.169.65.159),&nbsp; we have to ssh to Singapore servers as shown below. Please make sure Incase of High CPU utilisation, we are not running the below 5th command.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"ed5cc101-503e-4a1f-84a6-5e9dc25254fd\"><ac:plain-text-body><![CDATA[1. ssh username@hostipaddress (mumbai Region)\n2. ssh username@54.169.65.159 (bastion Server)\n3. ssh -i key-pair username@ipaddress (singapore Region)\n4. sudo du -hsx /*\n5. for i in G M K; do du -hsx /* | grep \\\"[0-9]$i\\b\\\" | sort -nr; done 2>/dev/null | head -n 14\n6. NOTE: Make sure we are not running the 5th command Incase if we noticed HIGH CPU Usage on Machine, which is \n\"for i in G M K; do du -hsx /* | grep \\\"[0-9]$i\\b\\\" | sort -nr; done 2>/dev/null | head -n 14\"]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p></li><li><p class=\"auto-cursor-target\">The above commands will help to analyse the Disk space and files occupying high space. So, paste the details of process and current usage of the alert to responsible Engineers.</p><p class=\"auto-cursor-target\">Contact sheet will be available:&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1he8GxwKL9UTpli2xBoGuSIm0q0Vk7Mf4oKq66j8nA4o/edit#gid=0\">https://docs.google.com/spreadsheets/d/1he8GxwKL9UTpli2xBoGuSIm0q0Vk7Mf4oKq66j8nA4o/edit#gid=0</a>. Find screen shots for reference<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 5.05.52 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /><br /></p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"c5c39233-3539-48e3-8e95-7b1632bf389e\"><ac:rich-text-body>Make sure the space of Log files are in threshold limits.<br />Be careful while executing commands.<br />Escalate things on priority basis.</ac:rich-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p></li></ol><h2>Solution for Keepalive Issue&nbsp;</h2><p>First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;Check the client Private IP address of the alert from dashboard accordingly.<br />Singapore:&nbsp;&nbsp;<a href=\"http://sensu.meesho.com:9090/#/clients\">http://sensu.meesho.com:9090/#/clients</a><br />Mumbai:&nbsp;<a href=\"http://sensu.meesho.co:9090/#/clients\">http://sensu.meesho.co:9090/#/clients</a><br />On Dashboard you will be quickly able to see the &quot;History&quot; on config of alert to understand the pattern of alert.<br /><br /></li><li><p class=\"auto-cursor-target\">Go to EC2 Search on AWS console &rarr; <a href=\"https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Home\">https://ap-south-1.console.aws.amazon.com/ec2/v2/home?region=ap-south-1#Home</a>:<br />Copy the Client private Ipaddress from sensu dashboard and search in Respective(Singapore/Mumbai) Ec2 instances search to get more details about the Host machine. We can quickly monitor the current DISK&nbsp; usage of the host and parallelly take ONLY the PRIVATE IP address of the machine and try to login from BASH ,most probably you wouldn't be able to access it. Escalate swiftly.<br />Use either IPaddress to check the instance or Client Name. Better to search with Private IP.<br /><br /></p></li><li>Contact sheet will be available:&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1he8GxwKL9UTpli2xBoGuSIm0q0Vk7Mf4oKq66j8nA4o/edit#gid=0\">https://docs.google.com/spreadsheets/d/1he8GxwKL9UTpli2xBoGuSIm0q0Vk7Mf4oKq66j8nA4o/edit#gid=0</a>. Find screen shots for reference.<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 5.17.56 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-05 at 5.15.50 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /><br /></li></ol><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"3c09ac3c-db61-4f72-ae88-fdeed28cd18d\"><ac:rich-text-body>Make sure KEEPALIVE alerts are escalated on priority basis.</ac:rich-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p><p><br /></p><h2>Problem For AWS Hosted Services related Alerts&nbsp;</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"d682bfc9-f190-4ff2-aa99-02ddab991fc7\"><ac:rich-text-body>If any issues comes under RDS wrt CPU | Memory | Disk | DB Connections, IOPS Related (Read/Write), then what are the steps that we need to follow, is mentioned below -</ac:rich-text-body></ac:structured-macro><h2>Solution for Fetching Logs from S3 (CPU/Disk/HighResponseTime/API)</h2><p>Depends on the Dev/Operational Team requirements/Requests, if they need any logs for troubleshooting the alerts like CPU/Disk/HighResponseTime/API.</p><ol><li><p class=\"p1\">Go to respective alert from Sensu Dashboard, as an example we are using this &ldquo;TG-data-p-metabase-tg-HTTPCode_Target_4XX_Count&rdquo; so, pick the TargetGroup of this alert which is &ldquo;data-p-metabase&rdquo; and search in EC2 TargetGroup tab to find the machine related information. Make sure you are choosing right region.</p></li><li class=\"li1\">Now, search the Loadbalancer associated to the targetGroup(data-p-metabase), in details tab of TargetGroup(data-p-metabase) page you will find the Load balancer associated to the TargetGroup(data-p-metabase). Navigate to the respective LoadBalancer tab to find the S3 bucket details, they will appear on details tab.<br /><br /></li><li class=\"li1\">Copy the S3 bucket log path and go to S3 search tab on AWS console and search the bucket/log_name to navigate inside to find the logs/logs.gz for the particular time stamps and download them on local machine.</li><li class=\"li1\">Use commands accordingly to gather the required information, you will able to see response time and api in the output taken for our example alert &ldquo;Target_4XX_Count&rdquo;.<p class=\"auto-cursor-target\"><br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"3b45746a-d8ab-4d3a-9611-586bccfcf7f9\"><ac:plain-text-body><![CDATA[cat *.log | awk -F' ' '{if($9~/^4/){print $7,$14}}'| sort -nrk 2\n\nOutput: 0.005 https://metabase.meesho.com:443/api/setup/admin_checklist]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p></li><li>Find the detailed screenshots below for reference and share the gathered details to respective Dev/Operational Team.<br /><br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-07 at 1.51.26 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-07 at 1.53.19 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-07 at 2.16.50 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-07 at 2.17.44 PM.png\" ri:version-at-save=\"1\" /></ac:image></li></ol><p><br /></p><h2>RDS: Solution for DB Connections Issue</h2><p>First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;<br /><br /></li><li><p class=\"auto-cursor-target\">select RDS service in AWS console</p></li><li>select the RDS for which connection related alert triggered</li><li><p class=\"auto-cursor-target\">click on monitoring tab<br /><ac:image ac:width=\"552\"><ri:attachment ri:filename=\"Screen Shot 2019-02-10 at 11.14.36 PM.png\" ri:version-at-save=\"2\" /></ac:image></p></li><li>select DB Connection count metric</li></ol><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this will show current connections count in database.</p><p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screen Shot 2019-02-10 at 11.14.25 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p><br /></p><p class=\"auto-cursor-target\">&nbsp;Commands and script that we need to run to check connection details:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"7b29dc08-ad76-4fcf-a4ac-3702c84b9d13\"><ac:parameter ac:name=\"language\">coldfusion</ac:parameter><ac:parameter ac:name=\"theme\">RDark</ac:parameter><ac:parameter ac:name=\"title\">Code Block</ac:parameter><ac:plain-text-body><![CDATA[1. below command give list of client that are currently connected to rds:\n   select host  from INFORMATION_SCHEMA.PROCESSLIST;\n2. run below script to get count of connection made by each client machines to rds:\n   output=$( mysql -uuser_name -ppassword -hhostname << EOF\n\n   select user, host  from INFORMATION_SCHEMA.PROCESSLIST\n   EOF\n   )\n   echo \"$output\" > /tmp/outfile\n   cat /tmp/outfile|awk '{print $2}'|awk -F':' '{print $1}'|sort |uniq -c]]></ac:plain-text-body></ac:structured-macro><p>Below is the output of the above script, where first count showing count and 2nd column showing client IP.&nbsp;</p><p><ac:image ac:width=\"552\"><ri:attachment ri:filename=\"Screen Shot 2019-02-10 at 11.55.35 PM.png\" ri:version-at-save=\"1\" /></ac:image>&nbsp;</p><p><br /></p><p>We can also add more filters in query.</p><p>let say, we want to fetch connection details only for select queries, below will be sql query for that:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"70280f3a-b3e6-42d5-8681-1e99206ac884\"><ac:parameter ac:name=\"language\">coldfusion</ac:parameter><ac:parameter ac:name=\"theme\">RDark</ac:parameter><ac:parameter ac:name=\"title\">Code Block</ac:parameter><ac:plain-text-body><![CDATA[1. run below script to get count of connection made by each client machines to rds(only select queries):\n   output=$( mysql -uuser_name -ppassword -hhostname << EOF\n\n   select user, host, info  from INFORMATION_SCHEMA.PROCESSLIST where info like 'select%'  \n   EOF\n   )\n   echo \"$output\" > /tmp/outfile\n   cat /tmp/outfile|awk '{print $2}'|awk -F':' '{print $1}'|sort |uniq -c]]></ac:plain-text-body></ac:structured-macro><h2>RDS: Solution for Memory Issue</h2><p>&nbsp; &nbsp; &nbsp; First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;<br /><br /></li><li><p class=\"auto-cursor-target\">After that check total number of client connections in performance&nbsp;<br /><br /></p></li><li><p class=\"auto-cursor-target\"><span style=\"color: rgb(36,39,41);\">Free-able memory field is used by MySQL for buffering and caching for it's own processes. It is normal for the amount of Free-able memory to decrease over time</span></p></li></ol><p><span style=\"color: rgb(36,39,41);\">&nbsp; &nbsp; &nbsp; 4. JOINs and ORDER BY queues are always frequent memory consumers as they'll need to allocate buffers for work tables. Depending on your engine or schema, </span></p><p><span style=\"color: rgb(36,39,41);\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; that could also lead to temporary or long term disk usage, evidenced by minimum FreeStorageSpace drops and spikes in maximum WriteIOPs. Or even a sudden change in SwapUsage.</span></p><p><span style=\"color: rgb(36,39,41);\">&nbsp; &nbsp; &nbsp; 5. Check performance insight for that RDS</span></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screen Shot 2019-02-18 at 6.28.29 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p><br /></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;This will list down current running queries, user and client IP detail.</p><p>&nbsp; &nbsp; &nbsp; 6. read this blog for more detail info:&nbsp;<a href=\"https://bobcares.com/blog/fix-mysql-high-memory-usage/\">https://bobcares.com/blog/fix-mysql-high-memory-usage/</a>&nbsp;</p><ol><li><p class=\"auto-cursor-target\"><br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"7e16fd42-4d23-4654-8c61-7f6edf1ecaed\"><ac:parameter ac:name=\"language\">coldfusion</ac:parameter><ac:parameter ac:name=\"theme\">RDark</ac:parameter><ac:parameter ac:name=\"title\">Code Block</ac:parameter><ac:plain-text-body><![CDATA[1. show full processlist;\ncheck all long running queries  \n\n2. run below command to check long running select queries:\nselect user, host, info,time from INFORMATION_SCHEMA.PROCESSLIST where info like 'select%' order by time desc;\n\n3. check slow query logs in cloud watch and performance insight.\n]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p></li></ol><p><br /></p><p><br /></p><h2>RDS: Solution for High CPU Utilization</h2><p>&nbsp; &nbsp; &nbsp; &nbsp;First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;<br /><br /></li></ol><p>&nbsp; &nbsp; &nbsp; &nbsp;2.&nbsp;these kind of incidents are derived of a spike in client application's load or users doing things without full knowledge of their impact.<br /><br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; check if there is spike in db connection count ?<br /><br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Does the Read or Write IO increase during the incident?</p><p>&nbsp; &nbsp; &nbsp; 3. check slow query logs in cloudwatch logs details</p><p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screen Shot 2019-02-19 at 1.03.27 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p><br /></p><ol><li><p class=\"auto-cursor-target\">Below are some sql queries that will help in identifying issues&nbsp;</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"dd2cdc04-abda-4f5b-b44b-ef5acdc84fb4\"><ac:parameter ac:name=\"language\">coldfusion</ac:parameter><ac:parameter ac:name=\"theme\">RDark</ac:parameter><ac:parameter ac:name=\"title\">Code Block</ac:parameter><ac:plain-text-body><![CDATA[1. FULL PROCESSLIST COMMAND\n\tshow full processlist;\n2. Get list of locked tables for current database\n\tSHOW OPEN TABLES WHERE In_use > 0\n3. Run below command to check current running process sort by their run time\n\tselect user, host, info, time from INFORMATION_SCHEMA.PROCESSLIST where info like 'select%' order by time desc;\n\n    host: client machine\n    user: db user\n    info: query\n4. Run below command to check status of innodb\n   show engine innodb status;\n    ]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p></li></ol><h2>RDS: Solution for High Read/Write IOPS</h2><p>&nbsp; &nbsp; &nbsp; &nbsp;First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;</li><li>Below are the steps you need to check :&nbsp;&nbsp;</li></ol><p>&nbsp; &nbsp; &nbsp; &nbsp;What are you running on RDS? (show full processlist)</p><p>&nbsp; &nbsp; &nbsp; &nbsp;How many people using it concurrently? (no of users and number of connections)</p><p>&nbsp; &nbsp; &nbsp; &nbsp;How much storage is used? (remaining storage in GB)</p><p>&nbsp; &nbsp; &nbsp; &nbsp;How big are the average files being used in the RDS?</p><p>&nbsp; &nbsp; &nbsp; &nbsp;Check with Dev Team as well if they are running some cron job which is reading and write data in bulk.</p><p><br /></p><p>&nbsp; &nbsp; &nbsp; 1.&nbsp;Below are some sql queries that will help in identifying issues&nbsp;</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"0101398c-f5ab-4925-b80c-89991f16e5de\"><ac:parameter ac:name=\"language\">coldfusion</ac:parameter><ac:parameter ac:name=\"theme\">RDark</ac:parameter><ac:parameter ac:name=\"title\">Code Block</ac:parameter><ac:plain-text-body><![CDATA[1. FULL PROCESSLIST COMMAND:\n\tshow full processlist;\n\n2. Command to check size of each database:\n\tselect table_schema, CONCAT(ROUND( sum((data_length+index_length)/1024/1024)/1024, 2), 'G') AS MB from information_schema.tables group by 1;\n Note- please don't run above command if cpu in more than 90%\n\n3. Command to check binary log files:\n\tshow binary logs;\n\n4. Check whether we are rotating binary log or not:\n    show global variables like 'max_binlog_size';\n   if value of this variable is very high like in GB's, then we need to change it. \n   \n\n    ]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p><p><br /></p><h2>Elastic Cache(Redis) Common Troubleshooting</h2><p>&nbsp; &nbsp; &nbsp; &nbsp;First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;</li><li>select Redis as a service in aws</li><li>select cluster for which we received alert</li><li>click on monitoring and check the metric for which alert is triggered&nbsp;</li></ol><p><br /></p><p>Below are important Redis metric with definition:</p><p>&nbsp; &nbsp;&nbsp;</p><p><strong>Cache Misses:</strong></p><p>CacheMisses The number of unsuccessful read-only key lookups in the main dictionary. This is derived from keyspace_misses at Redis INFO.</p><p><strong>CurrConnections:</strong></p><p>The number of client connections, excluding connections from read replicas. ElastiCache uses two to three of the connections to monitor the cluster in each case. This is derived from the connected_clients statistic at Redis INFO.</p><p><strong>Evictions:</strong></p><p>The number of keys that have been evicted due to the maxmemory limit. This is derived from the evicted_keys statistic at Redis INFO</p><p><strong>FreeMemory:</strong></p><p>Amount of free memory left in redis server.</p><p><br /></p><p>1.&nbsp;Below are some redis cli command that will be helpful in debugging redis issue:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"4a5ab6fa-302e-47b7-b11a-ab0f69dad57d\"><ac:parameter ac:name=\"language\">coldfusion</ac:parameter><ac:parameter ac:name=\"theme\">RDark</ac:parameter><ac:parameter ac:name=\"title\">Code Block</ac:parameter><ac:plain-text-body><![CDATA[Login to any production machine where redis-cli already installed.\n\n1- Command to connect redis server\nredis-cli -h host -a (if authentication enabled)\nredis-cli -h host    (if authentication not enabled)\n\n2- Memory details\n>info memory\ncheck the fields like: used_memory and \tmaxmemory\n\n3- Connection info\n>INFO clients\n>client list\n\naddr: address/port of the client\nfd: file descriptor corresponding to the socket\nage: total duration of the connection in seconds\nidle: idle time of the connection in seconds\n\n4- Command to get info related to keys\n> INFO keyspace\n\n5- Command to get basis stat of redis\n> INFO Stats\n\n6- Command to get basic stat of redis server info like (redis version, uptime, port)\n> Info Server\n\n7- Command to check replication in redis\n> Info Replication\n \n    ]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p><p class=\"auto-cursor-target\"><br /></p><h2>Elastic Search: Troubleshooting</h2><p><br /></p><p>This is completely managed by AWS. so most of the time, we need to check on cloudwatch for any alert.</p><p>Below are some useful commands that will give basic idea about elastic search current stat:</p><p><br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"a6ec032b-b21d-4a4d-b62e-786c12fbe948\"><ac:parameter ac:name=\"language\">coldfusion</ac:parameter><ac:parameter ac:name=\"theme\">RDark</ac:parameter><ac:parameter ac:name=\"title\">Code Block</ac:parameter><ac:plain-text-body><![CDATA[1. Command to check number of nodes in cluster \n   curl 'http://elastic_search_url/_cat/nodes'\n\nsample output:\nx.x.x.x 11 94 0 0.00 0.00 0.00 m  - yGosoYh\nx.x.x.x 13 94 0 0.01 0.04 0.01 m  * RRx2KpS\nx.x.x.x  21 88 0 0.00 0.00 0.00 di - LPzybav\nx.x.x.x  9 94 0 0.05 0.01 0.00 m  - pXA4aKR\nx.x.x.x  22 88 0 0.00 0.00 0.00 di - KxE-KR8\n\nwhere m is master. * means current master and di means data node.\n\n\n2. List of indices\n   curl 'http://elastic_search_url/_cat/indices'\n\nsample output: \n\ngreen open competition_images bsNLBTeiRHaGlzYTNCnnkA 5 1   29068 0 197.2mb 98.6mb\ngreen open master_images      qT_sIbjISWadftqrLZtgdg 5 1 1496338 0   9.1gb  4.5gb\n\n\"Green\" is an indication of the health of the index. It means that all primary shards are available and they each have at least one replica. “Yellow” would mean that all primary shards are available, but they don’t all have a replica. “Red” means not all primary shards are available.\n\n3. Cluster health status\n   curl -XGET 'elastic_search_url/_cluster/health?pretty'\nsample output:\n\n{\n  \"cluster_name\" : \"847438129436:bac-p-scraper\",\n  \"status\" : \"green\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 5,\n  \"number_of_data_nodes\" : 2,\n  \"active_primary_shards\" : 10,\n  \"active_shards\" : 20,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 0,\n  \"delayed_unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0,\n  \"number_of_in_flight_fetch\" : 0,\n  \"task_max_waiting_in_queue_millis\" : 0,\n  \"active_shards_percent_as_number\" : 100.0\n}    ]]></ac:plain-text-body></ac:structured-macro><p>First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</p><ol><li>Go to respective dashboard&nbsp;&rarr;&nbsp;</li><li>select EladsticSearch Service as a service in aws</li><li>select cluster for which we received alert</li><li>click on monitoring and check the metric for which alert is triggered&nbsp;</li></ol><p class=\"auto-cursor-target\"><br /></p><p class=\"auto-cursor-target\">SearchLatency::<span style=\"color: rgb(68,68,68);\">The average time, in milliseconds, that it takes a shard to complete a search operation.</span></p><p class=\"auto-cursor-target\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\">SearchRate::&nbsp;</span></span>The total number of search requests per minute for all shards on a node. A single call to the<span>&nbsp;</span><code class=\"code\">_search</code><span>&nbsp;</span>API might return results from many different shards. If five of these shards are on one node, the node would report 5 for this metric, even though the client only made one request.</p><p><br /></p><p><br /></p><p><strong>Lambda: Troubleshooting</strong></p><p><br /></p><p>In Lambda apart from basic AWS metric like&nbsp;Errors, Duration, Throttles we configured some filter based error as well.</p><p>Ex:&nbsp;alter-image-prod</p><p>Steps to troubleshoot lambda alert that we configured on the basis of error pattern</p><p>1- Select cloudwatch service</p><p>2- Click on &quot;<strong>Logs</strong>&quot;</p><p>3- Search &quot;log group name&quot; for the lambda</p><p><ac:image ac:width=\"605\"><ri:attachment ri:filename=\"Screen Shot 2019-02-20 at 12.08.38 PM.png\" ri:version-at-save=\"1\" /></ac:image>&nbsp;</p><p>4- Click on &quot;search log group&quot;</p><p>5- Enter the string that we configured in our cloudwatch alert. example &quot;BLOB_ERROR&quot;</p><p><ac:image ac:width=\"605\"><ri:attachment ri:filename=\"Screen Shot 2019-02-20 at 12.14.41 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p>Note- pass search pattern in double quotes.&nbsp;</p><p>6- Select the time range, it will start searching entered pattern in the logs and will display result after 5 to 10 mins</p><p><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screen Shot 2019-02-20 at 12.19.10 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p class=\"auto-cursor-target\"><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 3227
    },
    {
      "id": "41254916",
      "title": "ElastiCache Metrics Data",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/41254916",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/41254916/ElastiCache+Metrics+Data",
      "created": "2019-02-08T12:20:09.359Z",
      "content": "<p class=\"auto-cursor-target\"><br /></p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"bcc7f572-ea6c-4adc-b14f-dadb1403d162\"><ac:rich-text-body><h1 class=\"topictitle\" style=\"\">Metrics for Redis</h1></ac:rich-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"c924ae7a-bc45-48d8-bc26-c0b1f2df8575\"><ac:rich-text-body><p><span>Below Mentioned Metrics are available for Redis&nbsp; .Here We can see brief description about metrics and Unit of the Metrics&nbsp;</span></p></ac:rich-text-body></ac:structured-macro><p><span><strong>CPUUtilization:&nbsp;</strong>The percentage of CPU utilization for the entire host(<span>Unit:Percent</span>)</span></p><p><span><strong>EngineCPUUtilization</strong>: provides access to the Redis process CPU utilization to gain better insights into your Redis workloads(<span> Units:Percent</span>).</span></p><p><span><strong>FreeableMemory:&nbsp;</strong>The amount of free memory available on the host. This is derived from the RAM, buffers and cache that the OS reports as freeable.(<span>Unit:Bytes</span>)</span></p><p><span><strong>SwapUsage:</strong>The amount of swap used on the host.(<span>Unit:Bytes</span>)</span></p><p><span><strong>Evictions</strong>:</span><span>Number of evicted keys due to </span><span>max memory</span><span> limit(<span>Unit:Count</span>)</span></p><p><span><span><strong>Reclaimed</strong>:The total number of key expiration events.(<span>Unit:Count</span>)</span></span></p><p><span><strong>CurrConnections</strong>:&nbsp;</span><span>Number of client connections ,excluding connections from replicas(<span>Unit:Count</span>)</span></p><p><span><strong>CacheHits</strong>:The number of successful read-only key lookups in the main dictionary.(<span>Unit:Count</span>)</span></p><p><span><strong>CacheMisses</strong>:The number of unsuccessful read-only key lookups in the main dictionary(<span>Unit:Count</span>)</span></p><p><span><strong>NewConnections</strong>:</span><span>Total number of connections accepted by the server(<span>Unit:Count</span>)</span></p><p><span><strong>NetworkBytesIn</strong>:The number of bytes the host has read from the network.(<span>Unit:Bytes</span>)</span></p><p><span><strong>NetworkBytesOut</strong>:The number of bytes sent out on all network interfaces by the instance.(<span>Units:Bytes</span>)</span></p><p><span><span><strong>ActiveDefragHits</strong>:The number of value reallocations per minute performed by the active defragmentation process.(<span>Unit:Number</span>)</span></span></p><p><span><strong>BytesUsedForCache</strong>:The total number of bytes allocated by Redis for all purposes, including the dataset, buffers, etc.(<span>Units:Bytes</span>)</span></p><p><span><strong>ReplicationBytes</strong>:the number of bytes that the primary is sending to all of its replicas. </span><span>This metric is representative of the write load on the replication group.(<span>Units:Bytes</span>)</span></p><p><span><strong>ReplicationLag</strong>:This metric is only applicable for a node running as a read replica. It represents how far behind, in seconds, the replica is in applying changes from the primary node.(</span><span>Unit:Seconds)</span></p><p><span><strong>NetworkPacketsIn</strong>: The number of packets received on all network interfaces by the instance. This metric identifies the volume of incoming traffic in terms of the number of packets on a single instance.(<span>Unit:Count</span>)</span></p><p><span><strong>NetworkPacketsOut</strong>:The number of packets sent out on all network interfaces by the instance. This metric identifies the volume of outgoing traffic in terms of the number of packets on a single instance.(<span>Unit:Count</span>)</span></p><p><br /></p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"b6b2cb72-07bc-425d-86a6-1707e3009de7\"><ac:rich-text-body><span><strong>CPUUtilization,&nbsp;<span><strong>EngineCPUUtillization,&nbsp;<span><strong>FreeableMemory,&nbsp;<span><strong>SwapUsage,&nbsp;<span><strong>CacheMisses,&nbsp;</strong></span><span><strong>Evictions. </strong></span></strong></span></strong></span></strong></span></strong>These are the <strong>Important/Mandatory</strong>&nbsp;Metrics which need to be Monitor for Uninterrupted services of Redis</span></ac:rich-text-body></ac:structured-macro><p><br /></p><p><br /></p><hr /><p><span style=\"color: rgb(0,128,0);\"><strong>How to find existing configured Redis Services:</strong></span></p><hr /><p><span>Go to services tab in AWS console and select region which you want to check then search with ElastiCache in services Tab.then your can see the Below Screen .</span></p><p><br /></p><p><span><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 5.07.26 PM.png\" ri:version-at-save=\"1\" /></ac:image></span></p><p><span>Then Click on Redis on left top of screen (<span>&nbsp;Highlighted in above image</span>).Now you can see the existing configured services in Blow Image&nbsp;<br /></span><span>Here Redis service name is front-p-redis</span></p><p><br /></p><p><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 5.08.53 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p>Click on Redis service name (<span>front-p-redis</span>) then you are able to see all metrics stats for the selected redis as show in Below Image</p><p><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 5.10.00 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><hr /><p><span style=\"color: rgb(0,51,102);\"><strong>How to Find Existing Configured Metrics for Redis Server</strong></span></p><hr /><p><br /></p><p>Go to the Services tab in aws Console and search with CloudWatch then your are able to see the following cloudwacth dashboad as shown in&nbsp; below image</p><p><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 5.11.10 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p><br /></p><p>Then Click on Alarms (Highlighted in above Image) Then You are able to see all the metrics configured in that Region (all States)</p><p>Then Click Search with Redis name in search bar then you are able to find configured metrics for searched redis as show in Below Image</p><p><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 5.20.39 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p class=\"auto-cursor-target\"><br /></p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"4836be22-27af-4282-8301-4cde86556490\"><ac:rich-text-body><p>For More Information Please&nbsp; Refer Blow Links</p><p><br /><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.HostLevel.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.HostLevel.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.Redis.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.Redis.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.WhichShouldIMonitor.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.WhichShouldIMonitor.html</a></p></ac:rich-text-body></ac:structured-macro><p><br /></p><p><br /></p><h2>Related articles</h2><p><ac:placeholder>The content by label feature displays related articles automatically, based on labels you choose. To edit options for this feature, select the placeholder below and tap the pencil icon.</ac:placeholder></p><p><ac:structured-macro ac:name=\"contentbylabel\" ac:schema-version=\"4\" ac:macro-id=\"ea7466cc-d3c4-4f28-b183-6ec12cdf9ccb\"><ac:parameter ac:name=\"showLabels\">false</ac:parameter><ac:parameter ac:name=\"max\">5</ac:parameter><ac:parameter ac:name=\"spaces\">com.atlassian.confluence.content.render.xhtml.model.resource.identifiers.SpaceResourceIdentifier@77fe2a7d</ac:parameter><ac:parameter ac:name=\"showSpace\">false</ac:parameter><ac:parameter ac:name=\"sort\">modified</ac:parameter><ac:parameter ac:name=\"reverse\">true</ac:parameter><ac:parameter ac:name=\"type\">page</ac:parameter><ac:parameter ac:name=\"cql\">label in ( &quot;elasticache&quot; , &quot;metrics&quot; ) and type = &quot;page&quot; and space = &quot;DEVOPS&quot;</ac:parameter><ac:parameter ac:name=\"labels\">ElastiCache Metrics</ac:parameter></ac:structured-macro></p><p><br /></p><ac:structured-macro ac:name=\"details\" ac:schema-version=\"1\" ac:macro-id=\"94e31663-4e9f-4a5e-8717-bce168706da2\"><ac:parameter ac:name=\"hidden\">true</ac:parameter><ac:rich-text-body><p class=\"auto-cursor-target\"><br /></p><table class=\"wrapped\"><tbody><tr><th>Related issues</th><td><br /></td></tr></tbody></table><p class=\"auto-cursor-target\"><br /></p></ac:rich-text-body></ac:structured-macro><p><br /></p><p class=\"auto-cursor-target\"><br /></p><p class=\"auto-cursor-target\"><br /></p><p><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 630
    },
    {
      "id": "41287689",
      "title": "BASTION ACCES",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/41287689",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/41287689/BASTION+ACCES",
      "created": "2019-02-09T08:01:53.616Z",
      "content": "<p class=\"zw-paragraph\" style=\"text-align: center;margin-left: 30.0px;\"><strong><span style=\"color: rgb(0,0,0);\">Granting Access to Bastion Server</span></strong></p><p class=\"zw-paragraph\" style=\"text-align: left;\"><strong><span style=\"color: rgb(0,0,0);text-decoration: none;\">Prerequisites</span></strong><span style=\"color: rgb(0,0,0);\">:</span></p><ol><li><p class=\"zw-paragraph\" style=\"text-align: left;\"><span style=\"color: rgb(0,0,0);\">Make sure you have access to the bastion server first, If not reach out Devops Team.</span></p></li><li><p class=\"zw-paragraph\" style=\"text-align: left;\"><span style=\"color: rgb(0,0,0);\">Need the username and ssh-key(publickey) of the user to whom you are granting access.</span></p></li><li><p class=\"zw-paragraph\" style=\"text-align: left;\"><span style=\"color: rgb(0,0,0);\">You should have access to github of Meesho-devops and you should have had cloned it in your local machine before you begin.</span></p></li></ol><p class=\"zw-paragraph heading0\" style=\"margin-left: 0.0in;\"><br class=\"zw-br\" /><strong><span style=\"color: rgb(0,0,0);\">Steps</span></strong><span style=\"color: rgb(0,0,0);\">:</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">Check if you have legit access&nbsp; to clone devops-meesho repository.&nbsp;</span></p><p class=\"zw-paragraph heading0\" style=\"margin-left: 0.0in;\"><span style=\"color: rgb(0,0,0);text-decoration: none;\"><strong>TO CLONE MEESHO-DEVOPS REPO</strong>:</span></p><p class=\"zw-paragraph heading0\" style=\"margin-left: 0.0in;\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">Create a fresh directory some where safe on your local machine for cloning. get the url from github(devops-meesho)</span></p><ol><li><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">once you are inside dir, clone the devops-meesho repo by using below command. Once its executed successfully. check the folder devops-meesho</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">$ <strong><em>git clone url_of_devops_meesho</em></strong></span></p><hr /><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">meeshos-MacBook-Air-3:devops-meesho mahammedyasin$ <strong><em>pwd</em></strong></span><br class=\"zw-br\" /><em><span style=\"color: rgb(0,0,0);text-decoration: none;\">/Users/mahammedyasin/Yasin/devops-meesho</span></em></p><hr /><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">meeshos-MacBook-Air-3:devops-meesho mahammedyasin$ <strong><em>ls -lrth</em></strong></span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">total 8</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">-rw-r--r-- 1 mahammedyasin<span>&nbsp;</span> staff 95B Feb 8 06:03 <a href=\"http://README.md\">README.md</a></span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">drwxr-xr-x 5 mahammedyasin staff 160B Feb 8 06:03 ansible</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">drwxr-xr-x 4 mahammedyasin staff 128B Feb 8 06:03 scripts</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">drwxr-xr-x 3 mahammedyasin staff 96B Feb 8 06:03 terraform-modules</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">drwxr-xr-x 3 mahammedyasin staff 96B Feb 8 06:03 terraform<br /><br /><ac:image><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 3.30.56 PM.png\" ri:version-at-save=\"1\" /></ac:image></span></p><hr /><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\"><strong>Compare/Cross-check</strong> the data on GitHub vs your local machine.<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 1.36.40 PM.png\" ri:version-at-save=\"1\" /></ac:image></span><br class=\"zw-br\" /><br class=\"zw-br\" /></p></li><li><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">Now, create a file <strong><em>hosts</em></strong>&nbsp; on below path, <strong><em>hosts</em></strong> file should include Bastion IP address in it like the below. Navigate to <em>/etc/ansible</em></span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">&nbsp;</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">meeshos-MacBook-Air-3:ansible mahammedyasin$ <em><strong>pwd</strong></em>&nbsp;</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">/etc/ansible</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">meeshos-MacBook-Air-3:ansible mahammedyasin$ <em><strong>ls</strong></em></span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">hosts</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">meeshos-MacBook-Air-3:ansible mahammedyasin$ <em><strong>cat hosts</strong></em></span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">54.169.65.159<br /><br /></span></p></li><li><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">Now on your local machine go to the path where you have devops-meesho cloned repo exists. In my case it is&nbsp;</span><br class=\"zw-br\" /><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">meeshos-MacBook-a. Air-3:lib mahammedyasin$ <strong><em>pwd</em></strong></span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">/Users/mahammedyasin/Yasin/devops-meesho/ansible/lib/</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">&nbsp;</span></p></li><li><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">You will see list of scripts available in the path &ldquo;devops-meesho/ansible/lib&rdquo; as below, WE USE ONLY &quot;</span><span class=\"s1\">setup_bastion_access.yml&quot; ,</span><span style=\"color: rgb(0,0,0);text-decoration: none;\">now execute the below command(NOTE: Modify command according to the user) to grant access.<br /></span><span style=\"color: rgb(0,0,0);text-decoration: none;\"><strong>Scripts</strong>:&nbsp;<br /><ac:image><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 3.41.47 PM.png\" ri:version-at-save=\"1\" /></ac:image></span><strong><br /><br />Example command below</strong>:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"1ec801e3-1c0c-4cd0-b06e-ad5d6156074b\"><ac:plain-text-body><![CDATA[ansible-playbook -i /etc/ansible/hosts -u Mahammed.yasin -e user_name=‘arpit.jaiswal’ -e file_path='/Users/mahammedyasin/Downloads/arpit’ setup_bastion_access.yml ]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><strong>CODE NEEDED TO MODIFIED ACCORDINGLY IN HIGHLIGHTED PLACES:<br />ansible-playbook -i /etc/ansible/hosts -u <span style=\"color: rgb(255,0,0);\">ourFirst_name.ourLast_name </span>-e user_name=&lsquo;<span style=\"color: rgb(255,0,0);\">userFirst_name.UserLast_name</span>&rsquo; -e file_path='<span style=\"color: rgb(255,0,0);\">/any/path/user-key</span>&rsquo; setup_bastion_access.yml <br /></strong></p></li><li><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">Before running the script we have to modify the above command according to the user you are granting access.</span></p></li><li><p class=\"zw-paragraph heading0\"><strong>INFO</strong> of generating ssh-key<br /><u>STEPS TO GENERATE SSH-KEY</u><strong>:</strong><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\"><strong>a</strong>. Public key can be generated by command &quot;ssh-keygen&quot; and it will auto store at the ssh path as shown</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\">below.&nbsp; (<strong>Note</strong>: ssh-keygen should execute on the user machine)<br /><ac:image ac:border=\"true\" ac:height=\"400\" ac:width=\"449\"><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 3.46.10 PM.png\" ri:version-at-save=\"1\" /></ac:image></span><br /><strong>b</strong>. navigate to&nbsp;<br /><span class=\"s1\">&nbsp; &nbsp; cd ~/.ssh<br /><ac:image><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 4.10.01 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /></span></p><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\"><strong>c</strong>. Take the id_rsa.pub key from user and save it in some local path, in my case i have saved the user id_rsa.pub ssh key in below dir in file arpit<br /></span><em>/Users/mahammedyasin/Downloads/arpit</em><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\"><strong>c</strong>. Now take the username of the user, in my case it is <em>arpit.jaiswal</em></span></p></li><li><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">Now, use this below command at the exact path where &ldquo;<em>setup_bastion_access.yml</em>&rdquo; is present. i.e&nbsp;</span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\"><strong>a</strong>. Path: <strong><em>/devops-meesho/ansible/lib</em></strong></span><br class=\"zw-br\" /><span style=\"color: rgb(0,0,0);text-decoration: none;\"><strong>b</strong>. Now in my case i ran the below:<br /></span></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"6ec18717-6d01-45e0-bfba-2d7fcef51ab0\"><ac:plain-text-body><![CDATA[ansible-playbook -i /etc/ansible/hosts -u Mahammed.yasin -e user_name=‘arpit.jaiswal’ -e file_path='/Users/mahammedyasin/Downloads/arpit’ setup_bastion_access.yml ]]></ac:plain-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><ac:image><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 1.12.33 PM.png\" ri:version-at-save=\"1\" /></ac:image></p></li><li><p class=\"zw-paragraph heading0\"><span style=\"color: rgb(0,0,0);text-decoration: none;\">Script will take care rest of things. Once you ran it successfully. To check the access has granted to the user or not, login in to Bastion server. i.e 54.169.65.159 and navigate to path: /<em>home</em> and use <em><strong>ls</strong></em> to list the data and you should be able to see the newly added user. In my scenario it will list of dir on</span><span style=\"color: rgb(0,0,0);text-decoration: none;\"><span>&nbsp;</span>arpit.jaiswal<br /></span><br class=\"zw-br\" /><ac:image><ri:attachment ri:filename=\"Screenshot 2019-02-08 at 1.12.52 PM.png\" ri:version-at-save=\"1\" /></ac:image></p></li></ol><p class=\"zw-paragraph\" style=\"text-align: left;\"><br /></p><p class=\"zw-paragraph\" style=\"text-align: left;\"><span class=\"EOP\" style=\"text-decoration: none;\">&nbsp;</span></p><p class=\"zw-paragraph\" style=\"text-align: left;\"><span class=\"EOP\" style=\"text-decoration: none;\">&nbsp;</span></p><p class=\"zw-paragraph\" style=\"text-align: center;\"><span class=\"EOP\" style=\"text-decoration: none;\">&nbsp;</span></p>",
      "approach_used": "endpoint_2",
      "word_count": 826
    },
    {
      "id": "42434561",
      "title": "How to Silence Sensu Process and Hosts",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/42434561",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/42434561/How+to+Silence+Sensu+Process+and+Hosts",
      "created": "2019-02-12T09:39:44.552Z",
      "content": "<h2>Problem</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"f7347757-cdc5-421d-becb-5eb4d608d8b4\"><ac:rich-text-body><p>According to the Respective Team's requirement or due to any Deployments,Outage,TroubleShooting things, we might need to adjust our monitoring services like SENSU to either Silence/Un-silence.<br />We are monitoring this services/hosts through Uchiwa Dashboard. Links are given below for both regions &amp; if credentials required, contact DevOpsTeam.<br /><br />Singapore:<a href=\"http://sensu.meesho.com:9090/#/events\">http://sensu.meesho.com:9090/#/events</a><br />Mumbai:<a href=\"http://sensu.meesho.co:9090/#/events\">http://sensu.meesho.co:9090/#/events</a></p></ac:rich-text-body></ac:structured-macro><h2>Solution</h2><p>We have provided the detailed steps below on how to silence &amp; un-silence alert/hosts according to the Requests/Requirements.<br /><br />For every Host Machine/Server, we have certain checks placed to it. We have a check for entire host as well. Silencing should be done accordingly.</p><ol><li><strong>Silencing Check --&gt; For entire Host / Individual Check</strong><br />Open the respective Alert and Region Dashboard related to the alert. We are taking an example scenario below.&nbsp;<br /><strong>HOST</strong>:&nbsp;<span style=\"color: rgb(0,0,0);\">platform-p-qwest-pilot-02a<br /></span><strong>Check</strong>:&nbsp;<span style=\"color: rgb(0,0,0);\">UWSGI-PROCESS-CHECK</span><span style=\"color: rgb(0,0,0);\"><br /></span></li><li><span style=\"color: rgb(0,0,0);\">Go to monitoring page.<br />Go to Alerts&nbsp;&rarr; and hover your cursor on to soundIcon as shown below &amp; click on it to open the Silence Pop-Up</span><span style=\"color: rgb(0,0,0);\"><br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-12 at 1.10.29 PM.png\" ri:version-at-save=\"1\" /></ac:image><br />Remember, If you are opting for HOST DOWNTIME, all checks will be Downtime(i.e won't trigger any alerts). Choose as per the requirement wisely.<br /><br /><br /></span></li><li>Now choose the timings according to Deployment Window/Outage as requested.<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-12 at 1.15.44 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /><p class=\"p1\"><br /><br /></p></li><li><p class=\"p1\"><strong>REASON: </strong>as Per devTeam request<br /><strong>Silenced By: </strong>Siddarth<br /><strong>Silence Type: Permanent or Temporary&nbsp;&rarr; </strong>Temporary<br /><strong>If Temporary,ETA: </strong>12April2019 - 6:00am</p><p class=\"p1\">Going forward Please Mandatorily use the above reasons in transaction notes to make our work easy.<br /><br /><br /></p></li><li>Check <strong>Silenced</strong> Tab:<br />Always make sure this after you silence any Check/Host. here we have silenced the &quot;CLIENT:PLATFORM-P-QWEST-PILOT-02A:UWSGI-PROCESS-CHECK&quot;<br /><br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-12 at 1.20.08 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /><br /><br /></li><li><strong>Un-silencing Check:<br /></strong>Once we are good with trouble-shooting/Deployment/Outage. we are good to UnSilencing the alert. We can see if the alert turned <strong>Green</strong>(Good State) or Still <strong>Red</strong>(Critical State), Screenshot below for reference.<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-12 at 1.07.27 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-12 at 1.05.53 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-12 at 1.08.58 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /><br />Once you hit submit, You should see the SoundIcon on panel turn's to normal state.<br /><br /><br /></li></ol><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"f7f8b1dd-5bf4-4e34-85ad-361ae5428766\"><ac:rich-text-body>NOTE: <br />1.Never Silence Entire HostMachine/Server for one check related to it.<br />2.Silence the check as requested and Make sure you are cross-checking the monitor when(Silencing &amp; UnSilencing)<br />3.Use the Comments Format Always and keep it handy.<br /><br /></ac:rich-text-body></ac:structured-macro><h2>Related articles</h2><p><ac:placeholder>The content by label feature displays related articles automatically, based on labels you choose. To edit options for this feature, select the placeholder below and tap the pencil icon.</ac:placeholder></p><p><ac:structured-macro ac:name=\"contentbylabel\" ac:schema-version=\"4\" ac:macro-id=\"374b70a7-889b-4206-8955-bcf4c5177ff6\"><ac:parameter ac:name=\"showLabels\">false</ac:parameter><ac:parameter ac:name=\"max\">5</ac:parameter><ac:parameter ac:name=\"spaces\">com.atlassian.confluence.content.render.xhtml.model.resource.identifiers.SpaceResourceIdentifier@77fe2a7d</ac:parameter><ac:parameter ac:name=\"showSpace\">false</ac:parameter><ac:parameter ac:name=\"sort\">modified</ac:parameter><ac:parameter ac:name=\"reverse\">true</ac:parameter><ac:parameter ac:name=\"type\">page</ac:parameter><ac:parameter ac:name=\"cql\">label in ( &quot;uchiwa&quot; , &quot;sensu&quot; ) and type = &quot;page&quot; and space = &quot;DEVOPS&quot;</ac:parameter><ac:parameter ac:name=\"labels\">sensu uchiwa</ac:parameter></ac:structured-macro></p><p><br /></p><ac:structured-macro ac:name=\"details\" ac:schema-version=\"1\" ac:macro-id=\"50174df1-586a-462d-8976-7928f1cbfdbb\"><ac:parameter ac:name=\"hidden\">true</ac:parameter><ac:rich-text-body><p class=\"auto-cursor-target\"><br /></p><table class=\"wrapped\"><tbody><tr><th>Related issues</th><td><br /></td></tr></tbody></table><p class=\"auto-cursor-target\"><br /></p></ac:rich-text-body></ac:structured-macro><p><br /></p><p class=\"auto-cursor-target\"><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 473
    },
    {
      "id": "43220993",
      "title": "TroubleShooting of RDS/REDIS/Lambda/ES",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/43220993",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/43220993/TroubleShooting+of+RDS+REDIS+Lambda+ES",
      "created": "2019-02-18T13:17:41.582Z",
      "content": "<p class=\"auto-cursor-target\"><br /></p><hr /><h2>1.&nbsp;TroubleShooting RDS: Solutions</h2><ul><li>First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</li><li>Go to respective dashboard either (Singapore or Mumbai) according to alert.<br /><strong>Amazon RDS console</strong>, you can monitor the following items for your resources: Example below RDS&nbsp;&rarr; bac-P-Content<br /><p class=\"p1\"><strong>RDS-</strong>bac-p-content<strong>-CPUUtilization<br />RDS-</strong>bac-p-content<strong>-DatabaseConnections<br />RDS-</strong>bac-p-content<strong>-FreeableMemory<br />RDS-</strong>bac-p-content<strong>-FreeStorageSpace<br />RDS-</strong>bac-p-content<strong>-ReadIOPS</strong></p></li></ul><ol><li>The number of connections to a DB instance, number of database connections in use is required to understand the load and utilisation of resource.</li><li>The amount of storage that a DB instance is currently utilising is one of the factor we should be concerned.</li><li>The amount of memory and CPU being utilised for a DB instance. we can parallely check the rest of metrics from the drop down as mentioned in below picture and switch to different metrics.<br /><span class=\"s1\"><strong>AWS Console&nbsp;&rarr; RDS&nbsp;&rarr; Databases&nbsp;&rarr; &lt;Select_Rds&gt;&nbsp;&rarr; Monitoring&rarr; CPU Utlization<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-18 at 2.16.27 PM.png\" ri:version-at-save=\"1\" /></ac:image></strong></span><br /><br /></li><li><p class=\"auto-cursor-target\">The amount of network traffic to and from a DB instance, Sql, Users, Hosts can be monitored from <strong>RDS&rarr; Dashboard &rarr; Performance Insights&rarr; choose RDS<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-18 at 2.14.46 PM.png\" ri:version-at-save=\"1\" /></ac:image></strong><br /><br /></p></li><li class=\"li1\">The amount of read and write operations to a DB instance , You can navigate to this panel by got to<br /><span class=\"s1\"><strong>AWS Console&nbsp;&rarr; RDS&nbsp;&rarr; Databases&nbsp;&rarr; &lt;Select_Rds&gt;&nbsp;&rarr; Monitoring</strong>.<br /></span><strong>WriteIOPS</strong>: The average number of disk write I/O operations per second.<span class=\"s1\"><br /></span><strong>ReadIOPS</strong>: The average number of disk read I/O operations per second.<ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-18 at 12.48.52 PM.png\" ri:version-at-save=\"1\" /></ac:image></li><li class=\"li1\"><strong>LOGS to Verify: </strong>Incase of any High WriteIOPS, we can view/download the respective logs from RDS by going to<strong><br /></strong><strong><span class=\"s1\"><strong>AWS Console&nbsp;&rarr; RDS&nbsp;&rarr; Databases&nbsp;&rarr; &lt;Select_Rds&gt; &rarr;&nbsp;</strong></span>&nbsp;Logs&amp;Events<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-18 at 12.50.52 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-18 at 12.51.17 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /></strong></li><li class=\"li1\"><p class=\"auto-cursor-target\"><br /></p><p class=\"auto-cursor-target\">Finally, we can quickly check the <strong>slow query</strong> on respective RDS to find the recent running queries. by going to<br /><strong><span class=\"s1\"><strong>AWS Console&nbsp;&rarr; RDS&nbsp;&rarr; Databases&nbsp;&rarr; &lt;Select_Rds&gt; &rarr;&nbsp;</strong></span>&nbsp;Configuration&nbsp;&rarr; Slowquery(on bottom)<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-18 at 2.26.55 PM.png\" ri:version-at-save=\"1\" /></ac:image><br />Below are the queries running on respective RDS. Which helps to find the queries fast.<br /><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-18 at 2.27.31 PM.png\" ri:version-at-save=\"1\" /></ac:image><br /></strong></p></li></ol><hr /><h2>2.TroubleShooting REDIS: Solutions<br />Use this link :&nbsp;<ac:link><ri:page ri:content-title=\"ElastiCache Metrics Data\" ri:version-at-save=\"2\" /></ac:link></h2><hr /><h2>3.TroubleShooting ElasticSearch: Solutions</h2><p><span style=\"color: rgb(68,68,68);\">Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud.</span></p><p><span style=\"color: rgb(68,68,68);\">Things to remember while working on alerts:</span></p><ol><li>First thing is to check the respective Server/Host ip address from the alert triggered, if the alert is from SENSU(Singapore&amp;Mumbai) or CLOUD WATCH.</li><li>Go to respective AWS dashboard either (Singapore or Mumbai) region, according to alert.</li><li>Things to check in ES are<br /><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\"><strong>MasterCPUUtilization</strong>:&nbsp;<span style=\"color: rgb(68,68,68);\">The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent.</span><br /><span style=\"color: rgb(68,68,68);\"><strong>MasterFreeStorageSpace</strong>:&nbsp;<span style=\"color: rgb(68,68,68);\">This metric is not relevant and can be ignored. The service does not use master nodes as data nodes.</span></span><br /><span style=\"color: rgb(68,68,68);\"><strong>MasterJVMMemoryPressure</strong>:&nbsp;<span style=\"color: rgb(68,68,68);\">The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent.</span></span><br /><span style=\"color: rgb(68,68,68);\"><strong>ReadIOPS</strong>:&nbsp;<span style=\"color: rgb(68,68,68);\">The number of input and output (I/O) operations per second for read operations on EBS volumes.</span></span><br /><span style=\"color: rgb(68,68,68);\"><strong>WriteIOPS</strong>:&nbsp;<span style=\"color: rgb(68,68,68);\">The number of input and output (I/O) operations per second for write operations on EBS volumes.</span></span><br /><span style=\"color: rgb(68,68,68);\"><strong>ReadLatency</strong>:&nbsp;<span style=\"color: rgb(68,68,68);\">The latency, in seconds, for read operations on EBS volumes.</span></span><br /><span style=\"color: rgb(68,68,68);\"><strong>SearchLatency</strong>:&nbsp;<span style=\"color: rgb(68,68,68);\">The average time, in milliseconds, that it takes a shard to complete a search operation.</span></span><br /><span style=\"color: rgb(68,68,68);\"><strong>SearchRate</strong>:&nbsp;<span style=\"color: rgb(68,68,68);\">The total number of search requests per minute for all shards on a node. A single call to the<span style=\"color: rgb(51,51,51);\">&nbsp;</span><code class=\"code\" style=\"color: rgb(51,51,51);\">_search</code><span style=\"color: rgb(51,51,51);\">&nbsp;</span><span style=\"color: rgb(51,51,51);\">API might return results from many different shards. If five of these shards are on one node, the node would report 5 for this metric, even though the client only made one request.</span><br /></span></span></span></span></li><li><p class=\"auto-cursor-target\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(51,51,51);\">To know more about the Cluster Health go to <strong>AWS console</strong>&nbsp;&rarr; <strong>ElasticSearch Service</strong>&nbsp;&rarr; <strong>Dashboard</strong>&nbsp;&rarr; &lt;select_any_ES&gt;&nbsp;&rarr; <strong>ClusterHealth<br /></strong>you can see<strong> ClusterHealth, Totalfreestorage space, etc...<br /><ac:image><ri:attachment ri:filename=\"Screenshot 2019-02-18 at 6.06.55 PM.png\" ri:version-at-save=\"1\" /></ac:image></strong></span></span></span></span></span></p></li><li>Refer below for more info on ES<p class=\"auto-cursor-target\"><br /></p><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"4836be22-27af-4282-8301-4cde86556490\"><ac:rich-text-body><p>For More Information Please&nbsp; Refer Blow Link<br /><a href=\"https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-managedomains.html#es-managedomains-cloudwatchmetrics-master-node-metrics\">https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-managedomains.html#es-managedomains-cloudwatchmetrics-master-node-metrics</a></p></ac:rich-text-body></ac:structured-macro><p class=\"auto-cursor-target\"><br /></p></li></ol><hr /><h2>4. TroubleShooting Lambda: Solutions<br />&nbsp; &nbsp; &nbsp;working on this..</h2>",
      "approach_used": "endpoint_2",
      "word_count": 747
    },
    {
      "id": "43352104",
      "title": "RDS Monitoring Metrics",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/43352104",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/43352104/RDS+Monitoring+Metrics",
      "created": "2019-02-19T12:43:53.868Z",
      "content": "<h2>Required Metrics for all RDS:</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"e89f3552-9a5b-4f89-9861-0bc73e8709b9\"><ac:rich-text-body><p>1.<span style=\"color: rgb(32,33,36);\">FreeableMemory </span></p><p><span style=\"color: rgb(32,33,36);\">2.WriteIOPS</span></p><p><span style=\"color: rgb(32,33,36);\">3.ReadIOPS </span></p><p><span style=\"color: rgb(32,33,36);\">4.DatabaseConnections </span></p><p><span style=\"color: rgb(32,33,36);\">5.CPUUtilization </span></p><p><span style=\"color: rgb(32,33,36);\">6.FreeStorageSpace</span></p><p><span style=\"color: rgb(32,33,36);\">If it is slave we have add one more metric <strong>ReplicaLag</strong></span></p></ac:rich-text-body></ac:structured-macro><h2>1.<span style=\"color: rgb(32,33,36);\">FreeableMemory:</span></h2><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">The amount of available random access memory.Relevant Statistic:Average</span></span></p><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">If RDS i</span></span>t's memory size more than 8Gb of memory then by default we need to make it as 2Gb until we have an exception for the same.&nbsp;</p><p><br /></p><h2>2.<span style=\"color: rgb(32,33,36);\">WriteIOPS:</span></h2><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">The average number of disk write I/O operations per second.&nbsp;<span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">Relevant Statistic:Maximum.</span></span></span></span></p><p>If storage of RDS is 200GB Make it WriteIOPS 600 per/second.Otherwise make it by verifying past one week history and decide the WriteIOPS.</p><h2>3.<span style=\"color: rgb(32,33,36);\">ReadIOPS:</span></h2><p>The average number of disk read I/O operations per second.&nbsp;<span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">Relevant Statistic:Maximum.</span></span></span></span></p><p>If storage of RDS is 200GB Make it WriteIOPS 600 per/second.&nbsp;Otherwise make it by verifying past one week history and decide the ReadIOPS.</p><h2>4.<span style=\"color: rgb(32,33,36);\">DatabaseConnections:</span></h2><p>The number of database connections in use.&nbsp;<span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">Relevant Statistic:Maximum.</span></span></span></span></p><h2>5.<span style=\"color: rgb(32,33,36);\">CPUUtilization:<br /></span></h2><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">The percentage of CPU utilization. Relevant Statistic:Average.</span></span></p><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\">High values for CPU or RAM consumption might be appropriate, provided that they are in keeping with your goals for your application (like throughput or concurrency) and are expected.</span></span></span></p><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\">Threshold of CPU Utilization should be &gt;85%.</span></span></span></p><h2>6.<span style=\"color: rgb(32,33,36);\">FreeStorageSpace:</span></h2><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">The amount of available storage space.&nbsp;</span></span><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">Relevant Statistic:Maximum.</span></span></span></span></p><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\"><span>&nbsp;</span>if space used is consistently at or above 85 percent of the total disk space. See if it is possible to delete data from the instance or archive data to a different system to free up space.</span></span></span></span></span></p><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(68,68,68);\">If Storage of RDS is 200GB threshold of freeStorageSpace &lt;20GB, if storage is 500GB make it as &lt;50GB.</span></span></span></span></span></p><h2>7.<span style=\"color: rgb(32,33,36);\"> ReplicaLag:</span></h2><p><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">The amount of time a Read Replica DB instance lags behind the source DB instance. Applies to MySQL, MariaDB, and PostgreSQL Read Replicas.&nbsp;<span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\"><span style=\"color: rgb(32,33,36);\"><span style=\"color: rgb(68,68,68);\">Relevant Statistic:Maximum.</span></span></span></span></span></span></p><p><br /></p><p>How to create metrics for RDS in Cloudwatch:</p><p>Goto CloudWatch--&gt;Metrics---&gt;RDS-&rarr;PerDataBaseMetrics</p><p><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-19 at 5.58.54 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-19 at 6.05.15 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p><ac:image ac:height=\"250\"><ri:attachment ri:filename=\"Screenshot 2019-02-19 at 6.07.37 PM.png\" ri:version-at-save=\"1\" /></ac:image></p><p>Search with RDS name and Select whatever metrics required.</p><p><br /></p><p><br /></p><p><br /></p><p class=\"auto-cursor-target\"><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 422
    },
    {
      "id": "44269569",
      "title": "AWS Lambda Metrics",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/44269569",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/44269569/AWS+Lambda+Metrics",
      "created": "2019-02-21T13:39:56.012Z",
      "content": "<p>AWS Lambda automatically monitors functions on your behalf, reporting metrics through Amazon CloudWatch. These metrics include total invocations, errors, duration, throttles, DLQ errors and Iterator age for stream-based invocations.</p><p><br /></p><p>Critical Metrics::</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"f1e0ca98-ec1c-46b6-914b-46ad058dfa06\"><ac:plain-text-body><![CDATA[1.Invocations\n2.Errors\n3.Duration\n4.Throttles\n5.ConcurrentExecutions\n6.DeadLetterErrors]]></ac:plain-text-body></ac:structured-macro><p><br /></p><p><strong>1.Invocations::</strong></p><p>Measures the number of times a function is invoked in response to an event or invocation API call. This replaces the deprecated RequestCount metric. This includes successful and failed invocations, but does not include throttled attempts. This equals the billed requests for the function. Note that AWS Lambda only sends these metrics to CloudWatch if they have a nonzero value.</p><p>Units: Count</p><p><br /></p><p><strong>2.Errors::</strong></p><p>Measures the number of invocations that failed due to errors in the function (response code 4XX). This replaces the deprecated ErrorCount metric. Failed invocations may trigger a retry attempt that succeeds. This includes:</p><div class=\"itemizedlist\"><ul class=\"itemizedlist\"><li class=\"listitem\"><p>Handled exceptions (for example, context.fail(error))</p></li><li class=\"listitem\"><p>Unhandled exceptions causing the code to exit</p></li><li class=\"listitem\"><p>Out of memory exceptions</p></li><li class=\"listitem\"><p>Timeouts</p></li><li class=\"listitem\"><p>Permissions errors</p></li></ul></div><p>This does not include invocations that fail due to invocation rates exceeding default concurrent limits (error code 429) or failures due to internal service errors (error code 500).</p><p>Units: Count</p><p><br /></p><p><strong>3.Duration::</strong></p><p>Measures the elapsed wall clock time from when the function code starts executing as a result of an invocation to when it stops executing. The maximum data point value possible is the function timeout configuration. The billed duration will be rounded up to the nearest 100 millisecond. Note that AWS Lambda only sends these metrics to CloudWatch if they have a nonzero value.</p><p>Units: Milliseconds</p><p><br /></p><p><strong>4.Throttles::</strong></p><p>Measures the number of Lambda function invocation attempts that were throttled due to invocation rates exceeding the customer&rsquo;s concurrent limits (error code 429). Failed invocations may trigger a retry attempt that succeeds.</p><p>Units: Count</p><p><br /></p><p><strong>5.ConcurrentExecutions::</strong></p><p>Emitted as an aggregate metric for all functions in the account, and for functions that have a custom concurrency limit specified. Not applicable for versions or aliases. Measures the sum of concurrent executions for a given function at a given point in time. Must be viewed as an average metric if aggregated across a time period.</p><p>Units: Count</p><p><br /></p><p><strong>6.DeadLetterErrors::</strong></p><p>Incremented when Lambda is unable to write the failed event payload to your configured Dead Letter Queues. This could be due to the following:</p><div class=\"itemizedlist\"><ul class=\"itemizedlist\"><li class=\"listitem\"><p>Permissions errors</p></li><li class=\"listitem\"><p>Throttles from downstream services</p></li><li class=\"listitem\"><p>Misconfigured resources</p></li><li class=\"listitem\"><p>Timeouts</p></li></ul></div><p>Units: Count</p><p><br /></p><p><strong>For more details visit&nbsp;<a href=\"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-metrics.html\">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-metrics.html</a>.</strong></p>",
      "approach_used": "endpoint_2",
      "word_count": 377
    },
    {
      "id": "265355349",
      "title": "AWS Service Management",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/265355349",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/265355349/AWS+Service+Management",
      "created": "2020-02-18T07:05:13.396Z",
      "content": "<p />",
      "approach_used": "endpoint_2",
      "word_count": 2
    },
    {
      "id": "275513462",
      "title": "AWS Security Steps",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/275513462",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/275513462/AWS+Security+Steps",
      "created": "2020-02-25T18:12:26.981Z",
      "content": "<ul><li><p>Removed all unused security group from Prod and Sandbox.</p></li><li><p>IAM clean for prod is done for the most of the users as for our record, I have collect all evidence as well .</p></li><li><p>IAM users will have the permission as per their groups.(Removed all unwanted access.)</p></li><li><p>Location: <a href=\"https://drive.google.com/drive/folders/1PFNT6kgywTmJGV-a6KJRCx1gzEYpr86A\" data-card-appearance=\"inline\">https://drive.google.com/drive/folders/1PFNT6kgywTmJGV-a6KJRCx1gzEYpr86A</a> </p></li><li><p>S3 Sync is also going for the backup in case of DR.</p></li><li><p>S3-cleanup is going to save the cost and for security purpose as well.</p></li><li><p>Created Report for Prod (<a href=\"http://172.31.20.126:8000/account-data/prod.html\">http://172.31.20.126:8000/account-data/prod.html</a>) and doing  mitigation for the same.</p></li><li><p>S3 Bucket policy update.</p></li></ul><ol><li><p>    S3://meesho-mba - Access policy modified and removed unwanted ACL -  owner Vikas Putty .</p></li><li><p>    S3://facebook-post-images - Access policy modified and made it private - owner <a href=\"mailto:shikhar@meesho.com\">Shikhar Saxena</a></p></li><li><p>    S3:// qa-s-automation - In prod. Made it private now, will remove soon - Owner Avanish Sharma </p></li><li><p>     Dropped mail to the bucker owners for changing ht ACL configuration.</p></li></ol><ul><li><p>Created the life-cycle policy on most of the buckets for multi-upload cleanup approx 120 TB data is cleaned.</p></li><li><p>IAM report is also availble <a href=\"https://docs.google.com/spreadsheets/d/1909rrMbKLe2n0chE40a1J0s0nOo07BYHqFzmFiunaGg/edit#gid=1012671892\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1909rrMbKLe2n0chE40a1J0s0nOo07BYHqFzmFiunaGg/edit#gid=1012671892</a> .</p></li><li><p>Removed all unwanted records from Route53.</p></li><li><p>49.249.242.6 &rarr; <a href=\"http://meesho.co\">meesho.co</a> (A record was there in route53 that is removed now)</p></li><li><p>54.179.175.229  --&gt; <a href=\"http://meeshoapi.com\">meeshoapi.com</a>( A record was there in route53 that is removed now)</p></li><li><p>54.179.191.154, 18.140.42.82 this is ALB IP&rsquo;s and port 444 is open that is resolved.</p></li><li><p>Changes all ALB bucket ACL permission who has the access( <strong>objects can be public</strong> ) to (<strong>Bucket and Object not public</strong>) except the public buckets.</p></li><li><p>NOTE:-- <a href=\"https://s3.console.aws.amazon.com/s3/#\">platform-competitive-scrap</a> bucket objects are public, which is used in the app. We have to  make the bucket public with get and put access. </p></li></ul><p /><p /><p />",
      "approach_used": "endpoint_2",
      "word_count": 262
    },
    {
      "id": "275644485",
      "title": "AWS RDS",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/275644485",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/275644485/AWS+RDS",
      "created": "2020-02-24T13:38:03.521Z",
      "content": "<p><strong>Service Owner:</strong> Pankaj Pandey</p><p><strong>Monitoring</strong></p><p>Below are the list of RDS metrics that we are monitoring currently:</p><ol><li><p><strong>CPU Utilisation</strong>: The percentage of CPU utilisation (Ignoring single spike).</p></li><li><p><strong>DatabaseConnections</strong>: The number of database connections in use.</p></li><li><p><strong>FreeStorageSpace</strong>: The amount of available storage space (Alert is free space less than 50%).</p></li><li><p><strong>ReadIOPS+WriteIOPS</strong>: The average number of disk write + read I/O operations per second.</p></li><li><p><strong>ReplicaLag</strong>: The amount of time a Read Replica DB instance lags behind the source DB instance.</p></li><li><p><strong>IOPS</strong> : Read and Write IOPS high for more than allowed duration (Due to cron it spikes and expected to complete in given duration ).  </p></li></ol><p><strong>Monitoring Url</strong>: <a href=\"http://prometheus.meeshoint.in/alerts\">http://prometheus.meeshoint.in/alerts</a></p><p /><p><strong>Recent changes in RDS infra:</strong></p><ol><li><p>added more storage to get more IOPS</p></li><li><p>added provisioned IOPS for 'supply-tech-slave'</p></li><li><p>Upgrade and Downgrade instance as per usage</p></li><li><p>add replication lag alert for all slaves</p></li><li><p>tuned alerting for IOPS, replica lag for all needed RDS.</p></li><li><p>Removed individual metric for read and write IOPS, instead added combine read+write IOPS</p></li></ol><p /><p><strong>Reservation Details: </strong></p><table data-layout=\"default\"><tbody><tr><th><p><strong>Type</strong></p></th><th><p><strong>Count</strong></p></th><th><p><strong>Reserved</strong></p></th></tr><tr><td><p>t3.micro</p></td><td><p>1</p></td><td><p>0</p></td></tr><tr><td><p>t3.small</p></td><td><p>1</p></td><td><p>0</p></td></tr><tr><td><p>t3.medium</p></td><td><p>5</p></td><td><p>3</p></td></tr><tr><td><p>m5.large</p></td><td><p>19</p></td><td><p>19</p></td></tr><tr><td><p>m5.xlarge</p></td><td><p>9</p></td><td><p>9</p></td></tr><tr><td><p>m5.2xlarge</p></td><td><p>3</p></td><td><p>4</p></td></tr><tr><td><p>m5.4xlarge</p></td><td><p>2</p></td><td><p>2</p></td></tr><tr><td><p>m5.12xlarge</p></td><td><p>2</p></td><td><p>2</p></td></tr></tbody></table><p /><p><strong>Reservation Ur</strong>l:</p><p><a href=\"https://ap-southeast-1.console.aws.amazon.com/rds/home?region=ap-southeast-1#reserved-instance-list:\">https://ap-southeast-1.console.aws.amazon.com/rds/home?region=ap-southeast-1#reserved-instance-list:</a></p><p /><p />",
      "approach_used": "endpoint_2",
      "word_count": 156
    },
    {
      "id": "282820609",
      "title": "AWS EC2",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/282820609",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/282820609/AWS+EC2",
      "created": "2020-02-24T13:35:18.322Z",
      "content": "<h3>Alert tuning:</h3><ul><li><p>Changed  Disk space alert time from 60s to 2m</p></li><li><p>Modified high CPU utilisation check time from 60s to 2min  <br />CPU-High-80% from 60s to 2m</p></li><li><p>Modified CPU threshold from 60 to 65% <br />CPU-High-65% added with time 2min</p></li><li><p>Modified OutOfMemory alert from 60s to 2m</p></li><li><p>Added FileDescriptor alert for monitoring too many open files.</p></li><li><p>Monitoring newly launched  EC2 instance in prod and making sure to terminate if unused.</p></li></ul><p /><p>In progress:</p><h3>Backup:</h3><ul><li><p>Taking backups of non-newrelic instances.</p></li></ul><p>Reserved instance calculation.</p><h3>EMR monitoring:</h3><ul><li><p>StatusCheckFailed</p></li><li><p>LiveDataNodes</p></li><li><p>CoreNodesRunning</p></li><li><p>MRUnhealthyNodes</p></li><li><p>HDFSUtilization</p></li><li><p>AppsPending</p></li></ul><p /><p />",
      "approach_used": "endpoint_2",
      "word_count": 75
    },
    {
      "id": "285016078",
      "title": "AWS Elastic Search",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/285016078",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/285016078/AWS+Elastic+Search",
      "created": "2020-02-25T06:59:29.253Z",
      "content": "<p><strong>Service Owner:</strong> Pankaj Pandey</p><p><strong>Monitoring</strong></p><p>Below are the list of <strong>Elastic Search</strong> metrics that we are monitoring currently:</p><ol><li><p><strong>CPU Utilisation</strong>: The percentage of CPU utilisation (Ignoring single spike).</p></li><li><p><strong>FreeStorage</strong>: free space left in es nodes.</p></li><li><p><strong>JVMMemoryPressure</strong>: The maximum percentage of the Java heap used for all data nodes in the cluster. The cluster could encounter out of memory errors if usage increases. Amazon ES uses half of an instance's RAM for the Java heap, up to a heap size of 32 GiB.</p></li><li><p><strong>KibanaHealthyNodes</strong>: A health check for Kibana. A value of 1 indicates normal behaviour . A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster.</p></li><li><p><strong>Nodes</strong>: The number of nodes in the Amazon ES cluster, including dedicated master nodes.</p></li><li><p><strong>ClusterIndexWritesBlocked</strong>: Your cluster is blocking write requests.</p></li><li><p><strong>ClusterStatus.red</strong>: At least one primary shard and its replicas are not allocated to a node.</p></li><li><p><strong>4xx</strong>: The number of requests to the domain that resulted in the given HTTP response code 4xx.</p></li><li><p><strong>5xx</strong>: The number of requests to the domain that resulted in the given HTTP response code 5xx.</p></li></ol><p /><p><strong>Monitoring Url</strong>: <a href=\"http://prometheus.meeshoint.in/alerts\">http://prometheus.meeshoint.in/alerts</a></p><p>&nbsp;</p><p><strong>Recent changes in Elastic Search  infra:</strong></p><ol><li><p>Validated all the monitoring.</p></li><li><p>increased threshold for free storage from 20G to 100G for all ES.</p></li><li><p>added number of nodes in cluster for all ES</p></li><li><p>reduced CPU threshold for all ES from 85 to 80 except scraper ES.</p></li></ol><p /><p>&nbsp;</p><p><strong>Reservation Details:</strong></p><table data-layout=\"default\"><tbody><tr><th><p><strong>Type</strong></p></th><th><p><strong>Count</strong></p></th><th><p><strong>Reserved</strong></p></th></tr><tr><td><p>c5.large</p></td><td><p>9</p></td><td><p>9</p></td></tr><tr><td><p>c5.2xlarge</p></td><td><p>3</p></td><td><p>4</p></td></tr><tr><td><p>c5.9xlarge</p></td><td><p>4</p></td><td><p>4</p></td></tr><tr><td><p>m5.large</p></td><td><p>3</p></td><td><p>3</p></td></tr><tr><td><p>m5.xlarge</p></td><td><p>5</p></td><td><p>5</p></td></tr><tr><td><p>m5.2xlarge</p></td><td><p>3</p></td><td><p>3</p></td></tr><tr><td><p>r5.xlarge</p></td><td><p>4</p></td><td><p>4</p></td></tr></tbody></table><p>&nbsp;</p><p><strong>Reservation Ur</strong>l:</p><p><a href=\"https://ap-southeast-1.console.aws.amazon.com/es/home?region=ap-southeast-1#reserved-instances\">https://ap-southeast-1.console.aws.amazon.com/es/home?region=ap-southeast-1#reserved-instances</a></p>",
      "approach_used": "endpoint_2",
      "word_count": 225
    },
    {
      "id": "285048835",
      "title": "AWS Redis",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/285048835",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/285048835/AWS+Redis",
      "created": "2020-02-25T05:47:01.168Z",
      "content": "<p><strong>Service Owner:</strong> Pankaj Pandey</p><p><strong>Monitoring</strong></p><p>Below are the list of Redis metrics that we are monitoring currently:</p><ol><li><p><strong>EngineCPUUtilization</strong>: Provides CPU utilisation of the Redis engine thread. Since Redis is single-threaded, you can use this metric to analyse the load of the Redis process itself. The EngineCPUUtilization metric provides a more precise visibility of the Redis process and can be used in conjunction with CPUUtilization metric, which exposes CPU utilisation for the server instance as a whole, including other operating system and management processes.</p></li><li><p><strong>FreeableMemory: </strong>The amount of free memory available on the host. This is derived from the RAM, buffers and cache that the OS reports as freeable.</p></li><li><p><strong>CurrConnections</strong>: The number of client connections, excluding connections from read replicas. ElastiCache uses two to four of the connections to monitor the cluster in each case.</p></li><li><p><strong>NetworkBytesIn+NetworkBytesOut</strong>: The number of bytes the host has read from the network.The number of bytes sent out on all network interfaces by the instance.</p></li><li><p><strong>SwapUsage</strong>: The amount of swap used on the host.</p></li><li><p><strong>ReplicationLag</strong>: This metric is only applicable for a node running as a read replica. It represents how far behind, in seconds, the replica is in applying changes from the primary node.</p></li></ol><p><strong>Monitoring Url</strong>: <a href=\"http://prometheus.meeshoint.in/alerts\">http://prometheus.meeshoint.in/alerts</a></p><p>&nbsp;</p><p><strong>Recent changes in Redis infra:</strong></p><ol><li><p>Upgrade and Downgrade instance as per usage</p></li><li><p>reserved more redis and cancel 2 r5.2xlarge.</p></li></ol><p>&nbsp;</p><p><strong>Reservation Details:</strong></p><table data-layout=\"default\"><tbody><tr><th><p><strong>Type</strong></p></th><th><p><strong>Count</strong></p></th><th><p><strong>Reserved</strong></p></th></tr><tr><td><p>r5.large</p></td><td><p>22</p></td><td><p>16</p></td></tr><tr><td><p>m5.large</p></td><td><p>11</p></td><td><p>12</p></td></tr><tr><td><p>r5.2xlarge</p></td><td><p>2</p></td><td><p>2</p></td></tr><tr><td><p>r5.4xlarge</p></td><td><p>2</p></td><td><p>2</p></td></tr></tbody></table><p>&nbsp;</p><p><strong>Reservation Ur</strong>l:</p><p><a href=\"https://ap-southeast-1.console.aws.amazon.com/elasticache/home?region=ap-southeast-1#reserved-cache-nodes:\">https://ap-southeast-1.console.aws.amazon.com/elasticache/home?region=ap-southeast-1#reserved-cache-nodes:</a></p>",
      "approach_used": "endpoint_2",
      "word_count": 213
    },
    {
      "id": "328269850",
      "title": "Devops On Call Schedule",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/328269850",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/328269850/Devops+On+Call+Schedule",
      "created": "2020-07-17T16:44:16.524Z",
      "content": "<table data-layout=\"default\"><tbody><tr><td><p><strong>Name</strong></p></td><td><p><strong>Mobile Number</strong></p></td><td><p><strong>Month</strong></p></td><td><p><strong>Week</strong></p></td></tr><tr><td><p><span style=\"color: rgb(101,84,192);\">Pankaj</span></p></td><td><p><span style=\"color: rgb(101,84,192);\">9972898948</span></p></td><td><p><span style=\"color: rgb(101,84,192);\">July-2020</span></p></td><td><p><span style=\"color: rgb(101,84,192);\">3rd Week</span></p></td></tr><tr><td><p>Abhijeet</p></td><td><p>8884261740</p></td><td><p><span style=\"color: rgb(101,84,192);\">July-2020</span></p></td><td><p>4th Week</p></td></tr><tr><td><p>Rahul Arya</p></td><td><p>8218902503</p></td><td><p><span style=\"color: rgb(101,84,192);\">July-2020</span></p></td><td><p>5th Week</p></td></tr><tr><td><p>Arun</p></td><td data-highlight-colour=\"#f8f9fa\"><p>9645805227</p></td><td><p><span style=\"color: rgb(101,84,192);\">July-2020</span></p></td><td><p>2nd Week</p></td></tr><tr><td><p>Abhiroop Soni</p></td><td><p>9999008747</p></td><td><p><span style=\"color: rgb(101,84,192);\">July-2020</span></p></td><td><p>1st Week</p></td></tr></tbody></table><p /><p><strong>Note: </strong>Need to Fill by Every Individual by Friday EOD whoever is On call without fail</p><p />",
      "approach_used": "endpoint_2",
      "word_count": 44
    },
    {
      "id": "390397953",
      "title": "Superset Installation",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/390397953",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/390397953/Superset+Installation",
      "created": "2020-04-02T10:00:12.363Z",
      "content": "<p>Create user superset:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"3f3c003f-0a0b-4a02-84de-a9ad7df8cfaf\"><ac:plain-text-body><![CDATA[useradd -m -d /home/superset -s /bin/bash superset]]></ac:plain-text-body></ac:structured-macro><p>Edit sudoers:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"f691bb01-025a-4512-a871-ae6621d4ce73\"><ac:plain-text-body><![CDATA[vi /etc/sudoers]]></ac:plain-text-body></ac:structured-macro><p>Append the line:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"4643110c-c933-4fd3-80e8-cf184aade1af\"><ac:plain-text-body><![CDATA[superset ALL=(ALL) NOPASSWD:ALL]]></ac:plain-text-body></ac:structured-macro><p>Switch user to superset:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"494e6c94-2f22-4c9c-8471-8f5f49c15ebc\"><ac:plain-text-body><![CDATA[su - superset]]></ac:plain-text-body></ac:structured-macro><p>Now install the deps</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"586d00d5-cab6-45ac-a4a0-eeb1b3cf1189\"><ac:plain-text-body><![CDATA[sudo apt-get update\nsudo apt-get install build-essential libssl-dev libffi-dev python3.6-dev python-pip libsasl2-dev libldap2-dev]]></ac:plain-text-body></ac:structured-macro><p>Install and activate virtual env</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"34f32924-b895-4cbe-86b8-18f00911e8ad\"><ac:plain-text-body><![CDATA[pip install virtualenv\nsudo apt-get install python3-venv\npython3 -m venv venv\n. venv/bin/activate]]></ac:plain-text-body></ac:structured-macro><p>Install python modules</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"a780590f-4b0b-4308-bb92-63c9cb1517af\"><ac:plain-text-body><![CDATA[pip install --upgrade setuptools pip\nsudo apt-get install libmysqlclient-dev\npip install -r requirement.txt]]></ac:plain-text-body></ac:structured-macro><p><code>requirement.txt</code></p><p>Modules required as per Meesho&rsquo;s requirement .</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c80c7870-af36-465e-9ecc-9a559b755418\"><ac:plain-text-body><![CDATA[configparser==4.0.2\nfuture==0.18.2\nidna==2.8\nmysqlclient==1.4.6\npsycopg2-binary==2.8.4\nPyHive==0.6.1\nPyMySQL==0.9.3\npythrifthiveapi==0.1.1.0\nsasl==0.2.1\nsqlalchemy-redshift==0.7.5\nsuperset==0.30.0\nthrift==0.13.0\nthrift-sasl==0.3.0]]></ac:plain-text-body></ac:structured-macro><p>Update the config to change the superset db from sqlite to mysql(RDS):</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"ffbd9efb-09ec-40af-83b1-79d55e13d18e\"><ac:plain-text-body><![CDATA[vi /home/ubuntu/venv/lib/python3.6/site-packages/superset/config.py]]></ac:plain-text-body></ac:structured-macro><p>Comment the SQLALCHEMY_DATABASE_URI pointing to sqlite and add mysql url like this:</p><p>Example:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"2b0700dc-1d83-4581-a2bc-6e74253bca5b\"><ac:plain-text-body><![CDATA[SQLALCHEMY_DATABASE_URI = 'mysql://superset:###PASSWORD##@data-platform-s-superset.c0ut7uytrccv.ap-southeast-1.rds.amazonaws.com/superset']]></ac:plain-text-body></ac:structured-macro><p>Now rub the commands below</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"75e7d59f-6eb8-4b2f-8057-16b82e5d7f5c\"><ac:plain-text-body><![CDATA[superset db upgrade\nexport FLASK_APP=superset\nflask fab create-admin\nsuperset init]]></ac:plain-text-body></ac:structured-macro><p>Create systemd for superset:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"83af860b-b9ae-46d3-8f49-f6be216fca98\"><ac:plain-text-body><![CDATA[su - root\nvi /etc/systemd/system/superset.service]]></ac:plain-text-body></ac:structured-macro><p>Add below content in the &quot;superset.service&quot;</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"ffa9dba5-e0e0-4e0d-ac2d-e0455a830b1e\"><ac:plain-text-body><![CDATA[[Unit]\nDescription=\"Apache Superset\"\nAfter=network.target\n\n[Service]\nUser=superset\nGroup=superset\nEnvironment=PYTHONPATH=$PYTHONPATH:/venv\nExecStart=/home/superset/venv/bin/superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger\nRestart=on-failure\nRestartSec=5s\n\n[Install]\nWantedBy=multi-user.target]]></ac:plain-text-body></ac:structured-macro><p>Switch back to superset user make sure venv is active and run the command</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"fd49aa9f-eff3-4bb1-9c6a-9d71f9f25c86\"><ac:plain-text-body><![CDATA[sudo systemctl restart superset.service\nsystemctl status superset.service]]></ac:plain-text-body></ac:structured-macro><p />",
      "approach_used": "endpoint_2",
      "word_count": 215
    },
    {
      "id": "390430733",
      "title": "Automating Perf-Infra provisioning :",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/390430733",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/390430733/Automating+Perf-Infra+provisioning",
      "created": "2020-04-02T11:17:21.304Z",
      "content": "<p>Launch <a href=\"http://ci.meesho.com/\">http://ci.meesho.com/</a></p><p>Trigger the job of the respective service to launch the perf infra.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c86d5b67-7cf8-4a1d-bb74-73149d4fc9f6\"><ac:plain-text-body><![CDATA[- Open the job\n\n- Click on build with parameters.\n\n- You will get three options here:\n\t- Plan: Select plan and click on build will give you the terraform plan output, The services that will created when you deploy the job.\n\n\t- Deploy: You need to select this option to launch the real infra on AWS, the output from plan will be applied here.\n\n\t- Destroy: Once the infra is created by selecting the \"Deploy\" option and the testing is completed by the dev team, you can select this option to destory the Perf infra.]]></ac:plain-text-body></ac:structured-macro><p>The coordinator jobs:</p><p>This job can launch the entire storefront perf Infra, also you have options to launch the custom services within the job with it's dependency.</p><p>NB:<br /><a href=\"https://github.com/Meesho/devops-infra-tf/tree/master/sandbox/perf\" data-card-appearance=\"inline\">https://github.com/Meesho/devops-infra-tf/tree/master/sandbox/perf</a> <br />This is the terraform repo to understand more about the jobs.</p><p>Clone the repo &ldquo;devops-infra-tf&rdquo; and find the tf manifests.</p><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"267\" ac:original-width=\"244\" ac:width=\"340\"><ri:attachment ri:filename=\"Screenshot 2020-04-02 at 4.03.04 PM.png\" ri:version-at-save=\"1\" /></ac:image><p><a href=\"http://main.tf\">main.tf</a></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"bc0310d0-9aec-40fa-9c90-6eda3bfa887b\"><ac:plain-text-body><![CDATA[terraform {\n  backend \"s3\" {\n    bucket = \"devops-sandbox-tf-states\"\n    key    = \"cis/terraform.tfstate\"\n    region = \"ap-southeast-1\"\n  }\n}\n\nprovider \"aws\" {\n  region     = \"ap-southeast-1\"\n}\n\n\nmodule \"ec2\" {\n  source = \"../../../modules/ec2\"\n  env = \"sandbox\"\n  owners = \"898247906677\"\n  service = var.service\n  ami_name = \"bac-p-cis*\"\n  instance_type = \"c5.xlarge\"\n  suffix = \"perf\"\n  extra_tags = []\n  get_ami_from_prod_account = \"true\"\n  sg_name = \"sg-0cd7c096a4aa669b8\"\n  n_instance = 1\n  key_name = \"manifest_dev_deploy\"\n}\n\nmodule \"ec2_ip\" {\n  source = \"../../../modules/ec2_ip\"\n  suffix = \"perf\"\n  service = var.service\n  module_depends_on = module.ec2.asg_arn\n  env = \"sandbox\"\n}\n\nmodule \"route53\" {\n  source = \"../../../modules/route53\"\n  service = var.service\n  type = \"A\"\n  endpoint = [module.ec2_ip.private_ip]\n}]]></ac:plain-text-body></ac:structured-macro><h3>Saving the state of infra:</h3><p>Terraform must store state about your managed infrastructure and configuration. This state is used by Terraform to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures.</p><p>This state is stored by default in a local file named &quot;terraform.tfstate&quot;, but it can also be stored remotely, which works better in a team environment. Here we are saving the state in S3 bucket which is defined in the snippet below.</p><p>S3 Backend:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"61d0808d-6796-41a0-8d97-88de75abed7b\"><ac:plain-text-body><![CDATA[terraform {\n  backend \"s3\" {\n    bucket = \"devops-sandbox-tf-states\"\n    key    = \"cis/terraform.tfstate\"\n    region = \"ap-southeast-1\"\n  }\n}]]></ac:plain-text-body></ac:structured-macro><h3>Provider</h3><p>A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (&nbsp;AWS, GCP, Microsoft Azure etc..) in our case AWS.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"6cfaa147-baf6-4625-bd59-9c9c34ebc5a8\"><ac:plain-text-body><![CDATA[provider \"aws\" {\n  region     = \"ap-southeast-1\"\n}]]></ac:plain-text-body></ac:structured-macro><h3>Terraform module for perf:</h3><p>To define a module, create a new directory for it and place one or more&nbsp;<code>.tf</code>&nbsp;files inside just as you would do for a root module. Terraform can load modules either from local relative paths or from remote repositories, Modules can also call other modules using a&nbsp;<code>module</code>&nbsp;block like in the snippet below.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"58cccf09-4093-492b-b987-9021ad486bba\"><ac:plain-text-body><![CDATA[module \"ec2\" {\n  source = \"../../../modules/ec2\"\n  env = \"sandbox\"\n  owners = \"898247906677\"\n  service = var.service\n  ami_name = \"bac-p-cis*\"\n  instance_type = \"c5.xlarge\"\n  suffix = \"perf\"\n  extra_tags = []\n  get_ami_from_prod_account = \"true\"\n  sg_name = \"sg-0cd7c096a4aa669b8\"\n  n_instance = 1\n  key_name = \"manifest_dev_deploy\"\n}]]></ac:plain-text-body></ac:structured-macro><p>Passing the variable in <code>module</code>&nbsp;block will replace it with the default variable inside the module.</p><h3>Understanding EC2 module:</h3><p>My approach is to use autoscaling group for creating instances instead of creating individual  EC2&rsquo;s with terraform &ldquo;aws_instance&rdquo; resource by this approach it is easy to create and manage number of instance in a single run. </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"532c959b-bfc9-4dd6-a6ef-77c48f61b90c\"><ac:plain-text-body><![CDATA[variable \"module_depends_on\" {\n  default = [\"\"]\n}\n\nresource \"null_resource\" \"module_depends_on\" {\n  triggers = {\n    value = length(var.module_depends_on)\n  }\n}\n\nprovider \"aws\" {\n  alias   = \"prod_account\"\n  region                  = \"ap-southeast-1\"\n  shared_credentials_file = \"~/.aws/credentials\"\n  profile                 = \"prod\"\n}\n\n\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  provider = aws.prod_account\n\n  filter {\n    name   = \"name\"\n    values = [var.ami_name] \n  }\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n\n  owners = [ var.get_ami_from_prod_account == \"true\" ? var.prod_owners : var.owners ]\n}\nresource \"aws_ami_launch_permission\" \"ami_shared\" {\n  count = var.get_ami_from_prod_account == \"true\" ? 1 : 0\n  provider = aws.prod_account\n  image_id = data.aws_ami.ubuntu.id\n  account_id = var.owners\n\n}\ndata \"aws_vpc\" \"selected\" {\n  tags = {\n    env  = var.env\n  }  \n}\n\ndata \"aws_subnet_ids\" \"selected\" {\n  vpc_id = data.aws_vpc.selected.id\n}\n\nresource \"aws_launch_configuration\" \"lc\" {\n  name          = \"lc-${var.service}-${var.suffix}\"\n  image_id      = data.aws_ami.ubuntu.id\n  instance_type = var.instance_type\n  security_groups = split(\",\", var.sg_name)\n  key_name = var.key_name\n  user_data = var.user_data\n  depends_on = [aws_ami_launch_permission.ami_shared]\n  lifecycle {\n    ignore_changes = [image_id]\n    create_before_destroy = true\n  }\n}\n\nresource \"aws_autoscaling_group\" \"asg\" {\n  count                = 1\n  depends_on = [\n    null_resource.module_depends_on\n  ]\n  name                 = \"asg-${var.service}-${var.suffix}\"\n  launch_configuration = aws_launch_configuration.lc.name\n  vpc_zone_identifier  = [var.subnet_id == \"\" ? var.env == \"prod\" ? element(tolist(data.aws_subnet_ids.selected.ids), 3) : element(tolist(data.aws_subnet_ids.selected.ids), 0) : var.subnet_id]\n\n  min_size              = var.n_instance\n  max_size              = var.n_instance\n  protect_from_scale_in = false\n  force_delete          = true\n\n\n  tags = flatten([\"${concat(\n    list(\n      map(\"key\", \"Name\", \"value\", \"${var.pod}-${var.env}-${var.service}-${var.suffix}\", \"propagate_at_launch\", true),\n      map(\"key\", \"env\", \"value\", var.env, \"propagate_at_launch\", true),\n      map(\"key\", \"pod\", \"value\", var.pod, \"propagate_at_launch\", true),\n      map(\"key\", \"service\", \"value\", var.service, \"propagate_at_launch\", true)\n\n    ),\n    var.extra_tags)\n  }\"])\n\n  suspended_processes = [\n    \"ReplaceUnhealthy\",\n  ]\n}\n\nresource \"aws_autoscaling_attachment\" \"asg_attachment\" {\n  count                  = var.suffix == \"perf\" ? 0 : 1\n  autoscaling_group_name = aws_autoscaling_group.asg[count.index].id\n  alb_target_group_arn   = var.tg_arn\n}\n\noutput \"sg_names\" {\n  value = split(\",\", var.sg_name)\n}\n\noutput \"subnet_ids\" {\n  value = [var.subnet_id == \"\" ? var.env == \"prod\" ? element(tolist(data.aws_subnet_ids.selected.ids), 3) : element(tolist(data.aws_subnet_ids.selected.ids), 0) : var.subnet_id]\n}\n\noutput \"asg_arn\" {\n  description = \"ARN of the AutoScaling Group\"\n  value       = join(\"\", aws_autoscaling_group.asg.*.arn)\n}]]></ac:plain-text-body></ac:structured-macro><h5>First get the AMI id using filters:</h5><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"7e764a63-8ac1-4088-bfd4-a9d93e65fccf\"><ac:plain-text-body><![CDATA[data \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  provider = aws.prod_account\n\n  filter {\n    name   = \"name\"\n    values = [var.ami_name] \n  }\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n]]></ac:plain-text-body></ac:structured-macro><p>Here we are getting the AMI from the prod account then share it with dev account. This can be achieved using the provider in terraform.</p><p>Connecting with the prod account using AWS prod credentials.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"343dc7df-c44b-4fcd-ae95-e5a38f60994c\"><ac:plain-text-body><![CDATA[provider \"aws\" {\n  alias   = \"prod_account\"\n  region                  = \"ap-southeast-1\"\n  shared_credentials_file = \"~/.aws/credentials\"\n  profile                 = \"prod\"\n}]]></ac:plain-text-body></ac:structured-macro><p><code>provider = aws.prod_account</code> block in <code>data &quot;aws_ami&quot; &quot;ubuntu&quot; </code> connects it with prod account while working from the dev environment.</p><h5>Share the AMI with dev account:</h5><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"f40e815b-12cf-4bd1-b8c0-9f2a12a6c6b8\"><ac:plain-text-body><![CDATA[resource \"aws_ami_launch_permission\" \"ami_shared\" {\n...\n...\n}]]></ac:plain-text-body></ac:structured-macro><h5>Get VPC and subnet Id&rsquo;s</h5><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d5cb0e8b-0c53-488e-ac54-957577985a66\"><ac:plain-text-body><![CDATA[data \"aws_vpc\" \"selected\" {\n  tags = {\n    env  = var.env\n  }  \n}\n\ndata \"aws_subnet_ids\" \"selected\" {\n  vpc_id = data.aws_vpc.selected.id\n}]]></ac:plain-text-body></ac:structured-macro><h5>Launch configuration and autoscaling group:</h5><p>Create launch configuration and attach it with the autoscaling group.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"20c97ce8-601d-48e4-b695-aff5ef467e54\"><ac:plain-text-body><![CDATA[resource \"aws_autoscaling_group\" \"asg\" {\n  count                = 1\n  depends_on = [\n    null_resource.module_depends_on\n  ]\n  ...\n  ...\n}]]></ac:plain-text-body></ac:structured-macro><p>Terraform by default doesn&rsquo;t support the module dependency, hence did a hack with <code>null_resource</code> to achieve this, as in the snippet below.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"297205ec-270b-431c-bedd-7f714982384f\"><ac:plain-text-body><![CDATA[depends_on = [\n    null_resource.module_depends_on\n  ]]]></ac:plain-text-body></ac:structured-macro><p />",
      "approach_used": "endpoint_2",
      "word_count": 1060
    },
    {
      "id": "414285825",
      "title": "Launching a Cluster on Amazon EKS Managed Kubernetes Service",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/414285825",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/414285825/Launching+a+Cluster+on+Amazon+EKS+Managed+Kubernetes+Service",
      "created": "2020-04-10T08:04:11.482Z",
      "content": "<h3><strong>Creating a Cluster</strong></h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"5ee462c8-0505-4cb8-a02d-2c623b36689c\"><ac:plain-text-body><![CDATA[ a. $ eksctl create cluster \\ \n --name test-cluster-01 \\ \n --region ap-southeast-1 \\ \n --version 1.15 \\\n  --tags\n  \"stack_name=test-cluster-01\n  --nodegroup-name ng-1-workers \\\n  --nodes 6 \\\n  --ssh-access \\\n  --ssh-public-key ~/devops/sandbox.pem \\\n  --node-private-networking \\\n  --node-security-groups sg-XXXXX \\\n  --vpc-private-subnets\n  \"subnet-xxxx,subnet-xxxx,subnet-xxxx\" \\\n  --profile prod \\\n  --auto-kubeconfig\n]]></ac:plain-text-body></ac:structured-macro><p>Verify using AWS Console that the following are created:</p><p>a. Two CloudFormation stacks (see <a href=\"https://ap-southeast-1.console.aws.amazon.com/cloudformation/home?region=ap-southeast-1#/stac\">https://ap-southeast-1.console.aws.amazon.com/cloudformation/home?region=ap-southeast-1#/stac</a>)<br />ii. Stack eksctl-test-cluster-01-cluster manages an Amazon EKS cluster's control plane</p><p>iii. Stack eksctl-test-cluster-01-nodegroup-ng-1-workers manages Amazon EKS cluster's worker nodes in an Auto Scaling Group (ASG)</p><p>b. One Amazon EKS cluster (see <a href=\"https://eu-central-1.console.aws.amazon.com/eks/home?region=eu-central-1#/clusters\">https://ap-southeast-1.console.aws.amazon.com/eks/home?region=ap-southeast-1#/clusters</a> )</p><p>kubectl access to newly launched cluster</p><p>eksctl --auto-kubeconfig option writes kubeconfig file after a successful cluster launch. In this case the file was written to ~home/ .kube/eksctl/clusters/test-cluster-01</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"635329e2-0149-4545-8529-2ee3c71e9f22\"><ac:plain-text-body><![CDATA[$ export\nKUBECONFIG=/Users/jithin.raj/.kube/eksctl/clusters/test-cluste\nr-01\n$ kubectl cluster-info\nKubernetes master is running at\nhttps://07755F9ECD2787xxxxxxC.sk1.ap-southeast-1.eks.a\nmazonaws.com\nCoreDNS is running at\nhttps://07755F9ECD27876763xxxxx1E688C.sk1.ap-southeast-1.eks.a\nmazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dn\ns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl\ncluster-info dump'.\n]]></ac:plain-text-body></ac:structured-macro><p /><h3>Post Launch setup , installing Kubernetes Dashboard etc.</h3><p>Now that you have kubectl access to the cluster you can use it as with any other k8s cluster b. Install Kubernetes Dashboard UI</p><p>Create file eks-admin-service-account.yaml with contents</p><p><strong>eks-admin-service-account.yaml</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d38faf57-894b-4754-928f-a41dbea10406\"><ac:plain-text-body><![CDATA[apiVersion: v1\nkind: ServiceAccount\nmetadata:\n\n  name: eks-admin\n\n  namespace: kube-system\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n\n  name: eks-admin\nroleRef:\n\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\n\nsubjects:\n- kind: ServiceAccount\n\n  name: eks-admin\n  namespace: kube-system\n]]></ac:plain-text-body></ac:structured-macro><h4>Deploy Dashboard</h4><p><br />$ kubectl apply -f <a href=\"https://raw.githubusercontent.com/kubernetes/dashboard/v1\">https://raw.githubusercontent.com/kubernetes/dashboard/v1</a>. 10.1/src/deploy/recommended/kubernetes-dashboard.yaml</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d615d24a-7463-4489-9023-6b1ebcfe13f5\"><ac:plain-text-body><![CDATA[  # Create an eks-admin Service Account and Cluster Role\n  Binding\n  $ kubectl apply -f eks-admin-service-account.yaml\n\n  # Print token to connect to Dashboard\n  $ kubectl -n kube-system describe secret $(kubectl -n\n  kube-system get secret | grep eks-admin | awk '{print\n  $1}')\n\n  # Echo URL to connect to\n  $ echo \"Start kubectl proxy and then navigate to following\n  URL to login to Dashboard...\"\n  $ echo\n  \"http://localhost:8001/api/v1/namespaces/kube-system/servi\n  ces/https:kubernetes-dashboard:/proxy/#!/login\"\n]]></ac:plain-text-body></ac:structured-macro><p><strong>Destroying a Cluster</strong></p><ol><li><p>If you need to completely destroy an Amazon EKS cluster the easiest way is to delete the two CloudFormation stacks created above in the following order:</p></li><li><p>Delete Stack eksctl-test-cluster-01-nodegroup-ng-1-workers which manages Amazon EKS cluster's worker nodes in an Auto Scaling Group (ASG)</p></li><li><p>Delete Stack eksctl-test-cluster-01-cluster which manages an Amazon EKS cluster's control plane</p></li><li><p>You can delete a CloudFormation Stack via AWS Console, by selecting the stack and choosing Action = Delete 3. Please be patient as it can take 10 minutes or more per Stack for deletion</p></li></ol><p />",
      "approach_used": "endpoint_2",
      "word_count": 386
    },
    {
      "id": "414285996",
      "title": "CI/CD",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/414285996",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/414285996/CI+CD",
      "created": "2022-02-05T08:56:35.065Z",
      "content": "<p><ac:structured-macro ac:name=\"pagetree\" ac:schema-version=\"1\" ac:local-id=\"c00588ca-254c-43ab-8686-6de3d0e15542\" ac:macro-id=\"2195500f69d6d0e805aaf94658bfc1c7\"><ac:parameter ac:name=\"root\"><ac:link /></ac:parameter><ac:parameter ac:name=\"startDepth\">1</ac:parameter></ac:structured-macro></p>",
      "approach_used": "endpoint_2",
      "word_count": 8
    },
    {
      "id": "443351041",
      "title": "CD Changes review",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/443351041",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/443351041/CD+Changes+review",
      "created": "2020-04-24T14:10:58.415Z",
      "content": "<p>Pre_check:<br />AWS region Fail check.-- done<br />aws Role check.-- it stopped only at first step. -- if checking other role than cicd.<br />Check if config file exists in S3 -- Done<br />Check if systemd service exists -- Done<br />Check if valid artifact is present in S3 -- Done<br />Ensure jq is installed-- Done</p><p>service Tag is handled by KMS.</p><p>Push_artifact:<br /><br />In latest, keeping multiple jars need to resolve &ndash; Done.</p><p>Deploy_app:<br />selected state rotate. &ndash; Done.<br />stopping first,<br />pull-env<br />jar pulling.</p><p>if doing force_deployment and health check failed on 1st instance, should not go to 2nd one. &ndash; pending.</p><p><br />force_deploy need to make without removing from TG go deploy. &ndash; need to hold on this release.<br />pull jar from s3 in pre step and mv in deploy. &ndash; Jegadesh is reviewing that part.</p>",
      "approach_used": "endpoint_2",
      "word_count": 130
    },
    {
      "id": "473989143",
      "title": "Integrate Github webhook with Jenkins",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/473989143",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/473989143/Integrate+Github+webhook+with+Jenkins",
      "created": "2020-04-30T09:03:54.912Z",
      "content": "<p>We are trying to use smee firewall to integrate github push events/ PR requests with Jenkins for automated build.</p><p><strong>Setup webhook-firewalls</strong> - <a href=\"https://jenkins.io/blog/2019/01/07/webhook-firewalls/\">https://jenkins.io/blog/2019/01/07/webhook-firewalls/</a></p><ol><li><p>Firstly - go to&nbsp;<a href=\"https://smee.io/\">https://smee.io/</a>&nbsp;and click &ldquo;Start a new channel&rdquo;:</p></li><li><p>Save the webhook proxy URL displayed and follow the instructions to install smee client on jenkins master.</p></li><li><p>Run the below command on Jenkins Master node to start smee  for polling github requests. The port should be the same one used by Jenkins. Run it as a background process and write logs to a file.</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"6eaf3f29-94ab-47ab-a089-197c6e6a84d4\"><ac:plain-text-body><![CDATA[smee --url {webook proxy URL} --path /github-webhook/ --port 8080]]></ac:plain-text-body></ac:structured-macro><p><strong>Setup Github-webhook</strong></p><ol><li><p>Go to the repo where we need to configure automated build</p></li><li><p>Click on Settings &gt; Webhooks &gt; Add Webhook </p></li><li><p>Put the Webhook Proxy URL we got from above step in the Payload URL Section, choose content type as &ldquo;application/json&rdquo;</p></li><li><p>Choose the event we wants to trigger and click on &ldquo;Add Webhook&rdquo;</p></li><li><p>Make sure our new webhook is showing in the list with a green tick (first request will be a ping)</p></li></ol><p><strong>Setup GitRepo and Jenkins</strong></p><ol><li><p>Add a jenkinsfile with appname in the application repo. Please refer - <a href=\"https://github.com/Meesho/devopstest/blob/master/devopstest\" data-card-appearance=\"inline\">https://github.com/Meesho/devopstest/blob/master/devopstest</a> </p></li><li><p>In the Jenkins job, select &ldquo;GitHub hook trigger for GITScm polling&rdquo; option</p></li><li><p>Make repository URL as the repository of application.</p></li><li><p>Correct branch and path of the jenkinsfile in the Jenkins job.</p></li><li><p>Trigger a dummy push/PR in the application repo from the branch configure on the Jenkins job.</p></li><li><p>Check on the logs of smee configured inside jenkins master and make sure it has triggered a Post with 200 response.</p></li><li><p>A build will be automatically triggered for the github activity.</p></li><li><p>Logs can be monitored in Github hook Log section of Jenkins - <a href=\"http://cicd.meeshotest.in/view/CI/job/backend-build/job/bac-s-build-devopstest-pipeline/GitHubPollLog/\">http://cicd.meeshotest.in/view/CI/job/backend-build/job/bac-s-build-devopstest-pipeline/GitHubPollLog/</a><br /><br /></p></li></ol><p>Note:<br />We have created this in 2 jobs - commonutils, devopstest. We need to analyse and find why multiple builds are triggered in devopstest.</p><p />",
      "approach_used": "endpoint_2",
      "word_count": 292
    },
    {
      "id": "474546192",
      "title": "SSH Key Management Tool",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/474546192",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/474546192/SSH+Key+Management+Tool",
      "created": "2020-04-30T09:00:05.240Z",
      "content": "<p>This tool will be used by to ship and manage SSH public keys of Users. It has a dashboard to create users with their respective keys. We can then deploy these keys to the respective EC2 instances owned by the User / Team from the dashboard on a single click.<br /><br />Tool will be Managed by <strong>APP-Support</strong> Team.</p><p>Console URL for Stage - <a href=\"http://ssh.meesho.com/\">http://ssh-stage.meesho.com/</a><br />Console URL for Prod - <a href=\"http://ssh-prod.meesho.com/\">http://ssh-prod.meesho.com/</a><br />Repo - <a href=\"https://github.com/Meesho/devops-ssh-mgmt\">https://github.com/Meesho/devops-ssh-mgmt</a><br /></p><p><u><strong>Creating a New User </strong></u></p><ol><li><p>Click on <a href=\"http://ssh.meesho.com/users.php\">Users</a> and select <em><a href=\"http://ssh.meesho.com/keys_setup.php\">Add User and Key</a></em></p></li><li><p>Provide Username, SSH Public Key (created from bastion instance) and select the team from the drop down box.<br /><span style=\"color: rgb(255,153,31);\"><em>Note:</em></span><em> Username should not contain white spaces. Standard Format: firstname.lastname</em></p></li><li><p>Click add and the User will be displayed on the Users list</p></li><li><p>Click on <em>Edit</em> option to Edit details of a User and <em>Delete</em> option for deleting the User.</p></li></ol><p /><p><u><strong>Mapping Host Group (group of hosts)  with Team</strong></u></p><p><a href=\"http://ssh.meesho.com/show_all_hosts.php\">All Hosts</a> will display all the Hosts. We have a pagination at the top of the view.</p><p>Host details will be updated in the tool by a script which is running on every 6 hours for now.</p><p /><p><u><strong>Deploy Team to Host Groups</strong></u></p><ol><li><p>Click on <a href=\"http://ssh.meesho.com/deploy_keyring.php\">Deploy-Team</a> and select the team which is ready to get deployed.</p></li><li><p>Wait for the result page to come up which displays the details on Users being deployed to hosts.</p></li><li><p>An SSH failure error message is displayed with the list of Hosts, if the instance is not allowing for the Tool to SSH.</p></li><li><p>We have a status flag set in DB for the Users which helps in deploying only newly added users and ignore Users which are already deployed to the Host groups.</p></li><li><p>Login to bastion and try to SSH from bastion to any of the host deployed using the user name created.<br />eg: ssh <a href=\"mailto:arun.subash@172.31.29.33\">arun.subash@172.31.29.33</a></p></li><li><p>Deploy-Team can also be triggered after editing a user details / after mapping the team to a Host group.<br /></p></li></ol><p><br /></p>",
      "approach_used": "endpoint_2",
      "word_count": 319
    },
    {
      "id": "498466831",
      "title": "Configuration Management Tool",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/498466831",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/498466831/Configuration+Management+Tool",
      "created": "2020-07-24T18:30:59.046Z",
      "content": "<p>This tool will be used to manage environment variables of all services.</p><p>India Sandbox URL - <a href=\"http://configs.internal.meeshotest.in/index.php\">http://configs.internal.meeshotest.in</a><br />India Prod URL - <a href=\"http://configs-prod.meesho.com/\">https://configs-prod.meesho.com/</a></p><p>Indo prod URL : <a href=\"http://configs-prod.meesho.co.id/index.php\">http://configs-prod.meesho.co.id/index.php</a></p><p /><ol><li><p>Login with google authentication from the above URL.</p></li><li><p>Select the environment from the dropdown menu as suggested by the alert message.</p></li><li><p>Select the service from the next drop down menu.</p></li><li><p>Add, Edit or Delete key/values listed for the selected service.</p></li><li><p>Click on the submit button after all modifications are made.</p></li></ol><p>Note: Environment and Service will be listed according to the permissions users are having for it. Please reach out to App-Support Team if anyone needs access to any other service / environment which is not listed currently for them.</p><p>These updated environment variables will be applied to the services on the next CD deployment automatically. So, Please be careful while updating key/value pairs of the environment variables.</p><p /><p><strong>Admin Panel (Managing Service Access)</strong></p><p>This section will be used only by App-Support team and QA managers to manage users with service mapping.</p><ol><li><p> Select Admin from User Profile dropdown on the right end of the header section. This will be hidden for normal users.</p></li><li><p>Select the Environment from the dropdown menu.</p></li><li><p>Click on edit Action against the user to update services for that user in the selected environment.</p></li><li><p>Select / Unselect services for the User. There is a Select ALL and Unselect ALL button for bulk mapping.</p></li><li><p>Click on Submit button to update the service list for the User.</p></li></ol><p><strong>Future Updations:</strong></p><ol><li><p>Onboarding env variables for new service.</p></li><li><p>Onboarding new user via Admin Panel.</p></li><li><p>Providing history of the changes made for a particular service.</p></li></ol><p /><p />",
      "approach_used": "endpoint_2",
      "word_count": 255
    },
    {
      "id": "519307360",
      "title": "Sonarqube remaining tasks",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/519307360",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/519307360/Sonarqube+remaining+tasks",
      "created": "2020-05-17T09:31:41.157Z",
      "content": "<ul><li><p> jobs need to be created for all the services .</p></li><li><p> need to add check style for all  the project </p></li><li><p>need to test PR for master branch as well .</p></li><li><p>security aspects we need to check .</p></li><li><p>Need to check compute engine part for the sonar instances.</p></li></ul><p /><p><strong>Note</strong>: Will add some more tasks as per the suggestion by developers</p>",
      "approach_used": "endpoint_2",
      "word_count": 57
    },
    {
      "id": "651427845",
      "title": "DevOps SLA (Service Level Agreement)",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/651427845",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/651427845/DevOps+SLA+Service+Level+Agreement",
      "created": "2022-01-24T09:54:40.936Z",
      "content": "<p /><ul><li><p>SLA for tasks: In this document we cover SLA for only Tech Infrastructure tasks and not for Incidents (production issues). Incident management process is currently WIP.</p></li><li><p>SLA unit: The units here are in days(working days).</p></li><li><p>Adherence: Targeted adherence to SLA is<strong> 95%.</strong></p></li><li><p>Environment: Prod environment tickets will only be tracked for adherence to the stated SLA</p></li></ul><table data-layout=\"default\" ac:local-id=\"bf4c9436-4d2e-4106-a088-60a81dee1f5d\"><colgroup><col style=\"width: 178.0px;\" /><col style=\"width: 331.0px;\" /><col style=\"width: 119.0px;\" /><col style=\"width: 128.0px;\" /></colgroup><tbody><tr><td><p>Task</p></td><td><p>Definition</p></td><td><p>Acknowledgement SLA</p></td><td><p>Completion SLA</p></td></tr><tr><td><p>New Infra Provisioning</p></td><td><p>Includes new application onboarding with ASG/TG/CI/CD/Alerting/Monitoring etc set</p></td><td><p>1</p></td><td><p>8</p></td></tr><tr><td><p>Running Infra Augmentation</p></td><td><p>Adding/decommissioning servers/space, creating of S3 etc</p></td><td><p>1</p></td><td><p>3</p></td></tr><tr><td><p>New Joinee Access</p></td><td><p>New joinee in tech team</p></td><td><p>1</p></td><td><p>1</p></td></tr><tr><td><p>Existing User Access</p></td><td><p>An access level change to any existing user</p></td><td><p>1</p></td><td><p>1</p></td></tr><tr><td><p>CI/CD - Existing Pipeline Modification</p></td><td><p>Change to an existing pipeline</p></td><td><p>1</p></td><td><p>3</p></td></tr><tr><td><p>Alerting/Monitoring- Edit Existing One</p></td><td><p>New alert on production, change in threshold/recipient of existing alert</p></td><td><p>1</p></td><td><p>2</p></td></tr><tr><td><p>Adhoc Tasks</p></td><td><p>Anything that doesn&rsquo;t fall in the above&nbsp; category like file/data movement, debugging etc</p></td><td><p>1</p></td><td><p>Based on effort</p></td></tr></tbody></table><p /><p>SLA Tracking : </p><p>SLA tracking is manual with current DEVOPS Jira , moving to automation with the proper Task based SLA.</p><p>New Jira project is in place and tested well. It adds below values.</p><p><br /><br /></p><p />",
      "approach_used": "endpoint_2",
      "word_count": 172
    },
    {
      "id": "927629323",
      "title": "How to enable telegraf for existing services",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/927629323",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/927629323/How+to+enable+telegraf+for+existing+services",
      "created": "2020-09-25T06:19:33.840Z",
      "content": "<ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"93b55608-87b2-48a7-a775-713ea8ad555a\"><ac:rich-text-body><p>Once all the servers come up with new &ldquo;golden-ami&ldquo; this procedure no longer needed. New golden ami&rsquo;s already have telegraf pre-installed.</p></ac:rich-text-body></ac:structured-macro><p /><p><strong>Steps to enable telegraf for existing services:</strong></p><ol><li><p>There is a Ansible <a href=\"https://github.com/Meesho/devops-meesho/tree/master/ansible/lib/roles/software/setup_telegraf\">role</a> to install <code>telegraf</code> on all the instances of a service.</p></li><li><p>Add <code>telegraf:enabled</code> tag to your asg so that all the new nodes will have this tag</p></li></ol><p /><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"a2b3c7a0-bd7f-4045-a7ba-83f45ff2332b\"><ac:rich-text-body><p>If telegraf already installed on the machine then just make sure <code>tags</code> are set properly so prometheus will discovery and scrape your metrics</p></ac:rich-text-body></ac:structured-macro><p /><p><strong><u>Note to devops:</u></strong></p><p>Once all the backend nodes have telegraf installed, we no longer need <code>telegraf</code> tags on the nodes. Just update prometheus config to scrape all the nodes with <code>backend</code>tags.</p><p />",
      "approach_used": "endpoint_2",
      "word_count": 118
    },
    {
      "id": "2133033176",
      "title": "Infrastructure",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2133033176",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2133033176/Infrastructure",
      "created": "2021-08-30T05:29:24.058Z",
      "content": "<p><em><span style=\"color: rgb(255,86,48);\">K8s Infra requirements/ best practices:</span></em></p><ol><li><p>EKS infra should be deployed using tf/eksctl and the entire setup should be easily deployable using few commands/clicks.</p></li><li><p>Ensure sufficient #IPs in chosen subnets considering future growth. Separate VPC/Subnet is better.</p></li><li><p>Use diversified instance types spread across multi-AZ and using EKS-optimized AMI</p></li><li><p>Cluster Autoscaler and HPA should be rightly defined.</p></li><li><p>Cluster Observability/ Monitoring should be sufficient for easy debugging by developers. For logging, system logs/ CW logs should be sent to centralized logging system vua fluentd/flunt-bit. For metrics, enable kube-state-metrics/metrics-server and using node-exporter + Prometheus + Grafana or Newrelic.</p></li></ol>",
      "approach_used": "endpoint_2",
      "word_count": 91
    },
    {
      "id": "2133786660",
      "title": "Kubernetes",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2133786660",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2133786660/Kubernetes",
      "created": "2021-08-23T15:13:15.926Z",
      "content": "<p><ac:structured-macro ac:name=\"pagetree\" ac:schema-version=\"1\" ac:local-id=\"4ad4f9e8-5201-4a23-9a7e-592fa4a1e2bd\" ac:macro-id=\"16b14385937b26e4e932f33ab42d7829\"><ac:parameter ac:name=\"root\"><ac:link /></ac:parameter><ac:parameter ac:name=\"startDepth\">1</ac:parameter></ac:structured-macro></p><p />",
      "approach_used": "endpoint_2",
      "word_count": 9
    },
    {
      "id": "2133884936",
      "title": "Deployment process and requirements",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2133884936",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2133884936/Deployment+process+and+requirements",
      "created": "2021-08-30T05:26:55.327Z",
      "content": "<p><em><span style=\"color: rgb(255,86,48);\">Key points to discuss/decide around deployment of any application in the EKS/k8s @Meesho:</span></em></p><ol><li><p>Deployment manifests should be written using Helm chart instead of plain yaml files to maximize the reusability and ease of maintenance/changes across all applications. A single helm chart with different one/more values.yaml for each application is the best way to achieve this.</p></li><li><p>Helm project structure in github should be properly designed. Jenkins pipeline should just run &ldquo;helm upgrade -i -f $(values.yaml) -f $(values.yaml) -f &hellip;&rdquo; with appropriate timeout values and any failure in deployment should be reported via Slack/PD. A sample project structure can be found <a href=\"https://medium.com/faun/helm-charts-for-more-complex-projects-and-how-to-secure-them-a1dfde804226\">here</a>.</p></li><li><p>Rollback should be easy, a developer should be able to easily rollback to a previous version of release.</p></li><li><p>Canary deployment should be supported. This helps developers to gain confidence before rolling out changes across all PODs. Blue-green deployment support is also helpful (by creating new service for example).</p></li><li><p>Every application must have Liveness/Readiness/Startup probe</p></li><li><p>Request and Limit values for CPU/Memory needs to be present and shouldn't be too different.</p></li><li><p>For service to service internal communication, service.namespace should be used and and not via Ingress</p></li><li><p>Every app should have it&rsquo;s own namespace.</p></li><li><p>Every app should have minimum 2 PODs deployed (this is for production eks) with a PDB defined.</p></li><li><p>Log should be written to STDOUT/STDERR unbuffered and not to a file. Also, avoid multi-line logging (Use json format for multi-line logging).</p></li><li><p>Label logs with Namespace so that Indexing can happen accordingly.</p></li><li><p>For monitoring, telegraf exporter for prometheus should work as usual. Newrelic integration needs to be discussed as having NR in every POD is likely to increase our CU consumption (discussion scheduled with Newrelic team).</p></li><li><p>For every service/app in k8s, alerting for Container restart count, POD termination alerts should be in place to send alerts in Slack, for example.</p></li><li><p>HPA should be properly defined to ensure scaling happens in the right manner. A combination of CPU/Memory and any appropriate custom metrics can be used.</p></li><li><p>For configuration variables we can use Configmap and for secrets we can use SOPS+KMS &rarr; k8s/helm secret.</p></li><li><p>Define POD anti-affinity using appropriate topologyKey (Like <a href=\"http://kubernetes.io/hostname\">kubernetes.io/hostname</a>) to spread PODs across nodes (default scheduler is expected perform decently but that doesn&rsquo;t happen always).</p></li><li><p>Use MultiStage build to reduce Size of the Images and hence faster deployment.</p></li><li><p>Using Stable for community base images is fine but for app images, we should use a fixed standard. We can use tagging like $(branch)-$(commit-hash)-$(Jenkins build-id).</p></li><li><p>Container security analysis should be enabled (ECR scanning/ Clair).</p></li></ol>",
      "approach_used": "endpoint_2",
      "word_count": 396
    },
    {
      "id": "2142470194",
      "title": "Roadmap and Priorities",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2142470194",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2142470194/Roadmap+and+Priorities",
      "created": "2021-08-31T19:21:54.414Z",
      "content": "<p>tracked <a href=\"https://docs.google.com/spreadsheets/d/1P1C8MX8vXZLtycDxkhM_ViOMtTVhn-nFzE5xpVhHqhs/edit#gid=0\">here</a></p>",
      "approach_used": "endpoint_2",
      "word_count": 3
    },
    {
      "id": "2163769443",
      "title": "RDS-Proxy",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2163769443",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2163769443/RDS-Proxy",
      "created": "2021-09-17T06:47:07.509Z",
      "content": "<ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"9e6903e0-014b-4e0a-a144-d43115758a0d\" ac:macro-id=\"99a337d7-4547-4e1b-9106-c12bcd0e2dfd\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">7</ac:parameter></ac:structured-macro><h2><em><span style=\"color: rgb(255,86,48);\">Why RDS-Proxy</span></em></h2><p /><p>RDS proxy works by pooling and sharing DB connections and thus makes applications more scalable as well as resilient to database failures. They key benefits of using RDS proxy are the Connection pooling and sharing which Improves application performance by reducing the number of open database connections. Connection pooling is an optimization that enables applications to share and re-use database connections, thus reducing the load on the database itself. Opening and closing a new database connection is CPU-intensive whereas additional memory is needed for each open connection. Connection pooling also removes the need to worry about database connections in the application code. Each database transaction uses one underlying database connection which can be reused once the transaction has finished. This transaction-level reuse is called connection multiplexing (or connection reuse). In connection multiplexing, database connections are shared between client connections which helps minimize the resource overhead on the database server. </p><p>For more details on connection pooling: <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.howitworks.html#rds-proxy-connection-pooling\"><u>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.howitworks.html#rds-proxy-connection-pooling</u></a></p><hr /><p /><h2><em><span style=\"color: rgb(255,86,48);\">Limitations/ Challenges</span></em></h2><hr /><p /><h2><em><span style=\"color: rgb(255,86,48);\">Deployment diagram</span></em></h2><hr /><p /><h2><em><span style=\"color: rgb(255,86,48);\">Rollout plan</span></em></h2><hr />",
      "approach_used": "endpoint_2",
      "word_count": 184
    },
    {
      "id": "2164948997",
      "title": "Database Engineering",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2164948997",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2164948997/Database+Engineering",
      "created": "2021-09-15T05:17:53.977Z",
      "content": "<ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"cc8b93ee-46ae-48c5-8723-a9fa760873d8\" ac:macro-id=\"4cb6d03826f0baef31f8bce27c610bb8\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">7</ac:parameter></ac:structured-macro><p><ac:structured-macro ac:name=\"pagetree\" ac:schema-version=\"1\" ac:local-id=\"24ab1584-edee-487d-8bf0-fb63e24e91d2\" ac:macro-id=\"c6dd2fdbbe7f3e11b1fa92be388731fa\"><ac:parameter ac:name=\"root\"><ac:link /></ac:parameter><ac:parameter ac:name=\"startDepth\">1</ac:parameter></ac:structured-macro></p>",
      "approach_used": "endpoint_2",
      "word_count": 15
    },
    {
      "id": "2225143827",
      "title": "Naming Conventions and Mandatory Tags",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2225143827",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2225143827/Naming+Conventions+and+Mandatory+Tags",
      "created": "2022-05-25T14:03:47.198Z",
      "content": "<p><strong>EC2  and Other Resources</strong>:</p><p /><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"ee5776b7-0414-4476-a567-c6646c313309\"><ac:parameter ac:name=\"language\">none</ac:parameter><ac:plain-text-body><![CDATA[Mandatory Tags:\nName ( Name of the Resource )\nenv ( prod /stage / qa / perf )\npod ( backend / data-platform / data-science )\nbu ( backend / data-platform / data-science )\nteam (refer the team name from below sheet:\nhttps://docs.google.com/spreadsheets/d/1nNJBKL-qLTGfFSOnln3JFgeWcGmwFEl7OosLM5ZVzIY/edit#gid=0)\nservice ( Name of the Service )\nalerts ( slack channel name  to route alerts)\npriority ( p0 , p1 , p2 , p3 )\n\n\nOptional Tags Based on Usage\n\nbackup ( nightly ) Mandatory for one machine on each service\ntelegraf ( enabled ) for telegraf enabled machines\nnewrelic ( yes / true ) for new relic enabled machines\nmonitoring ( no ) to disable monitoring for EC2]]></ac:plain-text-body></ac:structured-macro><p /><p><strong>Database(RDS, Opensearch, MSK, Elasticache): </strong><br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d1a8b07d-4f94-4783-bc7b-1f5f7ed2634c\"><ac:plain-text-body><![CDATA[Mandatory Tags:\n\nenv ( prod /stage / qa / perf )\npod ( backend / data-platform / data-science )\nbu ( backend / data-platform / data-science )\nteam (refer the team name from below sheet:https://docs.google.com/spreadsheets/d/1nNJBKL-qLTGfFSOnln3JFgeWcGmwFEl7OosLM5ZVzIY/edit#gid=0)\nservice ( Name of the Service )\nbackup ( yes ) --> for at least one slave of each RDS, since it is being used \nfor daily manual snapshots]]></ac:plain-text-body></ac:structured-macro><p /><p>Naming Conventions On Infra or Resource Creation:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"cf16dcdd-1427-4942-85e2-d3562a973d98\"><ac:plain-text-body><![CDATA[1. bac-p-ServiceName ( for backend services)\n2. front-p-ServiceName ( for frontend Services )\n3. data-p-ServiceName ( for dataplatform services )\n4. platform-p-ServiceName ( few services are having like this, above 3 are mandatory\n conventions )]]></ac:plain-text-body></ac:structured-macro><p />",
      "approach_used": "endpoint_2",
      "word_count": 237
    },
    {
      "id": "2242314252",
      "title": "Database Best Practices",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2242314252",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2242314252/Database+Best+Practices",
      "created": "2021-11-30T10:39:36.829Z",
      "content": "<ac:structured-macro ac:local-id=\"c329000a-553a-4618-810c-af47ef253ed8\" ac:macro-id=\"242d81c472ace2abbe31174f68b41eff\" ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">7</ac:parameter><ac:parameter ac:name=\"outline\">true</ac:parameter></ac:structured-macro><h2><span style=\"color: rgb(255,86,48);\">Why ??</span></h2><p>As we all know that DB is a critical component of any application. Most of the time bottlenecks related to the DB can be for-seen during the development or designing time and these have to dealt before putting any Database system into production . and for this we need a set of good practices. In short small decisions in the beginning will have huge cumulative impact.</p><h2><u>Overview</u></h2><p>This document will contain all the best practices like Schema Design,Query Design and other Database related operations. </p><h2><u>PIC</u></h2><p>DBE Team.</p><h2><u>GOAL</u></h2><p>Agenda is to prepare a very detailed document which can help developers in designing  schema/queries more efficiently and in more optimised way.</p><p>We&rsquo;ll cover following topics in this document . These are mentioned below :-</p><ul><li><p>Schema Design</p></li><li><p>Query  Designing</p></li><li><p>DBE SLA</p></li><li><p>DB Provisioning Review Details.</p></li></ul><h3><u>Schema Design</u></h3><ol><li><p>Avoid using reserve  <a href=\"https://dev.mysql.com/doc/refman/5.7/en/keywords.html\">Keywords</a>. </p></li><li><p>Always use <ac:inline-comment-marker ac:ref=\"f65c594a-eebf-462f-8c98-084f1ab4844a\">proper Data Type Like f</ac:inline-comment-marker>or storing first_name and last name don&rsquo;t use varchar(500) or varchar(255).</p></li><li><p>Use proper and meaning full naming standards for the tables/columns.</p></li><li><p><ac:inline-comment-marker ac:ref=\"c65a36db-c332-40b0-b836-93bdc60d27dd\">Avoid using foreign keys</ac:inline-comment-marker> as this has serious performance impacts.</p></li><li><p>Use Enum over varchar where we have fixed values.</p></li><li><p>Every table should have an id column and it should be the primary key.</p></li><li><p>Keep the primary keys always unsigned and auto increment type only.</p></li><li><p>Every table must have <ac:inline-comment-marker ac:ref=\"b1fa66c5-0c55-424c-ae04-f152e8e9b986\">created_at and updated_at</ac:inline-comment-marker> column and these columns should be indexed and in default mode. It will help data engineering team in pulling delta data for report generation and make sure these columns are not being written via application.</p></li><li><p>Use &ldquo;NOT NULL&ldquo; if possible for fields in table.</p></li><li><p>Used char for storing fixed length things.</p></li><li><p>use <ac:inline-comment-marker ac:ref=\"24a5ea60-4a65-4b4b-80ea-033c923eed71\">small and compact columns</ac:inline-comment-marker> and tables name if possible.</p></li><li><p>Use vertical partitioning if required.</p></li><li><p>Avoid <ac:inline-comment-marker ac:ref=\"0015b0d5-0e20-43f6-825c-2de33c501e5c\">TEXT/BLOB Data types</ac:inline-comment-marker> as much as we can.</p></li><li><p>Use <ac:inline-comment-marker ac:ref=\"33743765-b399-4616-af69-5b4e070b01c5\">utf8mb4 character encoding</ac:inline-comment-marker> while schema creation.</p></li><li><p>Use datetime data type instead of timestamp data type.</p></li><li><p>Use of Stored Procedure , Triggers and functions are not recommended in prod env. They have serious scaling and debugging issues.</p></li><li><p>Table having delete use cases should have soft delete columns like (is_delete/deleted/is_deleted) etc.</p></li><li><p>Make Sure to use Indexes Prudently </p></li></ol><p>                                  a)  PRIMARY KEY &gt; UNIQUE &gt; INDEX = KEY</p><p>                                  b) An index may speed up a SELECT by orders of magnitude and will slow down </p><p>                                        INSERTs a little.                                </p><p>                                  c) Understand the power of compound (aka &quot;composite&quot;) indexes ,INDEX(a,b) may be               </p><p>                                        may be much better than Index(a),Index(b).              </p><p>                                  d) The columns in a composite index are used left to right.</p><p>                                  e) INDEX(a,b) covers for INDEX(a), so drop the latter.</p><p>                                  f) Index/Key must be named with prefix idx_ column.</p><p /><h3><u>Query Designing</u></h3><p /><ol><li><p>Use a semicolon (;) at end of every statement (Ex: select id from table_name limit 5; update table_name set id=yy where name='yyy';)</p></li><li><p>All DML should be on PRIMARY KEY.</p></li><li><p>Do not hide an indexed column inside a function call: DATE(x) = '...' or LCASE(col) = 'foo'.</p></li><li><p>While selecting data from data use column name , Avoid Using select * .</p></li><li><p>Don&rsquo;t use quotes while designing query for numeric fields like bigint ,int, smallint, tinyint etc.</p></li><li><p>Use quotes &lsquo;&rsquo; for string fields like varchar ,char ,enum ,text etc.</p></li><li><p>Manage connection properly.</p></li><li><p>Configure/optimise ORM, There should be no unnecessary from ORM (Like Show Character Set, Select collation) etc.</p></li><li><p>Use order by and sort by clause only if needed.</p></li><li><p>Split the big delete and inserts.</p></li><li><p>Always use joins instead of subqueries. Join should always be on indexed columns.</p></li><li><p>While designing any query , maintain the same order of columns as the indexes created . Especially for the case of composite keys. Precedence is left to right.</p></li><li><p>Avoid IN clause for larger values , There should be no subquery in 'IN' clause there should be absolute values. Turn them in Join (If really requires).</p></li><li><p>Use limit 1 , where you are looking for a row.</p></li><li><p>Avoid select on Null columns.</p></li><li><p>Avoid using ' ' for bulk searches.</p></li><li><p>Don't mix DISTINCT and GROUP BY.</p></li><li><p>Be explicit about UNION ALL vs UNION DISTINCT -- it makes you think about which to use.</p></li><li><p>Commit often. Keep transaction size small.</p></li><li><p>Do not use `=` condition on partition key with Partitioning query.</p></li><li><p>When updating multiple columns in mysql, use comma as a separator in SET clause instead of and (Ex &quot;update table_name set col1=value1, col2=value2 where .....;&quot; instead of &quot;update table_name set col1=value1 and col2=value2 where .....;&quot; ).</p><p /><p /><p /><p /><p /></li></ol><p /><h3><u>References</u></h3><ol><li><p>Mysql Reserved Keywords. <a data-card-appearance=\"inline\" href=\"https://dev.mysql.com/doc/refman/5.7/en/keywords.html\">https://dev.mysql.com/doc/refman/5.7/en/keywords.html</a> .</p></li><li><p> More About Indexes    <a data-card-appearance=\"inline\" href=\"https://dev.mysql.com/doc/refman/8.0/en/multiple-column-indexes.html\">https://dev.mysql.com/doc/refman/8.0/en/multiple-column-indexes.html</a> .</p></li><li><p>Why to use UTF8MB4  <a data-card-appearance=\"inline\" href=\"https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-utf8mb4.html\">https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-utf8mb4.html</a> .</p></li><li><p>Mysql Thumb Rule <a data-card-appearance=\"inline\" href=\"http://mysql.rjweb.org/doc.php/ricksrots\">http://mysql.rjweb.org/doc.php/ricksrots</a> .</p></li><li><p>MySQL Schema Design Best Practices  : <a data-card-appearance=\"inline\" href=\"http://mysql.rjweb.org/doc.php/schema_best_practices_mysql\">http://mysql.rjweb.org/doc.php/schema_best_practices_mysql</a> </p></li><li><p>More on Indexes : <a data-card-appearance=\"inline\" href=\"http://mysql.rjweb.org/doc.php/index_cookbook_mysql\">http://mysql.rjweb.org/doc.php/index_cookbook_mysql</a> </p></li><li><p>Splitting Bigger Updates/Deletes for Safer Side: <a data-card-appearance=\"inline\" href=\"http://mysql.rjweb.org/doc.php/deletebig\">http://mysql.rjweb.org/doc.php/deletebig</a> </p><p /></li></ol><p />",
      "approach_used": "endpoint_2",
      "word_count": 749
    },
    {
      "id": "2245263396",
      "title": "DBA Onboarding",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2245263396",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2245263396/DBA+Onboarding",
      "created": "2021-10-26T06:35:05.878Z",
      "content": "<h1><u>DBA Onboard:</u></h1><p /><p><span style=\"color: rgb(0,0,255);\">Welcome to the Meesho family !! </span>Meesho is an Indian social e-commerce company, headquartered in Bangalore. It was founded by IIT Delhi graduates Vidit Aatrey and Sanjeev Barnwal in December 2015. It enables small businesses and individuals to start their online stores via social channels such as WhatsApp, Facebook, Instagram</p><h3><strong>Required access:</strong></h3><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\"><strong>1. VPN Access - Prod and Sandbox</strong></span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\"><strong>2. New Relic Access</strong></span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\"><strong>3. AWS Access - Prod and Sandbox</strong></span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\"><strong>4. Jenkins Access - Prod and Dev. </strong></span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\"><strong>5. Prod and Dev Bastion access. (JUMPBOX)</strong></span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\"><strong>6. Confluence</strong></span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\"><strong>7. Jira</strong></span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\"><strong>8. grafana</strong></span></p><p style=\"margin-left: 30.0px;\" /><h2>1.Jira and Confluence access:</h2><table data-layout=\"default\" ac:local-id=\"50bbcc33-bb99-4883-88a2-43556adf4d22\"><colgroup><col style=\"width: 680.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#434343\"><p><span style=\"color: rgb(255,255,255);\">Drop a mail along with the manager approval to </span><u><span style=\"color: rgb(255,153,0);\"><a href=\"mailto:itsupport@meesho.com\">itsupport@meesho.com</a></span></u><span style=\"color: rgb(255,255,255);\"> for jira and confluence access, So you will receive mail with activation link once the access has been granted</span></p></td></tr></tbody></table><h3><span style=\"color: rgb(0,0,0);\"> 2. New Relic access:</span>       </h3><table data-layout=\"default\" ac:local-id=\"d55320ab-b6a1-4181-b312-2b8099995abe\"><colgroup><col style=\"width: 680.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#434343\"><p><span style=\"color: rgb(255,255,255);\">Drop a mail along with the manager approval to </span><u><span style=\"color: rgb(255,153,0);\"><a href=\"mailto:pavankumar.m@meesho.com\">pavankumar.m@meesho.com</a></span></u><span style=\"color: rgb(255,255,255);\"> for New relic access, Once the access has been granted, you will receive email like below</span></p></td></tr></tbody></table><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"205\" ac:original-width=\"419\" ac:width=\"340\"><ri:attachment ri:filename=\"att_1_for_2245263396.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>3. For the below access:</strong></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\">1. VPN Access - Prod and Sandbox </span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\">2. AWS Access - Prod and Sandbox </span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\">3. Jenkins Access - Prod and Dev.  </span></p><p style=\"margin-left: 30.0px;\"><span style=\"color: rgb(67,67,67);\">4. Prod and Dev Bastion access.</span></p><table data-layout=\"default\" ac:local-id=\"4130a3f8-41c1-43bf-b767-b020bad7be12\"><colgroup><col style=\"width: 680.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#000000\"><p><span style=\"color: rgb(255,255,255);\">Drop a mail along with the manager approval to </span><u><span style=\"color: rgb(255,153,0);\"><a href=\"mailto:app-support@meesho.com\">app-support@meesho.com</a></span></u><span style=\"color: rgb(255,255,255);\"> team, They will they will help you in getting the access for the above things</span><br /><br /><span style=\"color: rgb(255,0,0);\"><strong>Note:</strong></span><span style=\"color: rgb(255,255,255);\"><strong> </strong>kindly generate your public key and share the same with  </span><u><span style=\"color: rgb(255,153,0);\"><a href=\"mailto:app-support@meesho.com\">app-support@meesho.com</a></span></u><span style=\"color: rgb(255,255,255);\"> for Prod and Dev Bastion access</span></p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"261\" ac:original-width=\"369\"><ri:attachment ri:filename=\"att_2_for_2245263396.png\" ri:version-at-save=\"1\" /></ac:image></td></tr></tbody></table><p /><p><strong>4. Jira for getting admin access on AWS:</strong></p><table data-layout=\"default\" ac:local-id=\"af6a3b10-e5fd-43c5-86b1-b8d61ce3b097\"><colgroup><col style=\"width: 680.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#000000\"><p><span style=\"color: rgb(255,255,255);\">Raise a jira for getting the admin access on AWS console</span></p><p><span style=\"color: rgb(234,153,153);\">Reference Jira:</span><span style=\"color: rgb(255,153,0);\"> </span><span style=\"color: rgb(255,255,255);\"> </span><span style=\"color: rgb(255,153,31);\"> </span><span style=\"color: rgb(0,0,255);\"><a href=\"https://meesho.atlassian.net/browse/DEVOPS-7354\">https://meesho.atlassian.net/browse/DEVOPS-7354</a></span></p></td></tr></tbody></table><p /><p><strong>Bastion Host details:</strong></p><table data-layout=\"default\" ac:local-id=\"238c1bf3-a09c-4609-92e7-2763a7148259\"><colgroup><col style=\"width: 760.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#000000\"><p><span style=\"color: rgb(255,255,255);\">bastion.meeshotest.in   &mdash;&gt; sandbox bastion</span></p><p><span style=\"color: rgb(255,255,255);\">prod-bastion.meeshoint.in   &mdash;&gt; prod bastion</span></p></td></tr></tbody></table><p><strong>Some imp Links:</strong></p><table data-layout=\"default\" ac:local-id=\"871f768b-996e-4fcf-86b0-e747ed11f2e4\"><colgroup><col style=\"width: 340.0px;\" /><col style=\"width: 340.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#auto\"><p><strong>Item</strong></p></td><td data-highlight-colour=\"#auto\"><p><strong>Link URL</strong></p></td></tr><tr><td data-highlight-colour=\"#auto\"><p>Grafana </p></td><td data-highlight-colour=\"#auto\"><p><span style=\"color: rgb(17,85,204);\"><a href=\"https://grafana.meesho.com/d/000000010/linux-servers?orgId=1\">https://grafana.meesho.com/d/000000010/linux-servers?orgId=1</a></span></p><p><span style=\"color: rgb(17,85,204);\"><a href=\"https://grafana.meesho.com/d/J1woaUG7k/aws-rds-copy?orgId=1\">https://grafana.meesho.com/d/J1woaUG7k/aws-rds-copy?orgId=1</a></span></p></td></tr><tr><td data-highlight-colour=\"#auto\"><p>AWS Console </p></td><td data-highlight-colour=\"#auto\"><p>sandbox AWS :  <u><span style=\"color: rgb(17,85,204);\"><a href=\"https://898247906677.signin.aws.amazon.com/console\">https://898247906677.signin.aws.amazon.com/console</a></span></u></p><p>prod AWS :  <u><span style=\"color: rgb(17,85,204);\"><a href=\"https://meesho.signin.aws.amazon.com/console\">https://meesho.signin.aws.amazon.com/console</a></span></u></p></td></tr><tr><td data-highlight-colour=\"#auto\"><p>Jenkins</p></td><td data-highlight-colour=\"#auto\"><p>Prod: <u><span style=\"color: rgb(17,85,204);\"><a href=\"http://cicd-prod.meeshoint.in/\">http://cicd-prod.meeshoint.in/</a></span></u></p><p>Sandbox: <u><span style=\"color: rgb(17,85,204);\"><a href=\"http://cicd.meeshotest.in/\">http://cicd.meeshotest.in/</a></span></u></p></td></tr></tbody></table><p /><p><strong>DB Master Sheet:</strong></p><table data-layout=\"default\" ac:local-id=\"237d2434-e7ed-4a2d-86bf-aad5d3853fd9\"><colgroup><col style=\"width: 680.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#auto\"><p>Sheet contains the DB instance list: <a href=\"https://docs.google.com/spreadsheets/d/1sP_W9lrHQRR4Q_ZIOIgIQKrjP1U_J0rEfwuXnV7LxaU/edit#gid=0\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1sP_W9lrHQRR4Q_ZIOIgIQKrjP1U_J0rEfwuXnV7LxaU/edit#gid=0</a> </p><p>                                         Request access from the DBA for the DB Master sheet, which is the Database master sheet which contains the username and password to login to the RDS instances<br /><span style=\"color: rgb(67,67,67);\">It will be shared 1:1 internally</span></p></td></tr></tbody></table><p /><p><strong>Imp slack channel:</strong></p><table data-layout=\"default\" ac:local-id=\"30a19084-2f02-4106-838c-17266e78379f\"><colgroup><col style=\"width: 680.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#auto\"><p><span style=\"color: rgb(29,28,29);\">db-alerts</span></p><p><span style=\"color: rgb(29,28,29);\">database-engineering</span></p><p><span style=\"color: rgb(29,28,29);\">app-engg </span></p><p><span style=\"color: rgb(29,28,29);\">supplier-front-alerts </span></p></td></tr></tbody></table><p /><p /><p /><p style=\"margin-left: 30.0px;\" /><p style=\"margin-left: 30.0px;\" /><p style=\"margin-left: 30.0px;\" />",
      "approach_used": "endpoint_2",
      "word_count": 531
    },
    {
      "id": "2248146959",
      "title": "Increase max allowed connection in MongoDB",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2248146959",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2248146959/Increase+max+allowed+connection+in+MongoDB",
      "created": "2021-10-28T07:48:42.173Z",
      "content": "<p>Steps to increase max allowed connection in MongoDB</p><ol><li><p>Login to replica set (secondary node) of MongoDB.</p></li><li><p>Check current number of allowed max connection. Run below command to check:<br />db.serverStatus().connections;</p></li><li><p>Stop mongodb service if running. Check the status - start/stop using below commands:<br />service mongod status<br />service mongod stop</p></li><li><p>Create directory and file as follow:<br />sudo mkdir /etc/systemd/system/mongod.service.d/<br />sudo vi /etc/systemd/system/mongod.service.d/limits.conf</p></li><li><p>Add below lines to limits.conf:<br />[Service]<br />LimitFSIZE=infinity<br />LimitCPU=infinity<br />LimitAS=infinity<br />LimitMEMLOCK=infinity<br />LimitNOFILE=64000<br />LimitNPROC=64000</p></li><li><p>Run below commands to reload MongoDB and start mongo services:<br />systemctl daemon-reload<br />systemctl restart mongod</p></li><li><p>Verify the changes:<br />db.server.Status().connections;</p></li></ol><p>Error :</p><p>root@bac-p-manifest-mongodb-01a:/var/log/mongodb# tail -f mongod.log<br />2021-10-20T00:49:35.552+0530 I CONTROL  [initandlisten]     distarch: x86_64<br />2021-10-20T00:49:35.552+0530 I CONTROL  [initandlisten]     target_arch: x86_64<br />2021-10-20T00:49:35.552+0530 I CONTROL  [initandlisten] options: { config: &quot;/etc/mongod.conf&quot;, net: { bindIp: &quot;0.0.0.0&quot;, port: 27017 }, replication: { replSetName: &quot;rs0&quot; }, storage: { dbPath: &quot;/mnt/mongodb/mongodbdata&quot;, journal: { enabled: true } }, systemLog: { destination: &quot;file&quot;, logAppend: true, path: &quot;/var/log/mongodb/mongod.log&quot;, quiet: true } }<br />2021-10-20T00:49:35.552+0530 E NETWORK  [initandlisten] Failed to unlink socket file /tmp/mongodb-27017.sock Operation not permitted<br />2021-10-20T00:49:35.552+0530 F -        [initandlisten] Fatal Assertion 40486 at src/mongo/transport/transport_layer_asio.cpp 693<br />2021-10-20T00:49:35.552+0530 F -        [initandlisten]</p><p>Permission issue.<br />Fix: Give permission to mongodb-27017.sock to user 'mongodb'.<br />chown mongodb:mongodb mongodb-27017.sock</p>",
      "approach_used": "endpoint_2",
      "word_count": 181
    },
    {
      "id": "2249424918",
      "title": "RDS monitoring",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2249424918",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2249424918/RDS+monitoring",
      "created": "2021-10-29T05:49:16.765Z",
      "content": "<p>Monitoring our infrastructure is a key component , we have enabled monitoring and alerting of RDS using Cloudwatch.<br /></p><p>a)Types of alerts enabled </p><ol><li><p>RDS CPU utilization</p></li><li><p>RDS Low Memory</p></li><li><p>RDS High DB connections</p></li><li><p>Replica Lag </p></li></ol><p>b)We are also following name convections as below </p><ul><li><p>InstanceName-mysql-db-HighCPU</p></li><li><p>InstanceName-mysql-db-LowMem</p></li><li><p>InstanceName-mysql-Replicalag</p></li><li><p>InstanceName-mysql-db-HighConnections</p></li></ul><p>c)Follow below steps to create new alarm</p><ol><li><p>Go to Cloudwatch&nbsp;&nbsp;</p></li><li><p>Under All Alarms&nbsp;</p></li><li><p>Create new alarm</p></li><li><p>Select Metics&nbsp;</p></li><li><p>Select RDS -&gt; Per database metics&nbsp;</p></li><li><p>Select metic you want to create alert (eg: High CPU)</p></li><li><p>You will be prompted to below page , update tabs as below and click Next&nbsp;</p></li></ol><p>d)Threshold provided as of now&nbsp;</p><ul><li><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CPU&nbsp; ( &gt; 85%)</p></li><li><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Low memory ( &gt; Utilization more than 85% )</p></li><li><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;High Connections&nbsp;(Average application connection DB is using)  </p></li><li><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Replica Lag&nbsp;(&gt; 2s for 3mins)</p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"1600\" ac:original-width=\"1068\" ac:width=\"396\"><ri:attachment ri:filename=\"FsAqJQu1cNaVNFv0bQLJ7oFDo4y2aNRVtxHSld6dahLMCCCSrg2i4Sd4xEamFiVwezSHDLFOPw6wyurJ-31v8v34QBxDMtkIaYrsDE7_I87M_9pzMaKntpw8FErzrok6PmIY8aNg\" ri:version-at-save=\"1\" /></ac:image><p><br />e)Select existing SNS topic : <span style=\"color: rgb(76,154,255);\">db-prod-critical-alerts </span>to send notification </p><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"450\" ac:original-width=\"673\" ac:width=\"396\"><ri:attachment ri:filename=\"Screenshot 2021-10-29 at 11.17.05 AM.png\" ri:version-at-save=\"1\" /></ac:image><p>f)Update the alarm name and create new alert .</p><p />",
      "approach_used": "endpoint_2",
      "word_count": 148
    },
    {
      "id": "2249424935",
      "title": "Migrate DB/DBs from one MySQL RDS Community to another",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2249424935",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2249424935/Migrate+DB+DBs+from+one+MySQL+RDS+Community+to+another",
      "created": "2021-12-02T06:24:04.530Z",
      "content": "<p>Steps to Migrate DB/DBs from one MySQL RDS Community to another MySQL RDS Community Database.</p><p>Important point to make a note of before migrating:</p><ol><li><p>Go through the runbook for stopping/starting application services. Steps mentioned in the runbook will be executed by Dev Team only. </p></li><li><p>Don&rsquo;t proceed with the migration if you do not get the runbook from Application Team. </p></li><li><p>Check with the Developer team how much downtime we can have.&nbsp;</p></li><li><p>Pros and cons of downtime.&nbsp;</p></li><li><p>Figure out the loopholes that may cause data loss or increase in overall downtime.</p></li><li><p>Check the DB version.&nbsp;</p></li><li><p>Make a note of individual DB sizes which will be migrated.&nbsp;</p></li><li><p>Check with the Data Platform team if they require a slave for this Database.<br /></p></li></ol><p><strong>Prerequisite:</strong></p><ol><li><p>Create a parameter group for the new DB instance (Destination DB instance).&nbsp;</p></li><li><p>Create a slave for the instance from which the database/databases will be migrated(Source DB instance).&nbsp;</p></li><li><p>After the slave instance is available, assign the created parameter group, enable multiple AZ and set the backup retention period to 7 Days.&nbsp;</p></li><li><p>Keep an eye on replica lag. Make sure that there is no replica lag before final migration steps starts.&nbsp;</p><ol><li><p>SHOW MASTER STATUS\\G and SHOW SLAVE STATUS\\G</p></li></ol></li><li><p>Create DNS for a new endpoint (for slaves created in above steps) in Route53 and share it with the Dev team. After the upgrade, the application will point to this DNS.</p></li><li><p>Create the alerts on New instances. Follow this link for list of required alerts: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"RDS monitoring\" ri:version-at-save=\"1\" /><ac:link-body>RDS monitoring</ac:link-body></ac:link> </p></li></ol><p><br />Final steps during migration:</p><ol><li><p>Make a note of the Binary Log file and position from the master node.&nbsp;</p></li><li><p>Request application/Developers to stop the writes on master node.</p></li><li><p>Look for the application user name to have zero connection to confirm all the applications writes are stopped. </p></li><li><p>Check the replica lag on the slave and make sure there is no lag.&nbsp;</p></li><li><p>Note: We will be replicating all the databases from Master to slave at this stage.</p></li><li><p>Once lag is Zero, promote slave instances.&nbsp;</p></li><li><p>Request Dev to verify the data on new instances.</p></li><li><p>Request Dev team to connect to new DNS and enable writes.&nbsp;</p></li><li><p>Keep an eye of the process list on the DB instances.&nbsp;</p></li><li><p>Once the Dev team confirms data is intact, go to next steps.&nbsp;<br /></p></li></ol><p>Post-Migration Steps:</p><ol><li><p>Drop other databases which are not required.&nbsp;</p></li><li><p>Create a slave for the master created in above step 4.&nbsp;</p></li><li><p>Create another for the Data platform if it&rsquo;s required.&nbsp;</p></li><li><p>Scale down/up newly created instance as per the DB Size. </p></li></ol>",
      "approach_used": "endpoint_2",
      "word_count": 385
    },
    {
      "id": "2285600908",
      "title": "Table Migration for ID Autoincreement",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2285600908",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2285600908/Table+Migration+for+ID+Autoincreement",
      "created": "2021-12-06T10:10:15.552Z",
      "content": "<ol><li><p>Get the max and min ID value from table.<br />mysql&gt; select max(id) from order_detail_attributes;<br />+------------+<br />| max(id)    |<br />+------------+<br />| 1505133238 |<br />+------------+</p><p>mysql&gt; select min(id) from order_detail_attributes;<br />+---------+<br />| min(id) |<br />+---------+<br />|       1 |<br />+---------+<br />1 row in set (0.04 sec)</p></li><li><p>Check the existing schema.<br />CREATE TABLE <code>order_detail_attributes</code> (<br /><code>id</code> int(11) unsigned NOT NULL AUTO_INCREMENT,<br /><code>orderDetailId</code> int(11) NOT NULL,<br /><code>name</code> varchar(40) NOT NULL,<br /><code>attributeValue</code> json DEFAULT NULL,<br /><code>created</code> timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,<br /><code>rowLastUpdated</code> datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,<br /><code>isoCountryCode</code> varchar(10) NOT NULL DEFAULT 'IN',<br />PRIMARY KEY (<code>id</code>),<br />UNIQUE KEY <code>UC_ORDERDETAIL_NAME</code> (<code>orderDetailId</code>,<code>name</code>),<br />KEY <code>orderDetailId_idx</code> (<code>orderDetailId</code>)<br />) ENGINE=InnoDB AUTO_INCREMENT=1505134955 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci</p></li><li><p>Create new schema.<br />CREATE TABLE <code>order_detail_attributes_new</code> (<br /><code>id</code> bigint unsigned NOT NULL AUTO_INCREMENT,<br /><code>orderDetailId</code> int(11) NOT NULL,<br /><code>name</code> varchar(40) NOT NULL,<br /><code>attributeValue</code> json DEFAULT NULL,<br /><code>created</code> timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,<br /><code>rowLastUpdated</code> datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,<br /><code>isoCountryCode</code> varchar(10) NOT NULL DEFAULT 'IN',<br />PRIMARY KEY (<code>id</code>),<br />UNIQUE KEY <code>UC_ORDERDETAIL_NAME</code> (<code>orderDetailId</code>,<code>name</code>),<br />KEY <code>orderDetailId_idx</code> (<code>orderDetailId</code>)<br />) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</p></li><li><p>Will start moving in chunk of half-millions from main table to new table.<br />INSERT INTO order_detail_attributes_new select * from order_detail_attributes where ID &gt;=1 and ID &lt; 500001;</p></li><li><p>After the completion of step 4, make a note of duration. Wait for 5 min before executing next batch.</p></li><li><p>Once 85% of data migrated to new table, We will note down the max and min ID of new table and will perform the remaining data migration during final table cutover activity. Before running last batch, request app team to stop the traffic on the table.<br />INSERT INTO order_detail_attributes_new select * from order_detail_attributes where ID &gt; x; (x will value last ID migrated to new table).</p></li><li><p>From the application side, we will stop the supplier-payment-consumer application as order event consumers write data into the <code>order_detail_attributes</code> table.</p><ol><li><p>systemctl stop supplier-payment-consumer</p></li></ol></li><li><p>We will continue to monitor the graphs in the supplier-payment-web-tg  as the nodes in this target group will continue to read data from the slave DB which access the <code>order_detail_attributes</code> table. </p></li></ol><p>#NOTE: We would need detailed steps (along with commands) from Application team for starting/stopping Application using this table.#</p><ol><li><p>Get the max ID from new table (order_detail_attributes_new) and set buffer of 1 million.<br />ALTER TABLE order_detail_attributes_new AUTO_INCREMENT = x+10</p></li><li><p>Rename table old to as table_name_old. (ex: order_detail_attributes_old)<br />RENAME TABLE order_detail_attributes TO order_detail_attributes_old;<br />RENAME TABLE order_detail_attributes_new TO order_detail_attributes;</p></li><li><p>Do sanity check before handing over to application team.<br />a. Compare data based on IDs between old and new tables.</p></li><li><p>Request app team to enable the traffic.</p><ol><li><p>systemctl start supplier-payment-consumer</p></li></ol></li></ol>",
      "approach_used": "endpoint_2",
      "word_count": 400
    },
    {
      "id": "2303950962",
      "title": "Schema Creation Request",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2303950962",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2303950962/Schema+Creation+Request",
      "created": "2021-12-17T06:21:00.657Z",
      "content": "<p>Let's say we got any schema creation request from App team .</p><p>create table book(<br />id INT unsigned NOT NULL AUTO_INCREMENT,<br />title VARCHAR(100) NOT NULL,<br />author VARCHAR(40) NOT NULL,<br />status smallint(10),<br />submission_date DATETIME,<br />PRIMARY KEY ( tutorial_id )<br />);</p><ol><li><p>First of all we have too check id column . like it should be unsigned, not null and for auto increment type . For above case unsigned is missing . so we need to call this out. INT/BIGINT we'll decide according to the use case and data growth of the table.</p></li></ol><ol><li><p>Need to ask about data growth ? Like what is the expected data growth for this table and average row size we can calculate accordingly just to calculate the table size approximately which will help in Capacity Planning and also partitioning also where ever required.</p></li></ol><ol><li><p>Check for proper indexing like created_at and updated_at column must be there with proper indexing . It will help data engineering team for pulling some data from our system whenever required.</p></li></ol><ol><li><p>Ask for all possible queries which will hit this table that will give idea about indexing and other details.</p></li></ol><ol><li><p>Check for the datatypes and recommend if some small datatypes can suffice the use case . Like in above case status can be stored as Tinyint as well but we have taken smallint for same . So we can save 1 byte per call.</p></li></ol><ol><li><p>Ask for the detailed use case . like why we are creating the table try to know the app logic behind just to serve them better.</p></li></ol><ol><li><p>Check for other additional information regarding the deployment and please follow the Best practice document and ask app team to follow same.</p></li></ol><ol><li><p>After validating all these changes ask them to arrange approvals from their respective managers to take the change forward. and after that DBE will approve the the ticket from Database Side. Without approval no change will be pushed to prod.</p></li></ol><p>Same process will follow for Query review like Explain plan , possible Joins, Indexes type and if composite indexes are required. </p>",
      "approach_used": "endpoint_2",
      "word_count": 329
    },
    {
      "id": "2311422207",
      "title": "Database Provisioning",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2311422207",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2311422207/Database+Provisioning",
      "created": "2022-04-12T07:02:27.470Z",
      "content": "<ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"c329000a-553a-4618-810c-af47ef253ed8\" ac:macro-id=\"242d81c472ace2abbe31174f68b41eff\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">7</ac:parameter><ac:parameter ac:name=\"outline\">true</ac:parameter></ac:structured-macro><h2><span style=\"color: rgb(255,86,48);\">Why ??</span></h2><p>The Database Provisioning document is a starting point for your database provisioning activities. The documents details provides the information about provisioning setup, profiles, deployment procedures, and information about getting started with provisioning.</p><h2><u>Overview</u></h2><p>Provisioning involves repeatable and scheduled deployment of software, applications, or servers across different platforms, environments, and locations.</p><h2><u>PIC</u></h2><p>DBE Team.</p><h2><u>GOAL</u></h2><p>Agenda is to prepare a very detailed document that can help us in opting infrastructure you need to set up to get started with database provisioning.</p><p>We&rsquo;ll cover following topics in this document . These are mentioned below :-</p><ul><li><p>Environment : prod/sandbox</p></li><li><p>Service_tag: &lt;appln team email id&gt;</p></li><li><p>Database Type: Mysql/postgresql/</p></li><li><p>DB version : &lt;Standard&gt;</p></li><li><p>Provider Type: RDS&nbsp;</p></li><li><p>Database port: &lt;standard&gt;</p></li><li><p>Application name : &lt;application name&gt;&nbsp;</p></li><li><p>size&nbsp; = &quot;small | large | big&quot; (All Small are comes under m5 series, All Large and Big comes under r series)</p></li><li><p>Replica Required : yes /no&nbsp;</p></li><li><p>Data growth expected in next 6-12 months</p></li><li><p>Min and Max connection configuration&nbsp;</p></li><li><p>QPS &amp; TPS expected</p></li><li><p>Application Criticality (multi AZ or not)</p></li><li><p>Password Less  Authentication</p></li></ul><p /><p><strong>Environment:</strong></p><p>Describes the environment where the instance needs to be created</p><p /><p><strong>Service_tag:</strong></p><p>Email id of the support/application/tenant team&nbsp;</p><p /><p><strong>Database:</strong></p><p>Database to be chosen from various options like Mysql,postgresql,mongoDB, etc&hellip;</p><p /><p><strong>DB version:</strong></p><p>As a standard practice DB team will follow the current updated stable version&nbsp;</p><p><br /><strong>Provider Type:</strong></p><p>Can choose from different Provider options like Aurora,RDS (maybe in the future we may have GCP, Azure options as well to choose from)</p><p /><p><strong>Database port:</strong></p><p>As a standard practice, we will follow the standard port on the DB</p><p><br /><strong>Application name :</strong></p><p>Application Name/identifier</p><p><br /><strong>Size:</strong></p><p>&nbsp;&nbsp;&nbsp;&nbsp;Instances will be classified based on sizes like &quot;small | large | big&quot;&nbsp;,All Small are come under m series, All Large and Big comes under R series<br />Generally, all instances will be created with gp2 Storage, for some special request we will go with IO1/PIOPS request</p><p>Small: if the data set is less than 500GB<br />Large: If the data set between 500GB and 2TB<br />Big: If the data set is more than 2TB</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"277\" ac:original-width=\"518\" ac:width=\"544\"><ri:attachment ri:filename=\"image-20211227-111405.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Replica Required:&nbsp;</strong></p><p>Need replica instances for the same and planning to spread read queries over the replica</p><p /><p><strong>Data growth expected in next 6-12 months:</strong></p><p>Some insight on the data growth for the next 6/12 months to do capacity planning&nbsp;</p><p /><p><strong>Min and Max connection configuration:&nbsp;</strong></p><p>Total connections to be expected from the application side (idle + active)</p><p><br /><strong>QPS &amp; TPS expected:</strong></p><p>Expected QPS &amp; TPS count for every second</p><p /><p><strong>Application Criticality (multi-AZ or not):</strong></p><p>Yes/No</p><p><strong>Sample Tickets for DB instance creation and upgrading the instances</strong></p><table data-layout=\"default\" ac:local-id=\"1e0fdab8-79dc-40bd-af66-5b5d1cd07591\"><colgroup><col style=\"width: 680.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#ffffff\"><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7250\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7250</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7182\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7182</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7124\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7124</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7078\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7078</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7072\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7072</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7055\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7055</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7033\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7033</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7012\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-7012</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-6358\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-6358</a></p><p><a href=\"https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-5145\">https://meesho.atlassian.net/jira/software/projects/DEVOPS/issues/DEVOPS-5145</a></p></td></tr></tbody></table><p /><p>This is the format that we will finalize based on the shared input, And this one we will refer to create our instance</p><p><br /></p><table data-layout=\"default\" ac:local-id=\"0fb1486c-16c7-4a77-8096-33c5c9ff3eee\"><colgroup><col style=\"width: 680.0px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#b3bac5\"><p>&nbsp; instance_type &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &quot;db.r5.2xlarge&quot;</p><p>&nbsp;&nbsp;Local_replica_cnt&nbsp; &nbsp; &nbsp; = 2</p><p>&nbsp;&nbsp;size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;      &nbsp; = &quot;large&quot;</p><p>&nbsp;&nbsp;db_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;      = &quot;payment_prod&quot;</p><p>&nbsp;&nbsp;engine &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;           &nbsp; = &quot;aurora-mysql&quot;</p><p>&nbsp;&nbsp;Engine_version &nbsp; &nbsp; &nbsp;  &nbsp; = &quot;5.7.mysql_aurora.2.07.1&quot;</p><p>&nbsp;&nbsp;multi_az &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;        = true</p><p>&nbsp;&nbsp;port &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;               &nbsp; = 3306</p><p>&nbsp;&nbsp;performance_insights_enabled = true</p><p>&nbsp;&nbsp;storage_encrypted&nbsp; &nbsp; = true</p><p>&nbsp;&nbsp;jira_tag&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;     = &quot;DATAOPS-XXXX&quot;</p><p>&nbsp;&nbsp;service_tag &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  = &quot;appln team id&quot;</p><p>&nbsp;&nbsp;owner_tag&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   = &quot;DB TEAM EMAILID&quot;</p><p>&nbsp;&nbsp;tenantservice_tag&nbsp; &nbsp;  = &quot;payment-service&quot;</p></td></tr></tbody></table><p /><p><br /><br /><strong>Detailed explanation :  </strong></p><p /><p><strong>Data Size and Growth:</strong></p><p>Before creating the Instance kindly get the below required details&nbsp;</p><ol><li><p>Data grow in 3,6,12 months</p></li><li><p>Planning for data archival? If yes how regularly and what is the data size they are archiving</p></li><li><p>Make sure none of the tables w/o primary key on the schema and engine should be innodb</p></li><li><p>Mysql / postgresql version the Instance has to be the latest one(As we are using the latest versions going forward)</p></li></ol><p /><p><strong>READ &amp; WRITES per sec</strong></p><p>Understand the load for the DB</p><ol><li><p>Write and read per second count</p></li><li><p>Calculate the required IOPS for the same and choose the instance and storage accordingly</p></li></ol><p><br /><strong>Number of connections expected</strong></p><p>Get the info on max connections request expected as we observed we can see the appln team are not closing the connections once established</p><p /><p><strong>Instance Type:</strong></p><p>Once decided with Instance class please make sure below table to check its pricing&nbsp;</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"473\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"C7usHnFtu_6fwC0J1mZ_F28jLPPMkku_gUNr1rU5hS9hh7Lk_m43NslP0gvkb0SyWDQ7capZxSWyW6hCYz7CLoVMj4_k10tDuFwMoEGq05yNoqxXXmSzZJqdmRzqZAdwK6l2-WC6\" ri:version-at-save=\"1\" /></ac:image><p /><p><strong>DB backups:</strong></p><p>On Master/writer it is mandatory to enable backup for 7 days at least, on Replicas it depends</p><p /><p><strong>Prepare a separate parameter group with proper configuration</strong></p><p>Please run the below script to create the parameter group as per the DB instance</p><p>Script: To add alerts<br />Script: To Create parameter group</p><p /><p><strong>Additional Info:</strong></p><p>Please make sure to add the tag after the instance has been created</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"331\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"oCEMw9H7Hh2nddauEIausp_UG9-q32RQ6hQtQfQ39ZDHkRQekRVDca_e_KOfucdQYRuSUqdzLwkzxWRzyuvG4czMlXvqJNi8Xcu7ScpdEPCSRZf3MgTI-L1YzNEM0lWNhkKCVfYJ\" ri:version-at-save=\"1\" /></ac:image><p />",
      "approach_used": "endpoint_2",
      "word_count": 777
    },
    {
      "id": "2312372391",
      "title": "Best Practices for Redis Clusters",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2312372391",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2312372391/Best+Practices+for+Redis+Clusters",
      "created": "2021-12-29T13:58:52.163Z",
      "content": "<h2><u>Overview</u></h2><p>This document will contain all the best practices along with Do&rsquo;s and Dont&rsquo;s with Redis Cluster.</p><h2><u>PIC</u></h2><p>DBE Team.</p><h2><u>GOAL</u></h2><p>Agenda is to prepare a very detailed document which can help developers in managing Redis cluster more efficiently and in more optimised way.</p><h2><u>Recommendation and Guidelines for Spawning a New Cluster</u></h2><p>We have to follow below mentioned guidelines for spawning a new Redis cluster :-</p><h2><u>Subscriptions</u></h2><p>In Redis Cloud, a subscription is a Redis Enterprise cluster, which is multi-tenant and can have multiple databases. It is recommended to segregate production and non-production databases into separate subscriptions so as to avoid non-production usage impacting production workloads.</p><h2><u>Security</u></h2><h3><u>Establish VPC Peering</u></h3><p>Each database can have a subnet/IP Whitelist and public access can be blocked by adding the peered VPC&rsquo;s subnet to the whitelist (and additional restrictions as needed).</p><h2><u>Scaling</u></h2><p>Every database has a dataset size (max memory) limit and throughput (ops/sec) capacity. The cloud deployment will determine the number of shards required based on: one master shard for 25GB of data or 25k ops/sec (whichever is higher).</p><h2><u>Scaling Up</u></h2><p>A database can be scaled up (by increasing the memory limit and/or its ops/sec). This may result in needing to add shards and/or infrastructure.</p><p>If additional shards are needed, re-sharding the existing database will be triggered. Time taken for this will depend on the number of shards and dataset size. During this period, latency might increase and application can see timeouts as well.</p><p>If additional infrastructure is required, nodes will be added and shards may be migrated. This may result in short disconnects.</p><p><strong>Note</strong>:- <em><strong>It is recommended to scale up databases during low utilisation periods(when request count are less) periods to minimise the impact on the application.</strong></em></p><h2><u>Scaling Down</u></h2><p>A database can be scaled down (by reducing the memory limit and/or its ops/sec). If the memory limit is reduced, the new reduced limit has to be higher than current utilisation. There will be no impact on performance during this.</p><p><strong>Note</strong>:- <strong>For DBs above 1M ops/sec,  we will initially spawn cluster with 4x or 5x the required configuration so that bigger infra is spawned and then move to lower configuration where down scaling won't happen and only cost gets changed.</strong></p><h2><u>Segregation</u></h2><p>It is recommended to segregate similar capacity databases into a separate cluster. We need to split the cluster based on ops and size. Especially  for Meesho&rsquo;s usage Redis team recommended this :-</p><ul><li><p>Extra Small Subscriptions  <strong>(XS)</strong>  : For Databases of &lt;= 25k ops/sec.</p></li><li><p>Small Subscriptions <strong>(S)</strong> : For Databases &gt; 25k ops/sec and &lt; 1L ops/sec.</p></li><li><p>Medium Subscriptions <strong>(M)</strong>: For Databases &gt; 1L ops/sec and &lt; 5L ops/sec.</p></li><li><p>Large Subscriptions <strong>(L)</strong>:  For Databases &gt; 5L ops/sec and &lt; 1M ops/sec.</p></li></ul><p>Apart from these , Everything else goes as subscription of its own.</p><p>A Redis Enterprise cluster can have a maximum of 35 nodes. The cluster would add additional nodes as needed (when new databases are created or existing databases scaled up), we could grow to this limit over time.</p><h2><u>DO&rsquo;s</u></h2><ul><li><p>Think about the key-space. Is there a common feature of the key that divides your workload in a smart way (by users, by operation, by time, etc.). Use Hash tags and regular expressions to smartly divvy up the keys into hash slots.</p></li><li><p>Avoid global state under a single key that needs to be transactionally manipulated, otherwise you&rsquo;re likely to run into CROSSSLOTS errors.</p></li><li><p>Evaluate your MULTI/EXEC transactions. See if you truly need a transaction or if a pipeline will do. Don&rsquo;t forget to think about multi key commands and whether they can be replaced by multiple commands.</p></li><li><p>Always use Redis Benchmark as a Base line. Redis Benchmark provides an awesome baseline to make sure your redis server isn&rsquo;t behaving abnormally, but it should never be taken as a true &ldquo;load test&rdquo;. Load tests need to be reflective of how your application behaves, and from an environment as close to production as possible.</p></li><li><p>Set the TTL</p></li><li><p>Choose proper eviction policy.</p></li><li><p>If your data is important . Use Try/Except. Only suitable for Java Clients.</p></li></ul><h2><u>Dont&rsquo;s</u></h2><ul><li><p>Stop using Key *</p></li><li><p>Don&rsquo;t flood one instance. Whenever possible, split up the workload amongst multiple Redis instances.</p></li><li><p>More Cores = More Better . It&rsquo;s wrong. Redis is a single threaded process and will, at most, consume two cores if you have persistence enabled.</p></li></ul><p><br /></p><p><br /></p><p /><p><br /></p><p />",
      "approach_used": "endpoint_2",
      "word_count": 678
    },
    {
      "id": "2321645704",
      "title": "Enable MFA in your AWS account - Infra",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2321645704",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2321645704/Enable+MFA+in+your+AWS+account+-+Infra",
      "created": "2022-01-11T09:39:44.356Z",
      "content": "<h2>Introduction:</h2><p /><p><strong>MFA adds extra security because it requires users to provide unique authentication from an AWS supported MFA</strong>&nbsp;mechanism in addition to their regular sign-in credentials when they access AWS services.</p><h2>What to expect?</h2><p>Once you successfully&nbsp;login to your account, you will have to enable MFA for your account. Until and unless you do that you will not be able to access any of the resources granted for your account type in the AWS console. You will be getting below mentioned errors whenever you try to access the console,</p><ul><li><p>&ldquo;<span style=\"color: rgb(255,86,48);\">API Error</span>&rdquo; or &ldquo;<span style=\"color: rgb(255,86,48);\">You are not authorised to perform this operation</span>&rdquo;</p></li><li><p>Errors might look as shown in the below screenshots,</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"588\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"yt0GgK9dPVTnahVFnSe6eSNeH1QQXtp5udPhaBU6NQZ8rlkkK51Qd4qskdXAk06_nuAqocXjj45QZn3XA3WhZUJ5-sAqF5IA5uE5lM6f7mMDG-cwYYASw4SG5kst1GZSmGXUuzAp\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"600\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"IFWR75r2qzi1-b9UF2Z6vsPgVdFw1uTzpPO3Itzf7ZJZjfspoj2DITmt1DunAG28NzpI9gX-VMMNC4dl_TtccByenz7YH_tTWnPzhQ7FA6EbAkO2-VTc2bM9a17v3AwolN6p_RaA\" ri:version-at-save=\"1\" /></ac:image><h2><br /><strong>How to enable MFA for your account</strong>? </h2><p>Follow the below steps,</p><ol><li><p>You can Add MFA by navigating as shown in the below screenshot,</p></li><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"798\" ac:original-width=\"950\" ac:width=\"652\"><ri:attachment ri:filename=\"OydyZxRhO3QO6tXrnJWrTLRrOVy0Qn2qnJjcrXlnDSRk1YbKPR1JV0akzYkn5-lQviHYi9KgsnJciQ_On2cvWJtRW6QphSW0eRjGYNZeaoqFhmOWYAyIQKYYdeeKieIZ32cWiBRC\" ri:version-at-save=\"1\" /></ac:image><p>Add MFA device with any of the virtual device</p></li><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"248\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"FSl3UspsLV6LmNQjD5e63RavDvR3uheNJo2RECF5BfG_-EuKtV1JWK-3Hox_y2tlHVbyMG71yPgL7UPkveuvGLLSg3ve4LnT1LQa_qx_bS4yOsucYVUNCh9zRBvy52YT7OBvv5SD\" ri:version-at-save=\"1\" /></ac:image><p><strong>Sign-out from the console and re-login(Note: Changes won't reflect until you logout &amp; re-login)</strong></p></li><li><p>Now you will be able to access all the resources which are provided for the user groups you are added in.</p></li></ol><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"477\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"5z2uG9SEG4E7Y9wVPWz8k4zyr3kQA14-JyfYQAtlRzRgj_UlZP9U20_tDVTEOMc5FNVtGy1A2chfcyVL6P2DBf7tfncH-BD7TTupuD1FXNa0DIE26fUjHu2VISo_8dDJsUzRE9DO\" ri:version-at-save=\"1\" /></ac:image><p /><p><strong>References:</strong></p><ul><li><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html\" data-card-appearance=\"inline\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html</a> </p></li><li><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable.html\" data-card-appearance=\"inline\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable.html</a> </p></li></ul>",
      "approach_used": "endpoint_2",
      "word_count": 214
    },
    {
      "id": "2322006359",
      "title": "Enforcing MFA for all AWS users - Infra",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2322006359",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2322006359/Enforcing+MFA+for+all+AWS+users+-+Infra",
      "created": "2022-03-08T04:22:28.378Z",
      "content": "<h2>Introduction:</h2><p /><p><strong>MFA adds extra security because it requires users to provide unique authentication from an AWS supported MFA</strong>&nbsp;mechanism in addition to their regular sign-in credentials when they access AWS services.</p><p>This document will explain how to enforce any new user or existing user to enable MFA mandatory for their account without any dependency on App-Support or Devops team. </p><ul><li><p>Until and unless user enables MFA, they will not be able to access any of the resources in the console.</p></li><li><p>Common errors which user might get are<br />&ldquo;<span style=\"color: rgb(255,86,48);\">API Error</span>&rdquo; or &ldquo;<span style=\"color: rgb(255,86,48);\">You are not authorised to perform this operation</span>&rdquo; </p></li><li><p>Errors might look as shown in the below screenshots,</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"588\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"yt0GgK9dPVTnahVFnSe6eSNeH1QQXtp5udPhaBU6NQZ8rlkkK51Qd4qskdXAk06_nuAqocXjj45QZn3XA3WhZUJ5-sAqF5IA5uE5lM6f7mMDG-cwYYASw4SG5kst1GZSmGXUuzAp\" ri:version-at-save=\"1\" /></ac:image><p><br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"600\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"IFWR75r2qzi1-b9UF2Z6vsPgVdFw1uTzpPO3Itzf7ZJZjfspoj2DITmt1DunAG28NzpI9gX-VMMNC4dl_TtccByenz7YH_tTWnPzhQ7FA6EbAkO2-VTc2bM9a17v3AwolN6p_RaA\" ri:version-at-save=\"1\" /></ac:image><p /><h3><strong>Creating an Force_MFA_policy:</strong></h3><ol><li><p>Navigate to Policies tab in your IAM console</p></li><li><p>Click on &lsquo;Create Policy&rsquo; and select JSON Editor and paste the below policy json code</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"2e8fb734-0d99-45aa-aa8e-2f21470a8337\"><ac:parameter ac:name=\"language\">json</ac:parameter><ac:plain-text-body><![CDATA[{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowViewAccountInfo\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetAccountPasswordPolicy\",\n                \"iam:GetAccountSummary\",\n                \"iam:ListVirtualMFADevices\",\n                \"iam:ListMFADevices\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"AllowManageOwnPasswords\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:ChangePassword\",\n                \"iam:GetUser\",\n                \"iam:CreateLoginProfile\",\n                \"iam:DeleteLoginProfile\",\n                \"iam:GetLoginProfile\",\n                \"iam:UpdateLoginProfile\"\n            ],\n            \"Resource\": \"arn:aws:iam::*:user/${aws:username}\"\n        },\n        {\n            \"Sid\": \"AllowManageOwnAccessKeys\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreateAccessKey\",\n                \"iam:DeleteAccessKey\",\n                \"iam:ListAccessKeys\",\n                \"iam:UpdateAccessKey\"\n            ],\n            \"Resource\": \"arn:aws:iam::*:user/${aws:username}\"\n        },\n        {\n            \"Sid\": \"AllowManageOwnSigningCertificates\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:DeleteSigningCertificate\",\n                \"iam:ListSigningCertificates\",\n                \"iam:UpdateSigningCertificate\",\n                \"iam:UploadSigningCertificate\"\n            ],\n            \"Resource\": \"arn:aws:iam::*:user/${aws:username}\"\n        },\n        {\n            \"Sid\": \"AllowManageOwnSSHPublicKeys\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:DeleteSSHPublicKey\",\n                \"iam:GetSSHPublicKey\",\n                \"iam:ListSSHPublicKeys\",\n                \"iam:UpdateSSHPublicKey\",\n                \"iam:UploadSSHPublicKey\"\n            ],\n            \"Resource\": \"arn:aws:iam::*:user/${aws:username}\"\n        },\n        {\n            \"Sid\": \"AllowManageOwnGitCredentials\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreateServiceSpecificCredential\",\n                \"iam:DeleteServiceSpecificCredential\",\n                \"iam:ListServiceSpecificCredentials\",\n                \"iam:ResetServiceSpecificCredential\",\n                \"iam:UpdateServiceSpecificCredential\"\n            ],\n            \"Resource\": \"arn:aws:iam::*:user/${aws:username}\"\n        },\n        {\n            \"Sid\": \"AllowManageOwnVirtualMFADevice\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreateVirtualMFADevice\",\n                \"iam:DeleteVirtualMFADevice\"\n            ],\n            \"Resource\": \"arn:aws:iam::*:mfa/${aws:username}\"\n        },\n        {\n            \"Sid\": \"AllowManageOwnUserMFA\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:DeactivateMFADevice\",\n                \"iam:EnableMFADevice\",\n                \"iam:ListMFADevices\",\n                \"iam:ResyncMFADevice\"\n            ],\n            \"Resource\": \"arn:aws:iam::*:user/${aws:username}\"\n        },\n         {\n            \"Sid\": \"DenyAllExceptListedIfNoMFA\",\n            \"Effect\": \"Deny\",\n            \"NotAction\": [\n                \"iam:CreateVirtualMFADevice\",\n                \"iam:EnableMFADevice\",\n                \"iam:GetUser\",\n                \"iam:ListMFADevices\",\n                \"iam:ListVirtualMFADevices\",\n                \"iam:ResyncMFADevice\",\n                \"iam:CreateLoginProfile\",\n                \"iam:UpdateLoginProfile\",\n                \"sts:GetSessionToken\",\n                \"iam:ChangePassword\",\n                \"ecr:*\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"BoolIfExists\": {\n                    \"aws:MultiFactorAuthPresent\": \"false\"\n                }\n            }\n        }\n    ]\n}]]></ac:plain-text-body></ac:structured-macro></li><li><p>Next add tags are per required(OPTIONAL)</p></li><li><p>Next provide a valid name &amp; description &amp; click on &lsquo;Create Policy&rsquo;</p></li></ol><h3><strong>Creating an Users Group:</strong></h3><ol><li><p>Navigate to User Groups tab in your IAM console</p></li><li><p>Click on &lsquo;Create Group&rsquo;, provide a valid name &lsquo;Developer_Group&rsquo;</p></li><li><p>Select the users whom you want to add to this group</p></li><li><p>Select the previously created policy &lsquo;Force_MFA_Policy&rsquo; under permissions tab along with other AWS-Managed or custom policies as per your requirements</p></li></ol><h3><strong>How to assign this to new user or existing user?</strong></h3><ol><li><p>Create a new user and add him to above created group &lsquo;Developer_Group&rsquo; or just attaching the newly created policy to existing group</p></li></ol><p /><h3><span style=\"color: rgb(255,86,48);\">Should this policy be applicable for all users?</span></h3><ul><li><p>We need to segregate CLI-users and console users into different groups</p></li><li><p>CLI-Users (Programatic users) will not be logging into console and hence we need not attach policy to those groups</p></li><li><p>This should be enforced only to the console login users</p></li></ul><p /><p />",
      "approach_used": "endpoint_2",
      "word_count": 451
    },
    {
      "id": "2334261309",
      "title": "VPN Migration Plan - Infra",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2334261309",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2334261309/VPN+Migration+Plan+-+Infra",
      "created": "2022-01-21T17:02:27.204Z",
      "content": "<h2><strong>Introduction - </strong></h2><p>This doc is regarding  - </p><ul><li><p>Existing vpn setup in meesho</p></li><li><p>Problems we are facing</p></li><li><p>Solution </p></li></ul><p><strong>Existing VPN setup - </strong></p><p>For now we are using openvpn in meesho for private connections in sandbox and production accounts to connect with internal/private servers and internal/private urls. It was purchased in aws marketplace and created in september 2020. The present plan has 250 connections. </p><p><strong>Problems we are facing - </strong></p><p>Traditional VPN services are difficult to scale up, or down, in a rapid and on-demand fashion. Hardware constraints, licensing, and bandwidth can all be factors that prevent traditional client VPN services from scaling to meet the needs of a rapidly growing mobile workforce. Some of the issue we faced in meesho are listed below -</p><ul><li><p>Latency issues when many concurrent users are connected to vpn.</p></li><li><p>Difficult to increase user licenses and vertically scale the instance.</p></li><li><p>Managing EC2 instance for any security threats/attacks.</p></li></ul><p><strong>Solution - </strong></p><p>Fortunately, the elasticity of cloud and pay-as-you-go pricing of&nbsp;<a href=\"https://aws.amazon.com/vpn\">AWS Client VPN</a>&nbsp;can help.&nbsp;AWS Client VPN is a scalable and highly available OpenVPN based service that can be used to connect to both AWS and on-premises resources. </p><p><strong>Overview:</strong></p><p>AWS Client VPN is a fully managed service that provides customers with the ability to securely access AWS and on-premises resources from any location using OpenVPN based clients. Connectivity from remote end-users to AWS and on-premises resources can be facilitated by this highly available, scalable, and pay-as-you-go service. The undifferentiated heavy lifting of maintaining and running a client VPN solution is completely avoided. What&rsquo;s also unique with AWS Client VPN is the scalable nature of the service. The service will seamlessly scale to many users, without the need to acquire or manage any licenses or additional infrastructure.</p><p>AWS Client VPN supports both <ac:inline-comment-marker ac:ref=\"d2948cd3-a45b-4b3b-b894-979400334d5c\">certificate-based and Active Directory based authentication</ac:inline-comment-marker>. Customers get tighter security controls because they can define access control rules based on Active Directory groups and can use security groups to limit access of AWS Client VPN users. Using a single console, you can easily monitor and manage all of your client VPN connections. Client VPN allows you to choose from OpenVPN-based clients, including Windows, macOS, iOS, Android, and Linux based devices.</p><p><strong>How it works:</strong></p><p>AWS manages the back-end infrastructure for Client VPN. You only need to configure the service to meet your needs. The provisioning process is shown in the following architecture diagram.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"343\" ac:original-width=\"1039\"><ri:attachment ri:filename=\"client-vpn.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Client connectivity:</strong></p><p>The Client VPN endpoint attaches to one or more subnets per AZ. For high availability, at least two subnets are recommended. The attachment creates an elastic network interface (ENI) in the subnet. All of the network traffic from the client subnet is NATed (Network Address Translated) to the ENI IP address. This allows connected clients to use the subnet route table to connect to resources both inside and outside of the VPC.</p><p>Network access can be finely controlled through&nbsp;<a href=\"https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/authentication-authorization.html#client-authorization\">authorization</a>&nbsp;and is locked down by default. Clients can additionally be configured for full-tunnel access, where all network traffic goes through Client VPN, or&nbsp;<a href=\"https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/split-tunnel-vpn.html\">split-tunnel</a>.</p><p>Architecture:</p><p>Once a remote worker connects to the AWS Client VPN endpoint, they can reach any destination that the client VPN subnet has a route to (subject to an authorization allowing access). This enables access to resources within the VPC as well as resources in additional VPCs. </p><p><strong>Client VPN to many VPCs:</strong></p><p>Connected clients will already have a route to resource within the VPC. Since Client VPN is doing source NAT, client traffic has a source IP of the endpoint ENI. Resources like interface VPC endpoints and EC2 instances can connect seamlessly. VPC peering and&nbsp;<a href=\"https://aws.amazon.com/transit-gateway\">AWS Transit Gateway</a>&nbsp;can be used to connect to resources outside of the VPC. This is accomplished by adding routes to the Client VPN subnet route table. Since we already having vpc peering connections we might not required transit gateway.</p><p> This architecture in shown in the following diagram.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"502\" ac:original-width=\"967\"><ri:attachment ri:filename=\"client-many-vpc.png\" ri:version-at-save=\"1\" /></ac:image><p><span style=\"color: rgb(255,86,48);\">You can visit this </span><ac:link><ri:page ri:content-title=\"Aws Client VPN Setup - Infra\" ri:version-at-save=\"1\" /><ac:link-body>link</ac:link-body></ac:link><span style=\"color: rgb(255,86,48);\"> for Infra setup. </span> <br /><br /><strong>Conclusion:</strong></p><p>In this post I laid out several architectures that connect remote workers to VPC resources, the internet. These architectures can be combined and modified to meet your work from home requirements. AWS Client VPN makes it simple to create a scalable remote access solution to meet your work from home user needs.</p><p><br /></p><p />",
      "approach_used": "endpoint_2",
      "word_count": 714
    },
    {
      "id": "2336096257",
      "title": "Aws Client VPN Setup - Infra",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2336096257",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2336096257/Aws+Client+VPN+Setup+-+Infra",
      "created": "2022-01-18T17:39:42.289Z",
      "content": "<p />",
      "approach_used": "endpoint_2",
      "word_count": 2
    },
    {
      "id": "2340094009",
      "title": "Service Mesh",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2340094009",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2340094009/Service+Mesh",
      "created": "2022-02-07T16:29:27.253Z",
      "content": "<h3>What is a  service Mesh ?</h3><p>A service mesh, like the open source project <a href=\"https://www.redhat.com/en/topics/microservices/what-is-istio\">Istio,</a> is a way to control how different parts of an application share data with one another. Unlike other systems for managing this communication, a service mesh is a dedicated infrastructure layer built right into an app. This visible infrastructure layer can document how well (or not) different parts of an app interact, so it becomes easier to optimize communication and avoid downtime as an app grows.</p><p>There are a couple of ways of implementing a service mesh, but most often, a <a href=\"https://developers.redhat.com/blog/2017/05/31/microservices-patterns-with-envoy-sidecar-proxy-part-i-circuit-breaking/\">sidecar proxy</a> is applied to each microservice as a contact point. (The interactions among the sidecars put the <em>mesh</em> in the service mesh term.) Sidecars enable service requests to flow through the application, helping to smooth the data path among an application&rsquo;s microservices. </p><h2>Benefits of Service Mesh</h2><ul><li><p><strong>Traffic Management</strong> :  The most commonly-used features allow you to use &ldquo;destination rules&rdquo; to load balance traffic between instances of your application within your cluster using algorithms such as &ldquo;<em>round robin</em>,&rdquo; &ldquo;<em>random,&rdquo;</em> or &ldquo;<em>least requests</em>.&rdquo;&nbsp; you can extend the use of destination rules to weight traffic direction based on application versions. This can be useful in A/B testing scenarios where the request is routed based on which subnet the originating request came from. You can even route to entirely separate or external services if you want.</p></li><li><p><strong>Security</strong> : Applications can have mutual TLS security, which is often a requirement of applications running in enterprise organizations. Policies around these authentication requirements can be set at the namespace, cluster, or service level as required. These authentication policies can also be used to ensure that only specific services can to talk to each other, which, in turn, allows more sophisticated security policies to be defined and enforced.</p></li><li><p><strong>Observability</strong> : Let&rsquo;s assume you have an instance of a backend service responding slowly and creating a bottleneck in your complete stack. Requests from the frontend services will then timeout and retry to connect to the slow service instance. With the help of service mesh, you can use a circuit breaker that ensures frontend instances will only connect with healthy backend instances. Thus, using service mesh improves the visibility of your stack and helps you troubleshoot problems.</p></li><li><p><strong>Deployment Strategies : </strong>Deployment strategies (blue/green deployment, canary, etc.) are becoming the norm for releasing upgrades to cloud-native applications. Service mesh allows deployment strategies since most deployment strategies are based on diverting traffic to specific instances. For example, you can create traffic rules in service mesh so that only a small group of users (say, 10%) will be exposed to the new version.</p></li><li><p><strong>Testing</strong>: To keep your production stacks secure, it&rsquo;s best to harden them by testing delays, timeouts, and disaster recoveries.</p><p>Service mesh allows you to test its robustness by creating chaos in your system through delays and incorrect responses. For instance, by injecting delays in the service mesh traffic rules, you can test how the frontend and backend will behave when your database responds slowly to the queries from them.</p></li></ul><p /><h2>Comparison of Top Mesh Options</h2><p>While there are always a few startups with fancy service mesh products at every conference, only the three top mesh options are widely used in the cloud-native world: <a href=\"https://istio.io/\">Istio</a>, <a href=\"https://linkerd.io/\">Linkerd</a>. They are all open-source products with active communities. They also each have their own pros and cons based on their vision and implementation.</p><h3>Istio</h3><p><a href=\"https://istio.io/\">Istio</a> is a Kubernetes-native service mesh initially developed by Lyft and highly adopted in the industry. Leading cloud Kubernetes providers like Google, IBM, and Microsoft use Istio as the default service mesh in their services. Istio provides a robust set of features to create connectivity between services, including request routing, timeouts, circuit breaking, and fault injection. Additionally, Istio creates deep insights into applications with metrics such as latency, traffic, and errors.&nbsp;More details will be here <a href=\"https://meesho.atlassian.net/l/c/dUbHKTb1\" data-card-appearance=\"inline\">https://meesho.atlassian.net/l/c/dUbHKTb1</a> </p><h3>Linkerd</h3><p><a href=\"https://linkerd.io/\">Linkerd</a> is the second most popular service mesh and is part of the <a href=\"https://www.cncf.io/\">Cloud Native Computing Foundation (CNCF)</a>. More details here <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Linkerd POC\" ri:version-at-save=\"3\" /><ac:link-body>Linkerd POC</ac:link-body></ac:link> .</p><p>From an architectural point of view, Linkerd is similar to Istio but comes with more flexibility. This flexibility comes from multiple dimensions of pluggable architecture. For instance, in terms of connectivity, Linkerd works with the most popular ingress controllers, like Nginx, Traefik, or Kong. Similarly, in addition to its own GUI, it works with Grafana, Prometheus, and Jaeger for observability.&nbsp;&nbsp;</p><h2>Feature Comparison : Istio vs Linkerd</h2><table data-layout=\"default\" ac:local-id=\"98ffcc74-199d-48c3-b0aa-6233d2190cda\"><colgroup><col style=\"width: 226.67px;\" /><col style=\"width: 226.67px;\" /><col style=\"width: 226.67px;\" /></colgroup><tbody><tr><th><p /></th><th><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"157\" ac:original-width=\"300\"><ri:attachment ri:filename=\"e-x0EJ95y6asFcXv4L0chlWWAVL2fQQxoZLTAfnhOwtFo6VFi-BU42Hk9qkwc0Zntt3hSZgUAWFSi3NcWjfKW3HYC7yn0nVi24aE5CCDzYgjs6l83oy2dmZCvAY6LPQ8mQ7jylc\" ri:version-at-save=\"1\" /></ac:image></th><th><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"277\" ac:original-width=\"300\"><ri:attachment ri:filename=\"amN-rIYAVPXj3FFgHMTsZiAaJ5mfOZRFEzeqT5eMbSB88ghwbaKdtTy2zxVc6Cx4rGKuKSlql2wtE7HIRK_c7x63LepTN0nnnfKQXc22JvNYy2zMZ6VZWxu3YbyHGnxh_vSoxiM\" ri:version-at-save=\"1\" /></ac:image></th></tr><tr><td><p><strong>Supported platforms</strong></p></td><td><p>Kubernetes and VM</p></td><td><p>Kubernetes</p></td></tr><tr><td><p><strong>Supported ingress controllers</strong></p></td><td><p>Istio ingress</p></td><td><p>Any</p></td></tr><tr><td><p><strong>Traffic management features</strong></p></td><td><p>Blue/green deployment</p><p>Circuit breaking</p><p>Rate limiting</p></td><td><p>Blue/green deployment</p></td></tr><tr><td><p><strong>Prometheus and Grafana support</strong></p></td><td><p>Yes</p></td><td><p>Yes</p></td></tr><tr><td><p><strong>Chaos testing</strong></p></td><td><p>Yes</p></td><td><p>Yes</p></td></tr><tr><td><p><strong>Management complexity</strong></p></td><td><p>High</p></td><td><p>Low</p></td></tr><tr><td><p><strong>Native GUI</strong></p></td><td><p>No</p><p>Kiali is used</p></td><td><p>Yes</p></td></tr></tbody></table><p />",
      "approach_used": "endpoint_2",
      "word_count": 779
    },
    {
      "id": "2346254355",
      "title": "ArgoCD POC",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2346254355",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2346254355/ArgoCD+POC",
      "created": "2022-02-01T09:35:40.097Z",
      "content": "<p><strong>Objective:</strong> To come up with Kubernetes CICD using ArgoCD.</p><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"6100112a-5197-4d8c-8c9a-5f1ca9e86d9d\" ac:macro-id=\"5bd6e573ff2e2da93a41a53575dbc9dc\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">7</ac:parameter><ac:parameter ac:name=\"style\">circle</ac:parameter></ac:structured-macro><h2>Current Setup:</h2><h3>CI</h3><ul><li><p>Common generic jenkisnfile.</p></li><li><p>This jenkinsfile has the shell commands to build the docker image.</p></li><li><p>Steps include:</p><ul><li><p>Clone source code repo.</p></li><li><p>Extract version for tagging purpose. (currently only for java and js)</p></li><li><p>Lint Dockerfile</p></li><li><p>Build docker image.</p></li><li><p>Tag and push to ECR repo.</p></li><li><p>Send notification to slack.</p></li></ul></li></ul><h3>CD</h3><ul><li><p>Scan docker image.</p><ul><li><p>ignore scan if force_build selected.</p></li><li><p>fail the job if findings are HIGH or SEVERE</p></li></ul></li><li><p>Deploy image</p></li><li><p>Print rollout status</p></li><li><p>Send notification to slack.</p></li></ul><h2>Problems with current setup:</h2><ul><li><p>Jars being built are not being pushed to any artifactory.</p></li><li><p>No option to run tests as of now.</p></li><li><p>No integration of code analysis tools.</p></li><li><p>Output doesn&rsquo;t look appealing/good.</p></li><li><p>In CD, there is no versioning for any changes in github, changes being made directly in the deployment manifest inside the cluster. (No concept of GitOps)</p></li><li><p>Deployment rollout status is not updated properly in console output.</p></li><li><p>Sometimes, pipeline fails because of the timeout, but deployment succeeds in the background creating confusion for developers.</p></li><li><p>No option to adopt different deployment techniques like Canary, Blue/Green. (Rolling updates being used as default)</p></li><li><p>Rollback is not possible.</p></li><li><p>No clear visibility into the state of the applications in the cluster.</p></li></ul><h2>Proposed Solution:</h2><h3>DroneCI + ArgoCD</h3><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"126\" ac:original-width=\"400\" ac:width=\"224\"><ri:attachment ri:filename=\"Untitled.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"600\" ac:original-width=\"600\" ac:width=\"108\"><ri:attachment ri:filename=\"Untitled1.png\" ri:version-at-save=\"1\" /></ac:image><h2><strong>DroneCI</strong></h2><ul><li><p><a href=\"https://github.com/harness/drone\">Github</a></p></li><li><p><a href=\"https://www.drone.io/\">Website</a></p></li></ul><p>Drone is a continuous delivery system built on container technology.</p><p>Drone uses a simple YAML build file, to define and execute build pipelines inside Docker containers.</p><h3>Features</h3><ul><li><p>Configuration as a code</p></li><li><p>Pipelines are configured with a simple, easy‑to‑read&nbsp;file that you commit to your git repository.</p></li><li><p>Each Pipeline step is executed inside an isolated Docker container that is automatically downloaded at runtime.</p></li></ul><p>Example CI pipeline for a java project:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"e9bccd41-2d82-4ef3-8b06-e8c21c9a756c\" /><h2><strong>Argo CD</strong></h2><ul><li><p><a href=\"https://github.com/argoproj/argo-cd\">Github</a></p></li><li><p><a href=\"https://argo-cd.readthedocs.io/en/stable/\">Website</a></p></li></ul><p><strong>Argo CD</strong>&nbsp;is a delivery tool (CD) built for Kubernetes, based on&nbsp;<a href=\"https://www.cloudbees.com/gitops/what-is-gitops\">GitOps movement</a>.</p><p>So, what that means?</p><p>Basically that Argo CD works synchronizing &ldquo;Kubernetes files&rdquo; between a git repository and a Kubernetes cluster. That is, if there is a change in a YAML file, Argo CD will detect that there are changes and will try to apply those changes in the cluster.</p><p>Argo CD, like Drone, also creates&nbsp;<strong>its own Kubernetes custom resources</strong>&nbsp;that are installed into the Kubernetes cluster.</p><h3>Features</h3><ul><li><p>Automated deployment of applications to specified target environments</p></li><li><p>Support for multiple config management/templating tools (Kustomize, Helm, Ksonnet, Jsonnet, plain-YAML)</p></li><li><p>Ability to manage and deploy to multiple clusters</p></li><li><p>SSO Integration (OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn)</p></li><li><p>Multi-tenancy and RBAC policies for authorization</p></li><li><p>Rollback/Roll-anywhere to any application configuration committed in Git repository</p></li><li><p>Health status analysis of application resources</p></li><li><p>Automated configuration drift detection and visualization</p></li><li><p>Automated or manual syncing of applications to its desired state</p></li><li><p>Web UI which provides real-time view of application activity</p></li><li><p>CLI for automation and CI integration</p></li><li><p>Webhook integration (GitHub, BitBucket, GitLab)</p></li><li><p>PreSync, Sync, PostSync hooks to support complex application rollouts (e.g.blue/green &amp; canary upgrades)</p></li><li><p>Audit trails for application events and API calls</p></li><li><p>Prometheus metrics</p></li></ul><h3>Adoption</h3><p>Organizations who have officially adopted Argo CD can be found&nbsp;<a href=\"https://github.com/argoproj/argo-cd/blob/master/USERS.md\">here</a>.</p><h1><strong>What are we going to build?</strong></h1><p>We are going to build a CI/CD pipeline on Kubernetes, with these stages:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"241\" ac:original-width=\"1280\"><ri:attachment ri:filename=\"16430442489771.png\" ri:version-at-save=\"1\" /></ac:image><p /><ul><li><p><strong>DroneCI</strong>: to implement CI stages</p></li><li><p><strong>Argo CD</strong>: to implement CD stages (Gitops)</p></li></ul><p /><p>In this pipeline, we can see two different parts:</p><ul><li><p><strong>CI part</strong>, implemented by Drone and ending with a stage in which a push to a repository is done.</p><ul><li><p><em>Checkout</em>: in this stage, source code repository is cloned</p><p><em>Build &amp; Test</em>: in this stage, we use Maven to build and execute test</p><p><em>Code Analisys:</em>&nbsp;code is evaluated by Sonarqube</p><p><em>Publish</em>: if everything is ok, artifact is published to Nexus or jfrog</p><p><em>Build image</em>: in this stage, we build the image and publish to image registry</p><p><em>Push to GitOps repo</em>: this is the final CI stage, in which Kubernetes descriptors are cloned from the GitOps repository, they are modified in order to insert commit info and then, a push action is performed to upload changes to GitOps repository.</p></li><li><p>Last CI step can be removed if using ArgoCD image updater which can monitor image registries for any new images and trigger the CD pipeline. (<a href=\"https://argocd-image-updater.readthedocs.io/en/stable/\" data-card-appearance=\"inline\">https://argocd-image-updater.readthedocs.io/en/stable/</a>)</p></li></ul></li><li><p><strong>CD part</strong>, implemented by Argo CD, in which Argo CD detects that the repository has changed and perform the sync action against the Kubernetes cluster.</p></li></ul><h2>Canary Deployment Strategy</h2><p>A canary rollout is a deployment strategy where the operator releases a new version of their application to a small percentage of the production traffic.</p><ul><li><p>Moving to canary (ArgoCD as an example)</p></li><li><p>Defining a canary release strategy. (Tool agnostic)</p></li></ul><h3>Understanding canary releases</h3><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"524\" ac:original-width=\"1246\"><ri:attachment ri:filename=\"zBBD7ndyWd1GPq3r-VE5cvhnI3zUPalxd_Bn72UsXQDbrH1t17XolOgjMP1vmbh2im34aQB-10V_dBCAloi2fMelWdHbQrqLkiwuorf9IfEzhenMTdqR8BoHAV9kDvwnEU-SIwfe\" ri:version-at-save=\"1\" /></ac:image><h3>Here metrics provider can be:</h3><ul><li><p>Prometheus</p></li><li><p>Datadog</p></li><li><p>NewRelic</p></li><li><p>Wavefront</p></li><li><p>Cloudwatch</p></li></ul><p>Canary orchestrators can be ArgoRollouts or some other tool.</p><ul><li><p>Switch to canary rollout (after setting up ArgoRollouts)</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"445\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"ao6UPZU1vwUNfbs2fQBl37D9anC7iekr3XAiVvEfpk3dLAs67q555z3JtXmtiqgBj959nCnxhAWu81GArn-P5sze4NL5QEuoJgEqqXX3mJKhnI5weZcuOsTJDISBvzu6ouThLJHV\" ri:version-at-save=\"1\" /></ac:image><ul><li><p>Configuring a canary strategy (no. of steps, weight, and duration)</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"489\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"nXJ-1VfCxyKzC3WIkHbc0nMcaYIHSQzPG48bbmdDSSI4jRZOOf8wOBbmu5vxXmsVRYJV08yYv1yVFb63I8Il31tu-XQI4n1Pdj1J6bqgHhIXjBupFpq62NWjzgVziI62VjAXHBA6\" ri:version-at-save=\"1\" /></ac:image><ul><li><p>Configuring an analysis (using a template)</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"537\" ac:original-width=\"1181\"><ri:attachment ri:filename=\"su5P04rxmok4tzOMHTAW9KDWnEcezqKAJISABFiskDi9AKqfh-cORdK9biODJMxpKXS_Y6kexQ8L-6DOKA6WcUpENevemvo7uVsrjJzIzeGrYT2_omEPLI85oXtnPRmqURDUHyAX\" ri:version-at-save=\"1\" /></ac:image><p /><h3>Open questions (for canary deployment strategy):</h3><ol><li><p>Understanding HPA and canary strategy integration.</p></li><li><p>Visibility<br />Dev needs to know what is happening, when it is happening.</p></li><li><p>Enabling devs to interact with the canary process.</p><ol><li><p>Configure the canary analysis per case.There can be specific use cases when devs needs specific configuration.<br />Helm can be used to add this as a default strategy and devs can use values.yaml to override the strategy as needed.</p></li><li><p>Control the canary process<br />In case there is an urgent bug fix, we can't afford to wait for the canary to complete. Devs need to have the control to skip canary when needed.</p></li></ol></li></ol><h3>Open questions (for ArgoCD)</h3><ul><li><p>How many ArgoCD instances we need?</p><p><br /><strong>One Argo CD For All Clusters ?</strong></p><p>Pros:</p><ul><li><p>Single control plane</p><ul><li><p>Best developer experience.</p></li><li><p>Powerful dashboard for whole production</p></li></ul></li></ul><p>Cons:</p><ul><li><p>Single point of failure</p></li><li><p>Security concerns: external cluster access</p></li><li><p>Scalability challenges: Argo CD needs to process events of hundreds of clusters.</p></li></ul><p><br /><strong>Argo CD Per Cluster ?</strong></p><p>Pros :</p><ul><li><p>Secure - no external K8S credentials</p></li><li><p>Easier to scale - work is automatically distributed</p></li></ul><p>Cons:</p><ul><li><p>No single control plane</p></li><li><p>Management Issues</p></li></ul><p><br /><strong>One Argo CD Per BU/Org ?</strong></p><p>Pros:</p><ul><li><p>Single control plane per organization</p></li><li><p>Less security concerns</p></li><li><p>Moderate scalability challenges</p></li></ul><p>Cons:</p><ul><li><p>Management challenges: we need to manage multiple Argo CD instances<br /></p></li></ul></li><li><p>How to scale it?</p><p>It Scales Well! (picture taken from intuit&rsquo;s blog)<br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"568\" ac:original-width=\"1318\"><ri:attachment ri:filename=\"Screenshot 2022-01-25 at 4.48.44 AM.png\" ri:version-at-save=\"1\" /></ac:image><p /></li><li><p>How to secure it?<br /> <a href=\"https://argo-cd.readthedocs.io/en/release-1.8/operator-manual/security/\">Docs</a></p></li><li><p>How to monitor it?</p><ul><li><p>Argo CD exposes different sets of Prometheus metrics per server.<br /><a href=\"https://argo-cd.readthedocs.io/en/stable/operator-manual/metrics/\">Docs</a><br /><a href=\"https://grafana.apps.argoproj.io/d/LCAgc9rWz/argocd?orgId=1\">Demo dashboard</a></p></li></ul></li></ul>",
      "approach_used": "endpoint_2",
      "word_count": 993
    },
    {
      "id": "2348711995",
      "title": "HBase Table Backup And Restore Process",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2348711995",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2348711995/HBase+Table+Backup+And+Restore+Process",
      "created": "2022-01-28T05:26:58.113Z",
      "content": "<ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"67678680-76ce-427b-9ebb-4c37343e7ccd\"><ac:plain-text-body><![CDATA[\nTables:\n\ntracking:user_catalog_wishlist_map\ntracking:user_catalog_wishlist_timestamp\n\n\nCreate Snapshots:\n\nsudo hbase snapshot create -n user_catalog_wishlist_map_snapshot -t 'tracking:user_catalog_wishlist_map'\n\nsudo hbase snapshot create -n user_catalog_wishlist_timestamp_snapshot -t 'tracking:user_catalog_wishlist_timestamp'\n\n\ncopy the snapshot to s3:\n\nsudo hbase snapshot export -snapshot user_catalog_wishlist_map_snapshot \\\n-copy-to s3://hbase-perf-backup/tracking/user_catalog_wishlist_map -mappers 4\n\n\n\nsudo hbase snapshot export -snapshot user_catalog_wishlist_timestamp_snapshot \\\n-copy-to s3://hbase-perf-backup/tracking/user_catalog_wishlist_timestamp -mappers 4\n\n\n\nCreating the NameSpace in Destination EMR:\n\ncreate_namespace 'tracking'\n\n\n\n\nCreate Table in Destination EMR:\n\ncreate 'tracking:user_catalog_wishlist_map' , {NAME => 'cf'}\ncreate 'tracking:user_catalog_wishlist_timestamp' , {NAME => 'cf'}\n\n\n\n\n\nDestination EMR changes:\n\nbackup the conf:\n\ncp /etc/hbase/conf/hbase-site.xml /etc/hbase/conf/hbase-site_bkp.xml\n\n\nchange the conf:\n\nvim /etc/hbase/conf/hbase-site.xml \n\nAdd these properties to /etc/hbase/conf/hbase-site.xml: \n\n\n<property>\n\t<name>hbase.master.hfilecleaner.ttl</name>\n\t<value>7200000</value>\n</property>\n\n\n<property>\n\t<name>snapshot.export.skip.tmp</name>\n\t<value>true</value>\n</property>\n\n\n\nOnce changes were done, restart the hadoop\n\nsudo stop hbase-master\nsudo start hbase-master\n\n\nCopy snapshot from S3 to destination EMR:\n\nsudo -u hbase hbase snapshot export \\\n-D hbase.rootdir=s3://mesh-hbase-test/tracking/user_catalog_wishlist_map/ \\\n-snapshot user_catalog_wishlist_map_snapshot \\\n-copy-to hdfs://ec2-13-212-104-83.ap-southeast-1.compute.amazonaws.com:8020/user/hbase \\\n-mappers 16\n\n\nsudo -u hbase hbase snapshot export \\\n-D hbase.rootdir=s3://mesh-hbase-test/tracking/user_catalog_wishlist_timestamp/ \\\n-snapshot user_catalog_wishlist_timestamp_snapshot \\\n-copy-to hdfs://ec2-13-212-104-83.ap-southeast-1.compute.amazonaws.com:8020/user/hbase \\\n-mappers 16\n\n\nDisable The Table in Destination EMR:\ndisable 'tracking:user_catalog_wishlist_map'\ndisable 'tracking:user_catalog_wishlist_timestamp'\n\n\n\nRestore the Snapshot in Destination EMR:\nrestore_snapshot 'user_catalog_wishlist_map_snapshot'\nrestore_snapshot 'user_catalog_wishlist_timestamp_snapshot'\n\n\nOnce the restore is done, we want to revert the changes on\n/etc/hbase/conf/hbase-site.xml\nPlease do the changes and restart the Destination HBase again.]]></ac:plain-text-body></ac:structured-macro><p />",
      "approach_used": "endpoint_2",
      "word_count": 204
    },
    {
      "id": "2351038642",
      "title": "Continuous Delivery (CD)",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2351038642",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2351038642/Continuous+Delivery+CD",
      "created": "2022-02-09T08:14:10.736Z",
      "content": "<p /><p /><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"371\" ac:original-width=\"211\" ac:width=\"340\"><ri:attachment ri:filename=\"CD-new.drawio-20220131-112354.png\" ri:version-at-save=\"1\" /></ac:image><p /><ul><li><p><strong>Deployment Flow</strong></p><ul><li><p>Get the revision version/tag as  input from the user</p></li><li><p>Check the existence of revision in the Docker registry  </p></li><li><p>Deploy through  Helm</p></li><li><p>Kube controller/scheduler will automatically rollout the pods after probs  passed in one of the pod</p></li><li><p>Slack notifications about success/failure  of deployment</p></li><li><p>Only new releases will follow standard deployment strategies(Canary/Blue-green deployment), Rolling back to previous versions and Config update will follow rollout deployment strategy </p></li></ul></li><li><p><strong>Deployment Tools</strong></p><ul><li><p>CD supporting tools</p><ul><li><p>Helm(Package manager)</p><ul><li><p>Separate Helm Chart for Each application but same across environment </p></li><li><p>Environment specific properties will be maintained outside of the helm chart</p></li><li><p>Based on the environment will define a number of values.yaml</p><p /></li></ul></li></ul></li><li><p><strong>Deployment strategy </strong></p><ul><li><p>Canary Deployment </p></li><li><p>Blue/Green Deployment</p></li><li><p>Rollout Deployment</p></li></ul></li></ul></li></ul><p /><p><strong>Canary Deployment </strong></p><p>A canary rollout is a deployment strategy where the operator releases a new version of their application to a small percentage of the production traffic.</p><ol><li><p>Moving to canary (ArgoCD as an example)</p></li><li><p>Defining a canary release strategy. (Tool agnostic)</p></li></ol><p>Understanding canary releases</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"524\" ac:original-width=\"1246\"><ri:attachment ri:filename=\"I9QEsYgL9p_Sz6E32i298HBbVZ6563uBatoJMBapgodC1VEHFsVYfeGPLF7otNxw0yPOv-FXy44tgfhLh9TgchCK7oEC-8IcU3cF_vUP_-Ftl_7p9hzpmKWoT4h8W8k4RXwjMm6C\" ri:version-at-save=\"1\" /></ac:image><p>Here metrics provider can be:</p><ul><li><p>Prometheus</p></li><li><p>Datadog</p></li><li><p>NewRelic</p></li><li><p>Wavefront</p></li><li><p>Cloudwatch<br /><br />Configuring a canary strategy (no. of steps, weight, and duration)</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"489\" ac:original-width=\"1600\"><ri:attachment ri:filename=\"nXJ-1VfCxyKzC3WIkHbc0nMcaYIHSQzPG48bbmdDSSI4jRZOOf8wOBbmu5vxXmsVRYJV08yYv1yVFb63I8Il31tu-XQI4n1Pdj1J6bqgHhIXjBupFpq62NWjzgVziI62VjAXHBA6\" ri:version-at-save=\"1\" /></ac:image><ul><li><p>Configuring an analysis (using a template)</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"537\" ac:original-width=\"1181\"><ri:attachment ri:filename=\"su5P04rxmok4tzOMHTAW9KDWnEcezqKAJISABFiskDi9AKqfh-cORdK9biODJMxpKXS_Y6kexQ8L-6DOKA6WcUpENevemvo7uVsrjJzIzeGrYT2_omEPLI85oXtnPRmqURDUHyAX\" ri:version-at-save=\"1\" /></ac:image><p /><p><strong>Rollout Deployment</strong></p><p>Replacing pods of the previous version of the application with pods of the new version without any cluster downtime in a sequential manner.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"478\" ac:original-width=\"716\"><ri:attachment ri:filename=\"rolling-deployment-ww-20220201-060328.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Blue/Green Deployment</strong></p><p>The old version of the application (green) and the new version (blue) get deployed at the same time. When both of these are deployed, users only have access to the green; whereas, the blue is available for any last moment sanity checks on a separate service or via direct port-forwarding</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"488\" ac:original-width=\"704\"><ri:attachment ri:filename=\"blue-green-deployment-ww-20220201-060534.png\" ri:version-at-save=\"1\" /></ac:image>",
      "approach_used": "endpoint_2",
      "word_count": 284
    },
    {
      "id": "2351071308",
      "title": "CICD Revamping",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2351071308",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2351071308/CICD+Revamping",
      "created": "2022-02-01T07:48:01.891Z",
      "content": "<p /><h1><u>Current Setup</u></h1><p><br />We use Jenkins for CICD. We have a master-slave Jenkins setup; the master runs on a single ec2 instance. We have multiple slaves; we use one slave for CI jobs and another for CD jobs.&nbsp;</p><p>We have a generic groovy script for CI (for all languages).</p><p>In the CI process, we have the following stages:&nbsp;</p><ol><li><p>We are cloning code.</p></li><li><p>We are building code</p></li><li><p>We are deploying the generated artifact to the Jfrog repository.</p></li><li><p>We are releasing artifacts.</p></li></ol><p /><p>We have the following stages in the CD process: We use Ansible to deploy any application.</p><ol><li><p>We are validating job parameters.</p></li><li><p>We are disabling auto-scaling in ASG.</p></li><li><p>Checking ASG's Instances Lifecycle before Deploying.</p></li><li><p>Deploy pilot instance with new code.</p></li><li><p>We are waiting for user input for further process.</p></li><li><p>If required, we are rolling back.</p></li><li><p>If user input proceeds, then in the next stage, we are deploying code to other instances in ASG.</p></li><li><p>We are enabling auto-scaling.</p></li></ol><h1><br />Problems with the current setup:</h1><p>We will be listing problems with the current setup. It will be in two parts; one will be from the Jenkins infra side and another from the Jenkins jobs/pipeline side.&nbsp;</p><h2>Jenkins Infra:&nbsp;</h2><ol><li><p>We have do not have job distribution in multiple slaves, and it is hard to change because we have hardcoded.&nbsp;</p></li><li><p>We do not have a backup for our Jenkins.</p></li><li><p>We do not have a separate EBS volume for Jenkins.</p></li><li><p>We do not have an auto-scale feature based on load/Jobs.&nbsp;</p></li></ol><h2>Jenkins Job/pipeline:</h2><ol><li><p>We have generic pipeline scripts for CI for all languages; it is hard to change if we want to change a particular job.&nbsp;</p></li><li><p>We are using only the rolling update strategy.</p></li><li><p>We currently have NR agents in some ASG, so it is hard to use other deployment strategies.&nbsp;</p></li><li><p>We are using Pilot deployment that's it is hard to use other deployment strategies.&nbsp;</p></li><li><p>We do not do CI resource monitoring.&nbsp;</p></li><li><p>We have an AMI baking process in CD; we face scaling issues in one of the cases.&nbsp;</p></li><li><p>Pipeline/Jobs creation is manual for GUI.&nbsp;</p></li><li><p>We do not have a mechanism to run a security scan.&nbsp;</p></li></ol><h1>Milestone and success criteria:&nbsp;</h1><p>We will list milestones that we wanted in our CICD process.</p><h2>Infra:</h2><ol><li><p>Our CICD infra should auto-scale based on job/pipeline load.&nbsp;</p></li><li><p>Our CICD infra should containerize. It will help us upgrade to a newer version faster and with fast scaling.&nbsp;</p></li><li><p>Our CICD infra should be automated to bring it up quickly.&nbsp;</p></li><li><p>We should have resource monitoring.&nbsp;</p></li><li><p>Jenkins should be in a separate EBS volume.&nbsp;</p></li></ol><h2>Jobs/Pipeline:</h2><ol><li><p>We should create CICD jobs/pipelines automatically.&nbsp;</p></li><li><p>We should use the GitOps strategy.&nbsp;</p></li><li><p>We should have mechanisms to run our QA jobs/pipeline.</p></li><li><p>We should have modularized stages; It will give more flexibility in terms of pipeline maintenance.</p></li></ol><h2>Tools:</h2><ol><li><p>Our CD tool should support different deployment strategies like blue/green, canary, rolling updates.</p></li><li><p>Our CI tools should integrate with other tools like Sonarqube and custom QA tools.&nbsp;</p></li><li><p>Our CD tools should support both infra/application deployment.&nbsp;</p></li></ol><h1>Solution Identification:</h1><p /><p>For CI:&nbsp;<a href=\"https://meesho.atlassian.net/l/c/tK8BxQ1g\" data-card-appearance=\"inline\">https://meesho.atlassian.net/l/c/tK8BxQ1g</a> </p><p>For Kubernetes CD: <a href=\"https://meesho.atlassian.net/l/c/CysLBHTJ\" data-card-appearance=\"inline\">https://meesho.atlassian.net/l/c/CysLBHTJ</a> </p>",
      "approach_used": "endpoint_2",
      "word_count": 454
    },
    {
      "id": "2351431723",
      "title": "Continuous Integration (CI)",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2351431723",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2351431723/Continuous+Integration+CI",
      "created": "2022-02-09T08:13:52.974Z",
      "content": "<ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"784\" ac:original-width=\"738\"><ri:attachment ri:filename=\"Continuous Integration Process Flow (1).png\" ri:version-at-save=\"1\" /></ac:image><p /><p><strong>PR trigger Automation:</strong> Please refer to Pipeline as code using Github Organisation and multibranch pipeline -&gt; <a href=\"https://docs.cloudbees.com/docs/admin-resources/latest/pipelines/pipeline-as-code\" data-card-appearance=\"inline\">https://docs.cloudbees.com/docs/admin-resources/latest/pipelines/pipeline-as-code</a> </p><p /><p>As part of the CI process, we are automating the building and testing process with necessary linting and security scans, the results of these process will be mentioned in the Pull requests to make the Peer code review more agile and reliable. The process will remain the same for both containerised and non-containerised application. Only tools and modules will differ with respect to different applications/repositories. The description of stages are as below:</p><p style=\"margin-left: 30.0px;\"><strong>Linting and static code analysis: </strong>This stage of the pipeline ensure that there aren&rsquo;t any linting/styling errors present in the code. Static code analysis tools provide us with the functionality to identifying the bugs without building it. This helps ensuring that the code is upto standards even in the absence of unit tests.</p><p style=\"margin-left: 30.0px;\"><strong>Build and run unit tests: </strong>This stage builds the source code and if present runs the unit tests on the build artificats. This stage helps ensuring that the source code is building without any error and unit tests if present are passing.<br /><br /><strong>Code Coverage Report generation: </strong>This stage will generate the code coverage report on the basis of ran unit test. The reviewer can take an informed action on the Pull request using this report.<br /><br /><strong>Security scan: </strong>This stage ensure that there aren&rsquo;t any security gaps in the source code/ build artifacts.</p><p style=\"margin-left: 30.0px;\"><strong>Push artifacts to s3/artifactory:</strong> if the source code is built from the master branch(the branch that needs to be deployed to production) then the artifacts are pushed to s3 or appropriate artifactory.</p><p> Any failure in above stages will be notified to the developer.</p><p />",
      "approach_used": "endpoint_2",
      "word_count": 298
    },
    {
      "id": "2351988786",
      "title": "MongoDB - Access and Health Checks",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2351988786",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2351988786/MongoDB+-+Access+and+Health+Checks",
      "created": "2022-02-01T13:17:07.230Z",
      "content": "<p>MongoDB Prod Inventory : <a href=\"https://docs.google.com/spreadsheets/d/1_CWqBEMbkzhw_gJILLPR64owp6j9_y9NPiM1_vLZsv0/edit#gid=2026452155\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1_CWqBEMbkzhw_gJILLPR64owp6j9_y9NPiM1_vLZsv0/edit#gid=2026452155</a> </p><ol><li><p>Login to Prod Jumphost</p></li><li><p>Ssh to Mongo Node. : <em>ssh -i ~/.ssh/m-key-pair-singapore.pem <a href=\"mailto:ubuntu@172.31.22.241\">ubuntu@172.31.22.241</a></em></p></li><li><p>Type: Mongo<br />ubuntu@bac-p-mongo-invoice-01a:~$ <em>mongo</em><br />MongoDB shell version v4.0.6<br />rs0:PRIMARY&gt;</p></li></ol><p>Here:<br />rs0 - is replication name<br />PRIMARY - It means you are currenlty logges in to Primary Node of Mongo.</p><ol><li><p>Run below command to check current status of Replica Set.<br />rs0:PRIMARY&gt; <em>rs.status();</em></p></li><li><p>rs.conf() : To replica Set Configuration like Priority, votes etc.</p></li><li><p>Basic Health Checks:<br />rs0:PRIMARY&gt; <em>show dbs;</em><br />admin              0.000GB<br />config             0.000GB<br />exampleDB          0.000GB<br />invoice_supply  1476.976GB<br />local             10.576GB<br />rs0:PRIMARY&gt; <em>use invoice_supply;</em><br />switched to db invoice_supply<br />rs0:PRIMARY&gt; <em>show collections;</em><br />country_configs<br />creditInvoices_new<br />invoiceUrls_new<br />purchaseOrderInvoices<br />resellerToCustomerInvoices_new<br />supplierToResellerInvoices_new<br />userInvoiceDocuments</p></li></ol>",
      "approach_used": "endpoint_2",
      "word_count": 99
    },
    {
      "id": "2355232952",
      "title": "Meeting Notes",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2355232952",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2355232952/Meeting+Notes",
      "created": "2022-02-02T08:03:55.107Z",
      "content": "<ac:structured-macro ac:name=\"children\" ac:schema-version=\"2\" data-layout=\"default\" ac:local-id=\"439f9cec-cb5e-40e1-8dea-04a418db7e0c\" ac:macro-id=\"f1d0836c-72f6-498e-a86c-193127611660\" />",
      "approach_used": "endpoint_2",
      "word_count": 7
    },
    {
      "id": "2355888129",
      "title": "2022-02-02 Meeting notes",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2355888129",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2355888129/2022-02-02+Meeting+notes",
      "created": "2022-02-02T13:59:30.900Z",
      "content": "<h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":calendar_spiral:\" ac:emoji-id=\"1f5d3\" ac:emoji-fallback=\"\\uD83D\\uDDD3\" />&nbsp;Date</h2><p><time datetime=\"2022-02-02\" /></p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":busts_in_silhouette:\" ac:emoji-id=\"1f465\" ac:emoji-fallback=\"\\uD83D\\uDC65\" />&nbsp;Participants</h2><p><ac:placeholder>List meeting participants using their @ mention names</ac:placeholder></p><ul><li><p><ac:link><ri:user ri:userkey=\"8a7f808a7c4a60df017c4bd26baa0439\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7a5ca3fb017a600088bb05d0\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7812f8a5017817483600022a\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a78f6f818017901a6e1fb05a7\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7d2b9e4d017d31a8790b0608\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7db3842c017db6eae1330cce\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7de359a0017de5bf87ab0055\" /></ac:link> </p></li><li><p><ac:link><ri:user ri:userkey=\"8a7f808a7acf4419017ad2a9cd68018a\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7e59731e017e66aa5b6a02c8\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7c7ad20e017c7aed07b60006\" /></ac:link> </p></li><li><p><ac:link><ri:user ri:userkey=\"8a7f808668e839f30168f0218118019d\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7c148ed5017c174adb2c0f19\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808565cb41b70165cdabac17002c\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808971341f4c01713557a33c01cd\" /></ac:link> </p></li></ul><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":goal:\" ac:emoji-id=\"1f945\" ac:emoji-fallback=\"\\uD83E\\uDD45\" />&nbsp;Goals</h2><p><ac:placeholder>List goals for this meeting (e.g., Set design priorities for FY19)</ac:placeholder></p><ul><li><p>Discuss the proposed CICD process </p></li></ul><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":speaking_head:\" ac:emoji-id=\"1f5e3\" ac:emoji-fallback=\"\\uD83D\\uDDE3\" />&nbsp;Discussion topics</h2><table data-layout=\"default\" ac:local-id=\"0ecc084e-636d-49f4-8d48-47f578081022\"><colgroup><col style=\"width: 120.0px;\" /><col style=\"width: 127.0px;\" /><col style=\"width: 392.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"#deebff\"><p><strong>Item</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Presenter</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Notes</strong></p></th></tr><tr><td><p>CICD Revamping</p></td><td><p><ac:link><ri:user ri:userkey=\"8a7f808a7c4a60df017c4bd26baa0439\" /></ac:link> </p></td><td><ul><li><p>Presented flow and issues in current setup of CICD</p></li><li><p>Proposed changes and improvements </p></li><li><p>Exploring the different tools for CICD</p></li><li><p>Developer side the current problem is scale and speed; Time taken by CD</p></li><li><p>Proposed rolling based deployment</p></li><li><p>Add AMI creation process in CI </p></li></ul></td></tr><tr><td><p><br />CI Process</p></td><td><p><ac:link><ri:user ri:userkey=\"8a7f808a7de359a0017de5bf87ab0055\" /></ac:link> </p></td><td><ul><li><p>Proposed CI process</p></li><li><p>Code coverage is not a blocker but can be configured based on repo/application</p></li><li><p>Define set of standards across organisation for linting, static code analysis and code coverage tool configuration<br /></p></li></ul></td></tr></tbody></table><p /><p>Recording of the session: <a href=\"https://drive.google.com/file/d/17mMf1_JlTRUuNLGIhNWv6TuRQjQGbBhd/view?usp=sharing\" data-card-appearance=\"inline\">https://drive.google.com/file/d/17mMf1_JlTRUuNLGIhNWv6TuRQjQGbBhd/view?usp=sharing</a> </p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":white_check_mark:\" ac:emoji-id=\"2705\" ac:emoji-fallback=\"✅\" />&nbsp;Action items</h2><p><ac:placeholder>Add action items to close the loop on open questions or discussion topics:</ac:placeholder></p><ac:task-list>\n<ac:task>\n<ac:task-id>51</ac:task-id>\n<ac:task-status>incomplete</ac:task-status>\n\n</ac:task>\n</ac:task-list><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":arrow_heading_up:\" ac:emoji-id=\"2934\" ac:emoji-fallback=\"⤴\" />&nbsp;Decisions</h2><p><ac:placeholder>Type /decision to record the decisions you make in this meeting:</ac:placeholder></p><ac:adf-extension><ac:adf-node type=\"decision-list\"><ac:adf-attribute key=\"local-id\">0c8f2d9a-97fb-452d-aefb-c45334f56e8f</ac:adf-attribute><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-attribute key=\"local-id\">cf9706fc-11d8-4a9f-855d-fd1bc938b0e8</ac:adf-attribute></ac:adf-node></ac:adf-node></ac:adf-extension>",
      "approach_used": "endpoint_2",
      "word_count": 230
    },
    {
      "id": "2356019203",
      "title": "Meeting notes (2)",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2356019203",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2356019203/Meeting+notes+2",
      "created": "2022-02-02T08:49:43.640Z",
      "content": "<p style=\"text-align: right;\"><ac:macro ac:name=\"create-from-template\"><ac:parameter ac:name=\"contentBlueprintId\">5ade3f07-0d6a-4eca-93b7-4be0fe5a9d07</ac:parameter><ac:parameter ac:name=\"blueprintModuleCompleteKey\">com.atlassian.confluence.plugins.confluence-business-blueprints:meeting-notes-blueprint</ac:parameter><ac:parameter ac:name=\"createButtonLabel\">Create meeting note</ac:parameter></ac:macro></p><h2>Incomplete tasks from meetings</h2><p><ac:macro ac:name=\"tasks-report-macro\"><ac:parameter ac:name=\"spaces\">DEVOPS</ac:parameter><ac:parameter ac:name=\"pageSize\">10</ac:parameter><ac:parameter ac:name=\"spaceAndPage\">space:DEVOPS</ac:parameter><ac:parameter ac:name=\"status\">incomplete</ac:parameter><ac:parameter ac:name=\"labels\">meeting-notes</ac:parameter></ac:macro></p><h2>Decisions from meetings</h2><p><ac:macro ac:name=\"decisionreport\"><ac:parameter ac:name=\"cql\">space = &quot;DEVOPS&quot; and label = &quot;meeting-notes&quot;</ac:parameter></ac:macro></p><h2>All meeting notes</h2><p><ac:macro ac:name=\"content-report-table\"><ac:parameter ac:name=\"contentBlueprintId\">5ade3f07-0d6a-4eca-93b7-4be0fe5a9d07</ac:parameter><ac:parameter ac:name=\"blueprintModuleCompleteKey\">com.atlassian.confluence.plugins.confluence-business-blueprints:meeting-notes-blueprint</ac:parameter><ac:parameter ac:name=\"analyticsKey\">meeting-notes</ac:parameter><ac:parameter ac:name=\"blankDescription\">Set meeting agendas, take notes, and share action items with your team.</ac:parameter><ac:parameter ac:name=\"blankTitle\">Meeting notes</ac:parameter><ac:parameter ac:name=\"spaces\">DEVOPS</ac:parameter><ac:parameter ac:name=\"createButtonLabel\">Create meeting note</ac:parameter><ac:parameter ac:name=\"labels\">meeting-notes</ac:parameter></ac:macro></p>",
      "approach_used": "endpoint_2",
      "word_count": 53
    },
    {
      "id": "2356576328",
      "title": "k8s secrets management",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2356576328",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2356576328/k8s+secrets+management",
      "created": "2023-03-20T22:08:27.373Z",
      "content": "<h3>Objective:</h3><p>To come up with a secrets management solution in a Kubernetes based environment.</p><h2>Current Setup:</h2><h3>Non-k8s env:</h3><p>In-house tool to manage config and secrets for the applications at one place</p><p><a href=\"https://configs-prod.meesho.com/\">https://configs-prod.meesho.com/</a></p><p>Flow:</p><ol start=\"1\"><li><p>Create env file with the service name inside /etc/sysconfig folder when onboarding a new service.</p></li><li><p>Config creation:</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"e65b5e3b-fd14-4f66-aa73-1ff34272c813\"><ac:plain-text-body><![CDATA[$ sudo kms -p\nPlease enter type of env (stage or prod)\nprod\nPlease enter application name. eg: storefront or ums\nmessaging-api-perf\n\ncopying env file\nfetching kms_id\nencrypting env file\npushing encrypted file to s3]]></ac:plain-text-body></ac:structured-macro><ol start=\"1\"><li><p>Access is provided to config to devs from the Web UI.</p></li><li><p>Flow in CICD:</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"adf1eeee-a261-4b2c-bbea-d3a00a5929d5\"><ac:plain-text-body><![CDATA[TASK [pre_check : Check if config file exists in S3]\nTASK [deployment : Download latest properties]]]></ac:plain-text-body></ac:structured-macro><h3>K8S env:</h3><h3><strong>Kubernetes External Secrets</strong></h3><p>Kubernetes External Secrets allows you to use external secret management systems, like&nbsp;<a href=\"https://aws.amazon.com/secrets-manager/\">AWS Secrets Manager</a>&nbsp;or&nbsp;<a href=\"https://www.vaultproject.io/\">HashiCorp Vault</a>, to securely add secrets in Kubernetes.</p><p /><p>Flow:</p><ol start=\"1\"><li><p>create secrets via k8s manifest file.</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"ccdeab30-ebbd-4454-8a1a-b3a6e07faeff\"><ac:plain-text-body><![CDATA[apiVersion: \"kubernetes-client.io/v1\"\nkind: ExternalSecret\nmetadata:\n  name: notification-store-env\n  namespace: bac-p-notification-store\nspec:\n  backendType: secretsManager\n  dataFrom:\n    - notification-store-secrets\n]]></ac:plain-text-body></ac:structured-macro><p>b. Reference the created secrets in deployment manifest under spec:containers.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"fca74d0d-8cbe-495a-9801-bf1b724dfda2\"><ac:plain-text-body><![CDATA[containers:\n      - name: notification-store\n        image: 898247906677.dkr.ecr.ap-southeast-1.amazonaws.com/backend/notification-store:distroless\n        envFrom:\n        - secretRef:\n            name: notification-store-env\n]]></ac:plain-text-body></ac:structured-macro><h3>Pros:</h3><ol start=\"1\"><li><p>supports multiple backendtype for storing secrets.</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"2b03c094-8fc6-47ab-951a-28474da8c4d4\"><ac:plain-text-body><![CDATA[AWS Secrets Manager\nAWS SSM Parameter Store\nAkeyless Vault\nHashicorp Vault\nAzure Key Vault\nGCP Secret Manager]]></ac:plain-text-body></ac:structured-macro><ol start=\"1\"><li><p>Audit trails supported via backend secrets store trail logs. For eg. cloudtrail for AWS Secrets Manager.</p></li></ol><h3>Cons:</h3><ol start=\"1\"><li><p>No hot reload.</p></li></ol><h3>Open Options:</h3><ol start=\"1\"><li><p>k8s external secrets</p></li><li><p>Hashicorp vault</p></li><li><p>Mozilla sops.</p></li></ol><h3>k8s external secrets</h3><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"0fe4c62c-ebca-4eae-8932-156fc28afd00\"><ac:plain-text-body><![CDATA[github: https://github.com/external-secrets/kubernetes-external-secrets]]></ac:plain-text-body></ac:structured-macro><p>Kubernetes External Secrets allows you to use external secret management systems, like&nbsp;<a href=\"https://aws.amazon.com/secrets-manager/\">AWS Secrets Manager</a>&nbsp;or&nbsp;<a href=\"https://www.vaultproject.io/\">HashiCorp Vault</a>, to securely add secrets in Kubernetes.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"766\" ac:original-width=\"1990\"><ri:attachment ri:filename=\"Untitled111.png\" ri:version-at-save=\"1\" /></ac:image><p /><h3>Pros:</h3><ol start=\"1\"><li><p>supports multiple backendtype for storing secrets.</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"17b862ae-4048-47cf-9ac4-2109ddd50b11\"><ac:plain-text-body><![CDATA[AWS Secrets Manager\nAWS SSM Parameter Store\nAkeyless Vault\nHashicorp Vault\nAzure Key Vault\nGCP Secret Manager]]></ac:plain-text-body></ac:structured-macro><ol start=\"1\"><li><p>Audit trails supported via backend secrets store trail logs. For eg. cloudtrail for AWS Secrets Manager.</p></li><li><p>Easier to setup.</p></li><li><p>option to choose managed secret stores.</p></li><li><p>Cloud agnostic*</p></li></ol><h3>Cons:</h3><ol start=\"1\"><li><p>No hot reload. Pods needs to be restarted for new secrets to take effect.</p></li><li><p>No versioning.</p></li></ol><h3>Mozilla Sops:</h3><p><a href=\"https://github.com/mozilla/sops\" data-card-appearance=\"inline\">https://github.com/mozilla/sops</a></p><p><strong>sops</strong>&nbsp;is an editor of encrypted files that supports YAML, JSON, ENV, INI and BINARY formats and encrypts with AWS KMS, GCP KMS, Azure Key Vault, age, and PGP.</p><h3>Pros:</h3><ol start=\"1\"><li><p>Versioning via git. Secrets can be committed to source code repo.</p></li><li><p>Easier to maintain.</p></li><li><p>Supports all cloud providers (Cloud KMS)</p></li></ol><h3>Cons:</h3><ol start=\"1\"><li><p>No web UI to look at the secrets directly.</p></li><li><p>Chances of committing unencrypted secrets file to git by developers.</p></li></ol><h3>Hashicorp Vault:</h3><p><a href=\"https://github.com/hashicorp/vault\" data-card-appearance=\"inline\">https://github.com/hashicorp/vault</a> </p><p>Encryption tool of use in the management of secrets including credentials, passwords and other secrets, providing access control, audit trail, and support for multiple authentication methods. It is available open source, or under an enterprise license.</p><h3>Pros:</h3><ol start=\"1\"><li><p>Access control</p></li><li><p>Audit trail</p></li><li><p>Multiple ways for interfacing: API, CLI or the UI.</p></li><li><p>Open-source &amp; Enterprise version available (both cloud managed and self hosted) <a href=\"https://www.hashicorp.com/products/vault/pricing\" data-card-appearance=\"inline\">https://www.hashicorp.com/products/vault/pricing</a> </p></li></ol><h3>Cons:</h3><ol start=\"1\"><li><p>Complex to setup and maintain.</p></li></ol><p /><h2>Vault Audit Logging:</h2><p>In HashiCorp Vault, we can use the audit logging feature to track who modified the secrets. Audit logging allows us to log all actions taken in Vault, including read and write operations on secrets. To check who modified the secrets, we can search the audit log for the relevant actions. Here are the general steps:</p><ol start=\"1\"><li><p>Enable audit logging.</p></li><li><p>Search the audit log: Once audit logging is enabled, you can search the audit log for the relevant actions. The audit log will contain information about the user who performed the action, the time of the action, and other details. You can search the audit log using the Vault CLI or API. Here's an example using the CLI:</p></li></ol><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"f35b79cf-e979-449a-80e8-1e452fb13994\"><ac:plain-text-body><![CDATA[$ vault audit list -format=json | jq '.[] | select(.type == \"request\") | select(.request.path == \"secret/data/my-secret\")']]></ac:plain-text-body></ac:structured-macro><p>This command lists all requests in the audit log for the path &quot;secret/data/my-secret&quot;. You can modify the path to match the secret you want to search for. The output will contain information about the request, including the user who performed the action (auth.user), the time of the action (time), and other details. Note that you may need to filter the audit log further to find the specific modification you're looking for. For example, you can search for requests that included a POST method, indicating a write operation.</p><p><strong>1. </strong> What all operations can be in the audit log?</p><ul><li><p>Authentication events: login, logout, token creation and revocation, AppRole authentication</p></li><li><p>Secret engine operations: read, write, delete, list, lease management (renew, revoke, renewals)</p></li><li><p>Policy changes: creation, update, deletion</p></li><li><p>Vault administrative activities: configuration changes, unsealing, seal/unseal status changes, audit device changes, replication-related activities</p></li><li><p>Transit engine operations: encrypt, decrypt, re-wrap, sign, verify</p></li><li><p>Database engine operations: dynamic credential generation, revocation</p></li><li><p>PKI engine operations: certificate issuance, revocation, renewal</p></li></ul><p>The exact operations that are logged in the audit log will depend on the configuration of the audit device(s) and the policies in place in Vault. By default, Vault logs all actions taken by authenticated users, but you can configure more granular logging by specifying audit policies.</p><p><strong>2.</strong> What all audit devices are available in Vault?</p><p>HashiCorp Vault provides several audit devices that you can use to log all or a subset of Vault operations to various destinations. These audit devices include:</p><ul><li><p>File: This audit device writes the audit logs to a file on the Vault server or a remote file server.</p></li><li><p>Syslog: This audit device sends audit logs to a syslog server.</p></li><li><p>Socket: This audit device sends audit logs to a Unix domain socket or TCP/IP socket.</p></li><li><p>TCP: This audit device sends audit logs to a remote TCP endpoint.</p></li><li><p>UDP: This audit device sends audit logs to a remote UDP endpoint.</p></li><li><p>Kafka: This audit device sends audit logs to an Apache Kafka cluster.</p></li><li><p>RabbitMQ: This audit device sends audit logs to a RabbitMQ server.</p></li><li><p>Elasticsearch: This audit device sends audit logs to an Elasticsearch cluster.</p></li><li><p>Splunk: This audit device sends audit logs to a Splunk server.</p></li><li><p>Azure Event Hub: This audit device sends audit logs to an Azure Event Hub.</p></li><li><p>Google Cloud Pub/Sub: This audit device sends audit logs to a Google Cloud Pub/Sub topic.</p></li><li><p>AWS CloudWatch: This audit device sends audit logs to an AWS CloudWatch log group.</p></li></ul><p>Note that not all audit devices are available out of the box and some may require additional configuration. The available audit devices and their configuration options may also vary depending on the version of Vault we are using.</p><p />",
      "approach_used": "endpoint_2",
      "word_count": 1020
    },
    {
      "id": "2357198869",
      "title": "2022-02-03 Meeting notes",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2357198869",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2357198869/2022-02-03+Meeting+notes",
      "created": "2022-02-06T09:51:08.011Z",
      "content": "<h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":calendar_spiral:\" ac:emoji-id=\"1f5d3\" ac:emoji-fallback=\"\\uD83D\\uDDD3\" />&nbsp;Date</h2><p><time datetime=\"2022-02-03\" /></p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":busts_in_silhouette:\" ac:emoji-id=\"1f465\" ac:emoji-fallback=\"\\uD83D\\uDC65\" />&nbsp;Participants</h2><p><ac:placeholder>List meeting participants using their @ mention names</ac:placeholder></p><ul><li><p><ac:link><ri:user ri:userkey=\"8a7f808a7a5ca3fb017a600088bb05d0\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7c4a60df017c4bd26baa0439\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7de359a0017de5bf87ab0055\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7d2b9e4d017d31a8790b0608\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7aee2ecc017af34aceb30573\" /></ac:link>  <ac:link><ri:user ri:userkey=\"8a7f808565cb41b70165cdabac17002c\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7c148ed5017c174adb2c0f19\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808668e839f30168f0218118019d\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808971341f4c01713557a33c01cd\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7812f8a5017817483600022a\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a78f6f818017901a6e1fb05a7\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7acf4419017ad2a9cd68018a\" /></ac:link> </p></li></ul><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":goal:\" ac:emoji-id=\"1f945\" ac:emoji-fallback=\"\\uD83E\\uDD45\" />&nbsp;Goals</h2><p>Discuss proposed CD process for K8s.</p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":speaking_head:\" ac:emoji-id=\"1f5e3\" ac:emoji-fallback=\"\\uD83D\\uDDE3\" />&nbsp;Discussion topics</h2><table data-layout=\"default\" ac:local-id=\"6ebc5c2a-177e-4518-9a86-4c3cda1d1b9c\"><colgroup><col style=\"width: 120.0px;\" /><col style=\"width: 127.0px;\" /><col style=\"width: 392.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"#deebff\"><p><strong>Item</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Presenter</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Notes</strong></p></th></tr><tr><td><p>CD process</p></td><td><p><ac:link><ri:user ri:userkey=\"8a7f808a7812f8a5017817483600022a\" /></ac:link> </p></td><td><ul><li><p>Presented different Deployment strategies</p><ul><li><p>Canary Deployment</p><ul><li><p>strategy</p></li><li><p>templating for canary tests</p></li><li><p>integration with different metrics providers</p></li></ul></li><li><p>Rolling Deployment</p></li><li><p>Blue Green Deployment</p></li></ul></li></ul></td></tr></tbody></table><p /><p>Meeting recording: <a href=\"https://drive.google.com/file/d/1yFRC-FY8aYTZAhwjzLGWaWKjvMYKvzsK/view?usp=sharing\" data-card-appearance=\"inline\">https://drive.google.com/file/d/1yFRC-FY8aYTZAhwjzLGWaWKjvMYKvzsK/view?usp=sharing</a> </p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":white_check_mark:\" ac:emoji-id=\"2705\" ac:emoji-fallback=\"✅\" />&nbsp;Action items</h2><p><ac:placeholder>Add action items to close the loop on open questions or discussion topics:</ac:placeholder></p><ac:task-list>\n<ac:task>\n<ac:task-id>10</ac:task-id>\n<ac:task-status>incomplete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">If current CI/CD improvements are non-debatable, we will have to put good effort to work around the current blockers(newrelic on one instance, pilot instance for testing etc). And we will do some optimisation to it in the CI flow, but CD will remain.</span></ac:task-body>\n</ac:task>\n<ac:task>\n<ac:task-id>11</ac:task-id>\n<ac:task-status>incomplete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">We will do so by not blocking the rearchitected version(v2), but spending additional effort in March</span></ac:task-body>\n</ac:task>\n</ac:task-list><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":arrow_heading_up:\" ac:emoji-id=\"2934\" ac:emoji-fallback=\"⤴\" />&nbsp;Decisions</h2><p><ac:placeholder>Type /decision to record the decisions you make in this meeting:</ac:placeholder></p><ac:adf-extension><ac:adf-node type=\"decision-list\"><ac:adf-attribute key=\"local-id\">f3292fb6-82e5-49a8-9200-3ebf020a11b2</ac:adf-attribute><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-attribute key=\"local-id\">f52960e1-258e-44a4-9137-c2bfbeb64757</ac:adf-attribute><ac:adf-content>We will spend effort on the new CI/CD v2, which is focussed on containers</ac:adf-content></ac:adf-node><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"local-id\">52017139-a4ff-4278-8c64-5923b74e3c23</ac:adf-attribute><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-content>Timeline to complete the v2 is end of Feb</ac:adf-content></ac:adf-node><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"local-id\">e550442c-89ee-4794-93ac-cea1e8b60c43</ac:adf-attribute><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-content>We donot want to spend more time to optimise the current CI/CD process as there are many blockers in its current format(newrelic on one instance, pilot instance for testing etc) that will need much rework and time </ac:adf-content></ac:adf-node></ac:adf-node><ac:adf-fallback><ul class=\"decision-list\"><li>We will spend effort on the new CI/CD v2, which is focussed on containers</li><li>Timeline to complete the v2 is end of Feb</li><li>We donot want to spend more time to optimise the current CI/CD process as there are many blockers in its current format(newrelic on one instance, pilot instance for testing etc) that will need much rework and time </li></ul></ac:adf-fallback></ac:adf-extension>",
      "approach_used": "endpoint_2",
      "word_count": 341
    },
    {
      "id": "2358476818",
      "title": "Harness Meeting notes",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2358476818",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2358476818/Harness+Meeting+notes",
      "created": "2022-02-06T09:46:34.322Z",
      "content": "<h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":calendar_spiral:\" ac:emoji-id=\"1f5d3\" ac:emoji-fallback=\"\\uD83D\\uDDD3\" />&nbsp;Date</h2><p><time datetime=\"2022-02-04\" /></p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":busts_in_silhouette:\" ac:emoji-id=\"1f465\" ac:emoji-fallback=\"\\uD83D\\uDC65\" />&nbsp;Participants</h2><p><ac:placeholder>List meeting participants using their @ mention names</ac:placeholder></p><ul><li><p><ac:link><ri:user ri:userkey=\"8a7f808a7de359a0017de5bf87ab0055\" /></ac:link><ac:link><ri:user ri:userkey=\"8a7f808a7d2b9e4d017d31a8790b0608\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7c4a60df017c4bd26baa0439\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7812f8a5017817483600022a\" /></ac:link> </p></li><li><p><ac:placeholder>@ mention a person to add them as an attendee and they will be notified.</ac:placeholder></p></li></ul><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":goal:\" ac:emoji-id=\"1f945\" ac:emoji-fallback=\"\\uD83E\\uDD45\" />&nbsp;Goals</h2><p>Deep dive into Harness capabilities </p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":speaking_head:\" ac:emoji-id=\"1f5e3\" ac:emoji-fallback=\"\\uD83D\\uDDE3\" />&nbsp;Discussion topics</h2><table data-layout=\"default\" ac:local-id=\"13b68b8c-11f7-4086-b58e-505fb41039cd\"><colgroup><col style=\"width: 120.0px;\" /><col style=\"width: 127.0px;\" /><col style=\"width: 392.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"#deebff\"><p><strong>Item</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Presenter</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Notes</strong></p></th></tr><tr><td><p>Harness CI</p></td><td><p>Greg Kroon</p></td><td><ul><li><p>Harness delegates uses kubernetes cluster and the runners use docker containers, which are cleaned up after execution</p></li><li><p>Trigger using webhook, artifacts, schedule</p></li><li><p>UI to create the entire pipeline</p></li><li><p>Integration to slack notification for approval can be configured</p></li><li><p>Tool such as static code analysis are supported as docker images</p></li><li><p><a href=\"http://Drone.io\">Drone.io</a> for plugin integration</p></li><li><p><a href=\"https://ngdocs.harness.io/article/vtu9k1dsfa-test-intelligence-concepts\">Test intelligence</a> built in for reducing testing time </p></li><li><p>To post the results to tools such as sonarque, need to script ourselves</p></li><li><p>Comment to pull requests needs to be </p></li><li><p>CI currently don&rsquo;t support AMI backing for EC2 but can integrate the CD process with Jenkins CI</p></li><li><p>All deployment strategies available for kubernetes, functionality of using shell script, metrics can be integrated with different data sources</p></li><li><p>Secret manager available inbuilt, configuration with multiple secret manager</p></li><li><p>Templating functionality available for steps and stages for now, pipeline templating coming soon</p></li><li><p>Governance policy available to enforce code and security policies</p></li><li><p>SSO and Audit trail available</p></li><li><p>Harness delegate can be install within the cluster or outside the cluster</p></li><li><p>Best practice is to run the delegate in different namespace</p></li><li><p>Pipeline as a code available, sync available for both side</p></li><li><p>Using connectors to repo we can connect directly to git and sync, need to make sure that the yaml for pipeline is correct</p></li><li><p>AMI based deployment supports different deployments using inbuilt workflows</p></li><li><p>Doc references: <a href=\"https://docs.harness.io/\" data-card-appearance=\"inline\">https://docs.harness.io/</a> , <a href=\"https://ngdocs.harness.io/\" data-card-appearance=\"inline\">https://ngdocs.harness.io/</a>  </p></li><li><p>AMI baking process is not supported to Next gen CICD. It only support in Current Gen CD where you don&rsquo;t have CI. </p></li><li><p>PAAC is complex in Current Gen CD and not easy to maintain.</p></li></ul></td></tr></tbody></table><p /><p>Recoding link: <a href=\"https://harness-io.zoom.us/rec/share/xzrv7LZJc6WzsVnYy_wPBQLqXCKr4Gz8eOK9_ZeNXWHWgEOm-FPYc4_TRIaKfcE.dMGF3QOZa_zUfhZU\" data-card-appearance=\"inline\">https://harness-io.zoom.us/rec/share/xzrv7LZJc6WzsVnYy_wPBQLqXCKr4Gz8eOK9_ZeNXWHWgEOm-FPYc4_TRIaKfcE.dMGF3QOZa_zUfhZU</a> <br />Access Passcode: Meesho9!</p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":white_check_mark:\" ac:emoji-id=\"2705\" ac:emoji-fallback=\"✅\" />&nbsp;Action items</h2><p><ac:placeholder>Add action items to close the loop on open questions or discussion topics:</ac:placeholder></p><ac:task-list>\n<ac:task>\n<ac:task-id>51</ac:task-id>\n<ac:task-status>incomplete</ac:task-status>\n\n</ac:task>\n</ac:task-list><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":arrow_heading_up:\" ac:emoji-id=\"2934\" ac:emoji-fallback=\"⤴\" />&nbsp;Decisions</h2><p><ac:placeholder>Type /decision to record the decisions you make in this meeting:</ac:placeholder></p><ac:adf-extension><ac:adf-node type=\"decision-list\"><ac:adf-attribute key=\"local-id\">80033ad1-0286-4eb5-9d74-daa0b4d5b93a</ac:adf-attribute><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-attribute key=\"local-id\">c75be934-a7fc-4eb6-ba82-902953baeb66</ac:adf-attribute></ac:adf-node></ac:adf-node></ac:adf-extension>",
      "approach_used": "endpoint_2",
      "word_count": 371
    },
    {
      "id": "2359623710",
      "title": "Kubernetes Concept Note",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2359623710",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2359623710/Kubernetes+Concept+Note",
      "created": "2022-02-09T05:17:39.639Z",
      "content": "<ac:structured-macro ac:local-id=\"cdf083e0-964d-42a9-be21-8c52eaee9816\" ac:macro-id=\"29354b56-0692-4f3e-90f3-261778c117c1\" ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">7</ac:parameter></ac:structured-macro><h1>Current Infra Setup</h1><p>Currently, we are using the Ec2 instance to deploy our workload where we have one ASG and load balancer target group per application. In addition, we are using Jenkins for our CICD. Also, we have an Ansible playbook and running those Ansible-playbook from Jenkins to deploy any workload. To create new or modify existing infra, sometimes we use Terraform, and sometimes we create manually, including ASG, load balancer, and RDS based on the requirement.</p><h1>Problem Summary</h1><h3>Hybrid cloud:&nbsp;</h3><p>Currently, we are using AWS as our main and only cloud. However, we plan to go to GCP in the next two years, but we need to support both clouds in between. It is not easy to go to the multi-cloud with the current setup; Also, the maintenance of two clouds with the current setup will be a problem and time-consuming.</p><h3>Dev Productivity:</h3><p>With the current setup, It is not easy to increase Dev Productivity as we will be creating ASG, Load Balancer Target Group, route53 entry to access the application. The following bottleneck will be there while increasing Dev Productivity.&nbsp;</p><ol><li><p>It is hard to implement a self-serve platform in the current setup.&nbsp;</p></li><li><p>Creating new infra is time taking process.&nbsp;</p></li><li><p>Outmost of the Time is going in AMI creating stage whether we will move this stage to CI or else we keep in CD.&nbsp;</p></li><li><p>Very hard to implement other deployment strategies like Canary, blue/green in the current setup.&nbsp;</p></li><li><p>It will take weeks to create a new ENV or custom ENV.</p></li></ol><h3>Manageability:&nbsp;</h3><p>Manageability will be a bottleneck when we go to multi-cloud; it is tough to track even in our current setup. </p><h3>Scalability:</h3><p>We are currently scaling applications manually, and we have already kept the buffer more than sufficient, increasing cost&mdash;also, EC2 based infra support scaling based on CPU and memory. So if we want to scale based on custom metrics, we need to push custom metrics to cloud watch, which will increase our cost. Scaling can not be quick; it will also take time because it will try to bring up the EC2 instance and start deploying the application.&nbsp;</p><h3>Modernisation:&nbsp;</h3><p>Continued integration, deployment, and improved dev productivity will lead us to Modernize our infra. VM-based infra is old school now.&nbsp;</p><h3>Resource Optimisation and Cost:</h3><p>Currently, we have over provision our infra. Also, We do not have auto-scaling in place, result-in in most of the time, we are not even down scaling our infra after scaling for sell. Also, currently, we are using a minimum of one ASG and load balancer target group per application, which is an increased cost. Runs a complete operating system, including the kernel, thus requiring more system resources such as CPU, memory, and storage.</p><h1>Scope</h1><ol><li><p>All EC2 infra will be in scope.</p></li><li><p>Resource optimisation and cost reduction.</p></li><li><p>Moving out of the unsupported OS.</p></li><li><p>Horizontal Autoscaling for nodes and pods both.</p></li><li><p>SSO login to the cluster.</p></li><li><p><a href=\"https://meesho.atlassian.net/l/c/r01FEVnv\">Service Mesh</a>.</p></li><li><p>Complete Monitoring of Cluster and worker node and application.</p></li><li><p>Secret Management.</p></li><li><p>Our current config management will mostly work in containerisation platform, but we need to put config management in the scope if needed.&nbsp;</p></li></ol><h1>Not in Scope</h1><ol><li><p>The stateful set will not be in scopes like Kafka or any database.</p></li></ol><h1>Success Criteria</h1><ol><li><p>Container infra should be fully automated, starting from deploying infra to deploying the application.&nbsp;</p></li><li><p>Move 100% application and infra to Containerisation, which is in scope.</p></li><li><p>We should support Multi cluster communication.&nbsp;</p></li><li><p>Internal Service Communication should happen internally.</p></li><li><p>We will support time-based and custom metrics-based auto-scaling, which will optimize our resources.&nbsp;</p></li><li><p>We will be supporting the Rollback mechanism. (Thought CICD).</p></li><li><p>Should Support deployment strategies like Canary, blue-green, Rolling update. (Thought CICD)</p></li><li><p>Application onboarding, resource modification, deployment to containerisation platform should be self-serve.&nbsp;</p></li></ol><h1>Solution</h1><p>Kubernetes is one of the popular options to orchestrate containers. Kubernetes will help us in:</p><ol><li><p>Multi-cloud management.</p></li><li><p>Auto-scale based on CPU, memory, or custom metrics in a shorter time.</p></li><li><p>It will help us maintain a replica set of the application container.</p></li><li><p>It will help us to do internal service communication.&nbsp;</p></li></ol><p>Each application will be running in a separate namespace.&nbsp;To move to Kubernetes, we need to think about the following points.</p><ol><li><p>How many Kubernetes do we need? Some possible options are:</p><ol><li><p>We can have one cluster for sandbox (staging) and one for Perf testing.</p></li><li><p>For prod, we can have the following options:&nbsp;</p><ol><li><p>We can divide clusters based on priority, e.g., One cluster for P0 services, one for p1, and 1 for P2 and p3.</p></li><li><p>We can assign one cluster per BU.</p></li><li><p>We can have a fixed service count per cluster.</p></li><li><p>Also, we can have a combination of 1 and 3 or a combination of 2 and 3.&nbsp;</p></li></ol></li></ol></li><li><p>We need to decide how our multi-cluster and multi-cloud communication will happen; Istio service mesh is one option that gives us multi-cluster communication other than service mesh.</p></li><li><p>We will need an SSO login setup for the Kubernetes clusters.</p></li><li><p>We need to implement a secret management process, and also we need to decide on tooling around it. We have multiple options to use secrets management in Kubernetes:</p><ol><li><p>Vault + KMS</p></li><li><p>KMS + Config server</p></li><li><p>Mozilla-sops + KMS</p></li><li><p>KMS + secrets-store-csi-driver + aws secret manager.&nbsp;</p></li><li><p>More details will be here <a data-card-appearance=\"inline\" href=\"https://meesho.atlassian.net/l/c/pdz3FVph\">https://meesho.atlassian.net/l/c/pdz3FVph</a> .</p></li></ol></li><li><p>We needs to do templatize of our Kubernetes manifest and a deployment tool. One of the tool is helm. (Our CD tool will take help from helm to deploy the application). There are three aspects in terms of application manifests:</p><ol><li><p>Onboard a new application to Kubernetes.</p></li><li><p>Add a new Kubernetes resource to an existing application.</p></li><li><p>Modify an existing Kubernetes resource.</p></li></ol></li><li><p>Independent of the above, engineers will write their respective Kubernetes helm templates into the git repo(based on their environment and application). In addition, We will use CI tools to define a workflow. CI workflow will do the following steps broadly:&nbsp;</p><ol><li><p>Take a diff b/w cluster with the PR.&nbsp;</p></li><li><p>Comment this diff to the PR.&nbsp;</p></li><li><p>Two peers have to approve this PR.&nbsp;</p></li><li><p>It will upload this helm package in the versioned folder to the artifacts repository on merge.&nbsp;</p></li></ol></li><li><p>Furthermore, how does the CI workflow trigger? Based on a PR commit to the git repo. So, in essence, this is the comprehensive set of activities to be done:&nbsp;</p><ol><li><p>Engineer files a PR to the repo.</p></li><li><p>CI workflow triggers as above, and it will generate a diff between the cluster manifest and the current one and put the diff as comments on the PR.</p></li><li><p>A PR reviewer kicks in to review and merge the PR. (We wish to keep 2 PR reviewers here for sanity).</p></li><li><p>The CI workflow will merge the manifests to the master and upload artifacts to the artifacts repository.</p></li><li><p>Then CD pipeline will trigger. It can be automated and manual trigger.&nbsp;&nbsp;</p></li><li><p>The CD pipeline will get these artifacts and deploy them to the cluster.&nbsp;</p></li><li><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"394\" ac:original-width=\"753\"><ri:attachment ri:filename=\"cicd-CICD.jpg\" ri:version-at-save=\"1\" /></ac:image><p /></li></ol></li><li><p>Auto Scaling:</p><ol><li><p>Kubernetes pod autoscaling(commonly referred to as Horizontal Pod auto-scaling or HPA).</p></li><li><p>Kubernetes Cluster Autoscaling(needed when HPA does not have enough resources on the cluster).&nbsp;</p></li><li><p>In a containerized environment, We should be able to scale based on a variety of parameters:&nbsp;</p><ol><li><p>Time-based Scaling.</p></li><li><p>Request-based Scaling in / scaling out.</p></li><li><p>Based on custom queues like Kafka, Redis, etc.</p></li><li><p>Based on memory limits or CPU limits.</p></li></ol></li></ol></li></ol><h1>Expectation from <ac:inline-comment-marker ac:ref=\"8d89b829-6298-471f-b29a-ea44087a1683\">DevOps</ac:inline-comment-marker>:</h1><ol><li><p>DevOps will be providing a guideline to write a helm values.yaml and Dockerfile.&nbsp;</p></li><li><p>DevOps will provide a base docker image per language. </p></li><li><p>DevOps will provide a testing environment for Engineers.</p></li></ol><h1>Expectation from engineers:</h1><ol><li><p>The engineer needs to provide the priority and severity of their application.&nbsp;</p></li><li><p>It will be good if engineers have a basic understanding of Docker, Kubernetes, and helm.&nbsp;</p></li><li><p>Engineers need to write a docker file for their application to create a docker image.</p></li><li><p>Engineers need to write a helm values.yaml for their application and commit it to the git repo.</p></li><li><p>Engineer expects to provide metrics and scale details like When we want to scale and how much we want to scale.&nbsp;</p></li><li><p>The contact person will require per team/pod/OU for migration.</p></li><li><p>We will be needing QA support to test the application before going to prod.&nbsp;</p></li><li><p>Engineer needs to provide Deployment strategies like blue/green, canary, rolling updates.</p></li><li><p>The engineers need to provide performance metrics to watch during migration.&nbsp;</p></li><li><p>Engineer needs to provide the health check of their application (Liveness, readiness</p><p>, and Startup Probes).&nbsp;</p></li></ol>",
      "approach_used": "endpoint_2",
      "word_count": 1287
    },
    {
      "id": "2359656474",
      "title": "Comparison of CICD tools",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2359656474",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2359656474/Comparison+of+CICD+tools",
      "created": "2022-02-07T05:38:45.125Z",
      "content": "<p><ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Jenkins+Argo vs Harness vs Spinnaker\" ri:version-at-save=\"2\" /><ac:link-body>Jenkins+Argo vs Harness</ac:link-body></ac:link> </p><p><ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Spinnaker\" ri:version-at-save=\"1\" /><ac:link-body>Spinnaker</ac:link-body></ac:link> </p><p />",
      "approach_used": "endpoint_2",
      "word_count": 18
    },
    {
      "id": "2360508513",
      "title": "Kubernetes Multi Cluster Architecture",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2360508513",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2360508513/Kubernetes+Multi+Cluster+Architecture",
      "created": "2023-08-24T12:21:23.502Z",
      "content": "<ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"f2b5a6a0-61c5-4b4b-9841-56ef1bfb69dc\" ac:macro-id=\"12a1a89f-2aaf-48c7-87b5-cb5443865fbc\" /><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"610\" ac:original-width=\"626\" ac:width=\"680\"><ri:attachment ri:filename=\"ClusterMesh.jpg\" ri:version-at-save=\"1\" /></ac:image><p /><h2>How ClusterMesh Works</h2><ac:image ac:align=\"center\" ac:layout=\"wide\" ac:original-height=\"698\" ac:original-width=\"2030\" ac:width=\"680\"><ri:attachment ri:filename=\"image-20230809-043736.png\" ri:version-at-save=\"1\" /></ac:image><p /><h2>Cluster infra components</h2><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"541\" ac:original-width=\"591\" ac:width=\"680\"><ri:attachment ri:filename=\"EKSComponent.jpg\" ri:version-at-save=\"1\" /></ac:image><p /><h2>Service To Service Communication inside cluster</h2><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"391\" ac:original-width=\"501\" ac:width=\"680\"><ri:attachment ri:filename=\"svctosvcsamecluster.jpg\" ri:version-at-save=\"1\" /></ac:image><h2>Service To Service communication b/w Cluster</h2><p /><ac:image ac:align=\"center\" ac:layout=\"wide\" ac:original-height=\"461\" ac:original-width=\"941\" ac:width=\"680\"><ri:attachment ri:filename=\"svctosvcindiffcluster.jpg\" ri:version-at-save=\"1\" /></ac:image><h2>GRPC Flow b/w Cluster</h2><ac:image ac:align=\"center\" ac:layout=\"wide\" ac:original-height=\"401\" ac:original-width=\"941\" ac:width=\"680\"><ri:attachment ri:filename=\"grpcflow.jpg\" ri:version-at-save=\"1\" /></ac:image><p /><h2>Traffic Flow</h2><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"671\" ac:original-width=\"509\" ac:width=\"550\"><ri:attachment ri:filename=\"TrafficFlow.jpg\" ri:version-at-save=\"1\" /></ac:image><h2>Proposed Traffic flow for backup cluster </h2><p /><ac:image ac:align=\"center\" ac:layout=\"wide\" ac:original-height=\"771\" ac:original-width=\"712\" ac:width=\"680\"><ri:attachment ri:filename=\"network_arch-Proposed_HightLevelTrafficFlow.jpg\" ri:version-at-save=\"1\" /></ac:image><h2>Details Traffic Flow diagram</h2><p><ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"EW\" ri:content-title=\"Disaster Recovery / Backup Flow\" ri:version-at-save=\"1\" /><ac:link-body>Disaster Recovery / Backup Flow</ac:link-body></ac:link> </p><p />",
      "approach_used": "endpoint_2",
      "word_count": 121
    },
    {
      "id": "2360508900",
      "title": "Istio POC",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2360508900",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2360508900/Istio+POC",
      "created": "2022-02-16T04:07:29.206Z",
      "content": "<ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"afe7dc06-cfb6-4039-9738-a1a8cfe70727\" ac:macro-id=\"95d4c74c-987b-46ff-837b-ca9797d4df09\" /><h1>Istio Architecture</h1><p>Istio service mesh is logically split into a data plane and control plan.</p><ol><li><p><strong>Control plane</strong> manages and configures the proxies to route traffic.</p></li><li><p><strong>Data Plane</strong> comprises a set of intelligent proxies deployed as sidecars with application pods.</p></li></ol><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"611\" ac:original-width=\"825\"><ri:attachment ri:filename=\"image-20220211-085552.png\" ri:version-at-save=\"1\" /></ac:image><p>Istio uses an extended version of the Envoy proxy. Envoy is a high-performance proxy that mediates all inbound and outbound traffic for all services in the service mesh. <strong>The sidecar proxy model also allows you to add Istio capabilities to an existing deployment without requiring you to rearchitect or rewrite code.</strong></p><p>Some of the Istio features and tasks enabled by Envoy proxies include:</p><ol><li><p>Traffic Control Feature: Enforce fine-grained traffic control with rich routing rules for HTTP, gRPC, WebSocket, and TCP traffic.&nbsp;</p></li><li><p>Network Resiliency Features: setup retries, failovers, circuit breakers, and fault injection.&nbsp;</p></li><li><p>Security and authentications Features: enforce security policies and enforce access control and rate-limiting defined through the configuration API.</p></li></ol><h1>Multi-Cluster Service Mesh</h1><p>Istio supports various strategies for multi-cluster service mesh and inter-service communication b/w service.</p><ol><li><p>It supports shared Istio control-plane b/w multiple clusters.</p></li><li><p>We can have multiple control-plane in multiple clusters with remote setup.&nbsp;</p></li><li><p>We can use multiple Istio control-plane and then have <a href=\"https://github.com/istio-ecosystem/admiral+\">admiral</a> help us in a single service mesh in various clusters. (Recommended)</p></li></ol><h1>Traffic Management</h1><ul><li><p>You might want to direct a particular percentage of traffic to a new service version as part of A/B testing or apply a different load balancing policy to traffic for a specific subset of service instances.&nbsp;</p></li><li><p>Routing rules are a powerful tool for routing particular subsets of traffic to specific destinations. You can set match conditions on traffic ports, header fields, URIs, and more.&nbsp;</p></li><li><p>You can also select them using the exact value, a prefix, or a regex for some match conditions. For example, &quot;20% of calls go to the new version&quot; or &quot;calls from these users go to version 2&quot;.</p></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"0b550a82-3d66-4376-b162-b420d90cb7a2\"><ac:parameter ac:name=\"title\">Example</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"14a6d70b-6ae2-4d82-82f5-283a2a9d147d\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[http:\n  - match:\n      - headers: end-user:\n          exact: jason\n    route:\n      - destination:\n          host: reviews\n          subset: v2]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><ul><li><p>Istio also lets users send traffic to two different services, ratings and reviews (For example), as part of a more extensive virtual service.</p></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"8b0a2703-6db1-4c83-b8ff-063bb1b161f2\"><ac:parameter ac:name=\"title\">Example</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"db497016-c658-4a44-9658-fae63cd443be\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[spec:\n  hosts:\n    - bookinfo.com\n  http:\n  - match:\n    - uri:\n        prefix: /reviews\n    route:\n    - destination:\n        host: reviews\n  - match:\n    - uri:\n        prefix: /ratings\n    route:\n    - destination:\n        host: ratings]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><ul><li><p>You can also use routing rules to perform some actions on the traffic, for example:</p><ul><li><p>Append or remove headers.</p></li><li><p>Rewrite the URL.</p></li><li><p>Set a retry policy for calls to this destination.</p></li></ul></li><li><p>Load balancing options: Istio uses a round-robin load balancing policy by default. Istio also supports the following models:</p><ul><li><p>Random: Requests are forwarded at random to instances in the pool.</p></li><li><p>Weighted: Requests are forwarded to instances in the pool according to a specific percentage.</p></li><li><p>Least requests: Requests are forwarded to instances with the least number of requests.</p></li></ul></li><li><p>You use a gateway to manage inbound and outbound traffic for your mesh, which specifies which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies running at the edge of the mesh.</p></li><li><p>Gateways are primarily used to manage ingress traffic, but you can also configure egress gateways.</p></li><li><p>An egress gateway lets you configure a dedicated exit node for the traffic leaving the mesh, limiting which services can or should access external networks or enabling secure control of egress traffic to add security to your mesh.&nbsp;</p></li><li><p><strong>Mirroring: </strong>Traffic mirroring is a powerful concept that allows feature teams to bring changes to production with as little risk as possible. For example, mirroring sends a copy of live traffic to a mirrored service without serving live traffic.</p></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"cc040e45-3b69-48de-99fa-00b283cacf5c\"><ac:parameter ac:name=\"title\">Example</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"06bcc3c6-2430-47dd-98f0-c94a89a74596\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  hosts:\n    - httpbin\n  http:\n  - route:\n    - destination:\n        host: httpbin\n        subset: v1\n      weight: 100\n    mirror:\n      host: httpbin\n      subset: v2\n    mirrorPercentage:\n      value: 100.0]]></ac:plain-text-body></ac:structured-macro></ac:rich-text-body></ac:structured-macro><h1>Network Resilience and testing</h1><ul><li><p><strong>Timeouts</strong>: A timeout is the amount of time that an Envoy proxy should wait for replies from a given service, ensuring that services don't hang around waiting for responses indefinitely and that calls succeed or fail within a predictable timeframe.<strong>&nbsp;The Envoy timeout for HTTP requests is disabled in Istio by default.</strong></p></li><li><p><strong>Retries:</strong>&nbsp;A retry setting specifies the maximum number of times an Envoy proxy attempts to connect to a service if the initial call fails. T<strong>he default retry behavior for HTTP requests is to retry twice before returning the error.</strong></p></li><li><p><strong>Circuit breakers:</strong>&nbsp;A circuit breaker pattern enables fast failure rather than clients trying to connect to an overloaded or failing host.</p></li><li><p><strong>Fault injection</strong>: It's a testing method that introduces error into a system to ensure that it can withstand and recover from error conditions. Istio lets you inject faults at the application layer, such as HTTP error codes, to get more relevant results. You can inject two types of faults.&nbsp;</p><ul><li><p><strong>Delays</strong>: Delays are timing failures that mimic increased network latency or an overloaded upstream service.</p></li><li><p><strong>Aborts:</strong>&nbsp;Aborts usually manifest in HTTP error codes or TCP connection failures.</p></li></ul></li></ul><h1>Observability&nbsp;</h1><p><strong>All options that we will be discussing here are configurable.</strong></p><p>Istio provides a Telemetry API that enables the flexible configuration of metrics, access logs, and tracing.</p><p>Telemetry API resources inherit configuration from parent resources in the Istio configuration hierarchy:</p><ol><li><p>root configuration namespace (Cluster level)</p></li><li><p>local namespace.</p></li><li><p>Workload.</p></li></ol><p>As you know, now that Istio uses Telemetry API for observability, it also helps us in distributed tracing.</p><p>The following diagram will be a very generic flow of distributed tracing.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"121\" ac:original-width=\"581\" ac:width=\"442\"><ri:attachment ri:filename=\"tracing.jpg\" ri:version-at-save=\"1\" /></ac:image><p>Let's talk about how and what Istio provides us. Although Istio proxies can automatically send spans, they need some hints to tie together the entire trace. Therefore, applications need to propagate the appropriate HTTP headers so that when the proxies send span information, the spans can be correlated correctly into a single trace. (<strong>Expectation from Application</strong>)</p><h3>How Istio will help:</h3><p>Istio leverages&nbsp;<a href=\"https://www.envoyproxy.io/docs/envoy/v1.12.0/intro/arch_overview/observability/tracing\">Envoy's distributed tracing</a>&nbsp;feature to provide tracing integration out of the box. Specifically, Istio offers options to install various tracing backend and configure proxies to send trace spans to them automatically. In addition, Istio supported the backend: Zipkin, Jaeger, and Lightstep<strong>.&nbsp;</strong></p><h3>What Istio provides:&nbsp;</h3><p>Istio provides the following points, and all are configurable.</p><ol><li><p>Sampling:</p><ol><li><p>We can adjust the sampling rate on a cluster, namespace, and app level.</p></li><li><p>Istio supports the Random sampling rate for the percentage of requests.</p></li></ol></li><li><p>Istio supports the maximum length of the request path, after which the path will be truncated for reporting.&nbsp;</p></li><li><p>Istio also supports custom tagging. We can add custom tags to spans based on literal, environmental variables and client request headers to provide additional information in spans specific to your environment.</p></li><li><p>It also sends distributed tracing data to jaeger/Zipkin, Lightstep, and datalog.&nbsp;</p></li><li><p>Istio sends all (mesh level, service level, proxy level) metrics to the Prometheus.</p></li><li><p>Kiali is also an excellent tool to see insides. For example, we have deployed one microservice demo app, and here is the screen recording of the Kiali graph dashboard.</p></li></ol><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"662\" ac:original-width=\"650\"><ri:attachment ri:filename=\"Screen Recording 2022-02-11 at 6.26.46 PM.mov\" ri:version-at-save=\"1\" /></ac:image><p /><h1>Security</h1><p>Istio supports encryption, mTLS, audit logging.&nbsp;</p><ol><li><p><strong>Authentication</strong>: Istio supports cluster level, namespace level, and application-level authentication.&nbsp;</p><ol><li><p>mTLS</p></li><li><p>JWT Validation</p></li><li><p>Open ID connects the provider.</p></li></ol></li><li><p><strong>Authorization</strong></p></li><li><p><strong>Certificate Management</strong>: We can have our ROOT cert, and Istio will use that cert for signing requests.</p></li></ol><h1>Important Point before deciding Istio:</h1><ol><li><p>All feature is configurable; it depends if we want to enable them.&nbsp;</p></li><li><p>Complexity will be from the infra side, not from the application side. Application owner no need to change any code because of Istio by default. (Except Distributed Tracing )&nbsp;</p></li><li><p>If we do not take up service mesh with Kubernetes migration, then in the future, it will be a separate project which will take another 6-7 months.&nbsp;</p></li><li><p>For multi-cluster service discovery, we will be relying on any DNS-based approach, e.g., route53--&gt;loadbalancer--&gt;ingress--&gt;application.</p></li><li><p>Combined with Kiali and Prometheus, It will solve most of our observability problems.&nbsp;</p><p>The Istio security features provide strong identity, robust policy, transparent TLS encryption, authentication, authorization, and audit (AAA) tools to protect your services and data.</p></li><li><p>The Envoy proxy uses&nbsp;<strong>0.35 vCPU</strong>&nbsp;and&nbsp;<strong>40 MB memory</strong>&nbsp;per 1000 requests per second going through the proxy. Istiod(Istio control plane) uses&nbsp;<strong>1 vCPU</strong>&nbsp;and 1.5 GB of memory as per <a href=\"https://istio.io/latest/docs/ops/deployment/performance-and-scalability/\">Istio documentation</a>.</p></li><li><p>There is an extra 10ms-20ms overhead with Istio. Perf team did the test on the demo <a href=\"https://github.com/GoogleCloudPlatform/microservices-demo/\">microservice application</a>. Here is the result&nbsp;<ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"PE\" ri:content-title=\"Service Mesh Latency POC\" ri:version-at-save=\"3\" /><ac:link-body>Service Mesh Latency POC</ac:link-body></ac:link>.</p></li></ol><h1>Application Monitoring Metrics by Istio</h1><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"b4816600-62b0-4b3f-9b88-8bd117bae8a7\"><ac:parameter ac:name=\"title\">Istio Grafana Dashboard</ac:parameter><ac:rich-text-body><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"467\" ac:original-width=\"1835\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.10.21 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"923\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.06.47 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"923\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.06.34 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"615\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.06.22 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"615\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.06.14 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"615\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.06.03 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"615\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.05.37 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"615\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.05.27 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"615\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.05.04 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"913\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.04.49 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"234\" ac:original-width=\"920\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.04.32 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"326\" ac:original-width=\"1407\"><ri:attachment ri:filename=\"Screenshot 2022-02-11 at 10.04.02 PM.png\" ri:version-at-save=\"1\" /></ac:image></ac:rich-text-body></ac:structured-macro><p>Reference: <a href=\"https://istio.io/latest/docs/\">Istio Documentation</a> </p><p />",
      "approach_used": "endpoint_2",
      "word_count": 1486
    },
    {
      "id": "2361032859",
      "title": "Jenkins+Argo vs Harness vs Spinnaker",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2361032859",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2361032859/Jenkins+Argo+vs+Harness+vs+Spinnaker",
      "created": "2022-03-24T09:46:10.781Z",
      "content": "<p>The criteria for this comparison will be as follows:</p><ul><li><p>Efforts</p></li><li><p>Scalability</p></li><li><p>Maintenance</p></li><li><p>Monitoring</p></li><li><p>Cost</p></li><li><p>Pros/cons</p></li></ul><table ac:local-id=\"5c223060-0f34-4e76-a36e-b32c1a3f6c1c\" data-layout=\"wide\"><colgroup><col style=\"width: 108.0px;\" /><col style=\"width: 297.0px;\" /><col style=\"width: 285.0px;\" /><col style=\"width: 270.0px;\" /></colgroup><tbody><tr><th><p /></th><th><p><strong>Jenkins+Argo</strong></p></th><th><p><strong>Harness</strong></p></th><th><p><strong>Spinnaker</strong></p></th></tr><tr><td><p>Efforts</p></td><td><p>Need to setup on our own, SaaS is available for Argo, which we need to explore. Infrastructure side effort is needed, even if we decide to go with drone CI.</p><p>Argo CD &rarr; One time setup will take medium efforts due to SSO and RBAC</p><p>Need to setup backup strategies for both</p></td><td><p>Easy to setup, SaaS is available for master, for delegate we need to provide Kubernetes cluster<br /><br />SSO and RBAC provided and easy to setup</p><p>Don&rsquo;t have to look into backup provided by Harness</p></td><td><p>Armory is the enterprise version of spinnaker which reduces  the manitance of spinnaker microservices, otherwise we have to setup , manage infrastructure and define backup strategies.</p><p /></td></tr><tr><td><p>Scalability</p></td><td><p>In Jenkins, for ondemand container we need to setup autoscaling, for vms we need to setup plugins(one time effort)</p><p>Argo CD &rarr; Able to scale for current workload</p></td><td><p>Depends on Harness delegates, we need to maintain this from the k8 cluster in which the delegates are setup. </p><p>We have to setup the scaling by ourselves</p></td><td><p>Based on demand on executioner and retention  of data we have to define HPA and persistence storage services.</p></td></tr><tr><td><p>Maintenance</p></td><td><p>We have to do the maintenance for ourselves for both, if we use ArgoCD opensource.</p><p>Maintenance of Jenkins plugins also comes into picture</p></td><td><p>Harness manager manitenance is not required, but we have to maintain the worker node</p></td><td><p>Unlike Armory , We have to do the maintenance by ourselves.</p></td></tr><tr><td><p>Monitoring</p></td><td><p>Argo CD exposes endpoints for monitoring through Prometheus<br />Jenkins can be managed by Prometheus Plugin</p></td><td><p>Monitoring needed only for delegates</p></td><td><p>Spinnaker supports Datadog, Prometheus, and Google Stackdriver, the daemon is quite extensible and can be integrated with any monitoring solutions</p></td></tr><tr><td><p>Cost</p></td><td><p>low cost, only hosting cost will be there for hosting solution</p></td><td><p>Not sure</p></td><td><p>Armory is monetized and it vary depends on usage.</p><p>spinnaker is fully open-source.</p><p /></td></tr><tr><td><p>Pros/Cons</p></td><td><p><strong>Pros:</strong></p><ul><li><p>Customisation is available through Jenkins pipelines</p></li><li><p>Stable product, community support as well</p></li><li><p>Jenkins shared library for shared functionality</p></li><li><p>Argo CD community support:  <a data-card-appearance=\"inline\" href=\"https://github.com/argoproj/argo-cd/blob/master/USERS.md\">https://github.com/argoproj/argo-cd/blob/master/USERS.md</a> </p></li><li><p>Multi-tenancy and RBAC policies for authorisation</p></li><li><p>Huge adoption and freestyle job gives new users an easy way to create small jobs for maintenance or recurring tasks</p></li><li><p>Less costly</p></li><li><p>Easy integration of other tools via plugins or Jenkins specific commands are available</p></li></ul><p><strong>Cons:</strong></p><ul><li><p>Organisation wide governance policy is not available</p></li><li><p>Pipeline requires groovy knowledge, can be complex</p></li><li><p>Argo CD is only for Kubernetes</p></li><li><p>In Jenkins need to write all the steps and stages ourself, plugins give minimum support</p></li><li><p>We need to automate the maintenance activity</p></li><li><p>New feature integration needs to be done by us</p></li></ul></td><td><p><strong>Pros:</strong></p><ul><li><p>No need to maintain harness manager</p></li><li><p>Most of the basic stages as modules/packages</p></li><li><p>Drag and drop facility from Harness UI</p></li><li><p>Easy to understand </p></li><li><p>Bi-directional sync with github</p></li><li><p>Harness Support is available</p></li><li><p>Organisation wide governance policies are available</p></li><li><p>Visibility is good</p></li><li><p>Mult-Cloud will be easy to setup.</p></li></ul><p><strong>Cons:</strong></p><ul><li><p>We still need to write the 3rd party tool functionality, effort is same as Jenkins</p></li><li><p>Delegate management is on us</p></li><li><p>Templatisation of pipeline is not available</p></li><li><p>VM based deployment are not properly supported</p></li><li><p>Next gen is completely Kubernetes based </p></li><li><p>Pipeline as a code is complex, Creation from UI is needed</p></li><li><p>Need to share the knowledge of tool to other teams, like QA, so they can migrate their jobs to Harness</p></li><li><p>Debugging is tedious and not straight forward</p></li><li><p>For CI, only container based agents are available</p></li><li><p>Might need to change existing build process</p></li></ul></td><td><p><strong>Pros:</strong></p><ul><li><p>Open Source and active development.</p></li><li><p>Multi-cloud deployments ( also can integrate with Kubernetes)</p></li><li><p>Automated triggers.</p></li><li><p>In-house bakery service, which helps in immutable deployments.</p></li><li><p>Easy pipeline setups using the UI, no need to write complex CFNs for code deployments.</p></li><li><p>Declarative and imperative pipeline</p></li><li><p>Both high level and low-level views of clusters, which has fine-grained options to control cloud infra from Spinnaker UI itself.</p></li><li><p>SSO and RBAC supported.(OAuth 2.0 / OIDC, SAML, LDAP, and X.509)</p></li></ul><p><strong>Cons:</strong></p><ul><li><p>Initial setup and configuration can be complex.</p></li><li><p>Relatively larger learning curve compared to other delivery tools.</p></li><li><p>Management strategy and available services are dependent on the cloud provider.</p></li><li><p>No support to deploy the artifacts without re-creating the servers. Only pure immutable deployment is allowed.</p></li><li><p>No support for secret management</p></li><li><p>No direct audit trail support(But Armory supports)</p></li></ul></td></tr><tr><td><p>Time Needed</p></td><td><p>Jenkins:</p><ul><li><p>Infra setup &rarr; 2 days</p></li><li><p>Pipeline as a code &rarr; 2 weeks (only for proposed CI and CD)</p></li><li><p>First application onboarding per language &rarr; 2-3 days</p></li></ul><p>Argo CD:</p><ul><li><p>Infra setup &rarr; 4 days</p></li><li><p>Pipeline as a code &rarr; 2 days</p></li><li><p>First application onboarding per deployment strategy &rarr; 3 days</p></li></ul><p>Infra setup &rarr; Initial setup for Argo CD, Argo rollout, and Jenkins. SSO, monitoring, scaling.</p><p>Tools decision &rarr; Tools to use for different stages, example: what tool to use for static code analysis, code coverage, security scan etc.</p></td><td><p>Setting up Harness &rarr; 2 days<br />CI &amp; CD &rarr; 2 weeks 2 days</p><p>First Application onbording &rarr; 2-3 days</p><p>Tools decision &rarr; Tools to use for different stages, example: what tool to use for static code analysis, code coverage, security scan etc.</p></td><td><p><ac:inline-comment-marker ac:ref=\"b232ba0a-4705-4e51-9236-3075e42d3bc8\">Setting up Harness &rarr; 2 days</ac:inline-comment-marker></p><p /><p>First Application onbording &rarr; 5-7 days</p></td></tr></tbody></table>",
      "approach_used": "endpoint_2",
      "word_count": 774
    },
    {
      "id": "2361131049",
      "title": "Spinnaker",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2361131049",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2361131049/Spinnaker",
      "created": "2022-02-07T05:22:14.639Z",
      "content": "<p><strong>Why spinnaker?</strong></p><p>The primary purpose or goal of Spinnaker is to make reliable deployments. Spinnaker offers deployment, and it also allows for the simplest rollbacks. Multi-Cloud Continuous Delivery Platform for releasing software changes with high confidence</p><p /><p><strong>Spinnaker Vs Armory</strong></p><p>Armory is just an enterprise version of spinnaker, but it does not mean that the entire product is sass. we have to provide infrastructure (VPC and other resources) to install the application. But they will do installation, upgrades, monitoring and maintenance and performance tuning. And few plugin are restricted to Armory alone.</p><p /><p><strong>Minumum Resources required to run the application</strong></p><p>four cores and 16GB of RAM</p><p /><p><strong>Features</strong> </p><ul><li><p>Out of the box deployment strategies.</p><ul><li><p><strong>Highlander &rarr; </strong>Similar to Blue/Green, except the old deployment, is destroyed once traffic is shifted</p><p><strong>Red/Black</strong> (a.k.a. Blue/Green)<strong>&rarr; </strong> The load balancer routes traffic to the active (enabled) cluster/server group. Then, a new deployment replaces servers (w/ K8s provider -&gt; Replica Sets &amp; Pods) in the disabled cluster/server group. When the newly enabled cluster/server group is ready, the load balancer routes traffic to this cluster and the previous cluster becomes disabled. The currently disabled cluster/server group (previously enabled cluster/server groups) is kept around by spinnaker in case a rollback is needed for the next X deployments (which is a configurable parameter).</p><p><strong>Rolling Red/Black &rarr;  </strong>Similar to Blue/Green, but traffic is gradually shifted from the older deployment to the new one.</p><p><strong>Canary &rarr;  </strong>production, a baseline instance (a smaller clone of production), and a canary instance with the new deployment. Production handles most of the load while the baseline and canary each receive a smaller amount. After a predetermined amount of time, the performance of the baseline and canary are compared. The new deployment becomes the new production build if requirements are met. The canary analysis can be automated or manual.</p></li><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"495\" ac:original-width=\"967\" ac:width=\"638\"><ri:attachment ri:filename=\"deployment-strategies-20220207-022653.png\" ri:version-at-save=\"1\" /></ac:image></li></ul></li><li><p>Open Source and active development.</p></li><li><p>Multi-cloud deployments ( also can integrate with Kubernetes)</p></li><li><p>Automated triggers.</p></li><li><p>Manual Judgements.</p></li><li><p>In-house bakery service, which helps in immutable deployments. </p></li><li><p>Easy pipeline setups using the UI, no need to write complex CFNs for code deployments.</p></li><li><p>Exactly &quot;one-click rollback&quot;.</p></li><li><p>Declarative and imperative pipeline</p></li><li><p>Both high level and low-level views of clusters, which has fine-grained options to control cloud infra from Spinnaker UI itself.</p></li><li><p>SSO and RBAC supported.(OAuth 2.0 / OIDC, SAML, LDAP, and X.509)</p></li><li><p>support both GitOps style deployment and ondemand delivery</p></li><li><p>Integration with Jira and servicenow</p></li></ul><p><strong>Cons/Drawbacks</strong></p><ul><li><p>Initial setup and configuration can be complex.</p></li><li><p>Relatively larger learning curve compared to other delivery tools.</p></li><li><p>Management strategy and available services are dependent on the cloud provider.</p></li><li><p>No support to deploy the artifacts without re-creating the servers. Only pure immutable deployment is allowed.</p></li><li><p>No support for  secret management </p></li><li><p>No direct audit trail support(But Armory supports)</p></li></ul><p /><h3>Metrics provider</h3><p style=\"margin-left: 30.0px;\">Spinnaker exposes metrics (both counter and gauge type) through the Spinnaker-monitoring daemon. Although Spinnaker supports Datadog, Prometheus, and Google Stackdriver, the daemon is quite extensible and can be integrated with any monitoring solutions in your premise.</p><h3><strong>Notification</strong></h3><p style=\"margin-left: 30.0px;\">Spinnaker supports notifications through email and provides integrations with many collaboration and service management tools such as Slack, ServiceNow, JIRA, Twilio, PagerDuty, Microsoft Teams, and others</p><h3><strong>Logs</strong></h3><p style=\"margin-left: 30.0px;\">Logs can be forwarded to whatever logging solution we have configured.</p><h3><strong>Productionization</strong></h3><ul><li><p>Caching &rarr; Spinnaker relies on its Redis cache(storing live executions, storing infrastructure information)</p></li><li><p>Scaling &rarr; performance and reliability can be improved  by scaling its microservices(Clouddriver, Orca)</p></li><li><p>Persistence &rarr; Redis can act as storage but it is not recommended for production (s3 , etc)</p></li></ul><p /><p /><p /><p /><p><a href=\"https://www.opsmx.com/blog/spinnaker-vs-argo-cd/#:~:text=For%20safe%20deployment%2C%20Spinnaker%20offers,Argo%20Rollouts%20to%20strategically%20deploy.&amp;text=Both%20tools%20support%20on%2Dprem%20and%20manageed%20Kubernetes.\">Why spinnaker over ArgoCD</a></p><p><a href=\"https://www.opsmx.com/blog/jenkins-vs-spinnaker/\">Why spinnaker over Jenkins</a></p><p><a href=\"https://www.opsmx.com/blog/spinnaker-vs-harness/\">Why Spinnaker over Harness</a></p><p><a href=\"https://www.armory.io/blog/how-armorys-enterprise-distribution-compares-to-oss-spinnaker/\">Why Armory over Spinnaker</a></p><p><a href=\"https://www.armory.io/pricing/\">Armory pricing</a></p><p /><p /><p /><p /><p /><p><br /></p><p style=\"margin-left: 60.0px;\" />",
      "approach_used": "endpoint_2",
      "word_count": 581
    },
    {
      "id": "2361229415",
      "title": "AWS SSO integration using SAML - Infra",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2361229415",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2361229415/AWS+SSO+integration+using+SAML+-+Infra",
      "created": "2022-04-29T06:05:32.811Z",
      "content": "<p /><ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"e1d6d491-4d7e-4bbc-9d7f-bb90aace69d6\" ac:macro-id=\"c50c9adc-9d1f-43a9-9b9e-f3c2871d7452\" /><h2><em><strong>Problem Statement</strong></em></h2><hr /><p>Presently, We don&rsquo;t have a proper access management process for AWS IAM users in Meesho. IAM access for AWS accounts are done manually via AWS console, which poses a few challenges:</p><ul><li><p>Creating IAM users manually and setting up MFA for all users is a hectic work</p></li><li><p>Difficult to manage AWS access keys and secret access keys for all users. Keys can be misplaced also and managing key rotation becomes difficult. This can lead to many security vulnerability if not managed correctly</p></li><li><p>Difficult to switch accounts between different environments since we have multi accounts setup</p></li></ul><h2><em><strong>Solution</strong></em></h2><hr /><p>Since meesho is using G Suite as email provider, we can use G Suite as an identity provider (IdP) for AWS access. You can connect AWS federated SSO to G Suite, allowing users to access AWS accounts with their G Suite credentials. Few benefits are mentioned below - </p><ul><li><p>Assume role using SAML / Google SSO</p></li><li><p>Prevent from creating extra credentials for the same user in different accounts</p></li><li><p>Authentication happens using google credentials which is already enforced with 2FA</p></li><li><p>Ease in switching multiple roles by creating different profiles both on console and CLI</p></li></ul><h2><em><strong>Implementation Overview</strong></em></h2><hr /><ul><li><p>All AWS users access will be managed via terraform and users will be logged in from gmail apps</p></li><li><p>Any change around lifecycle management of a user (Like create, delete , update etc) will mean appropriate change in the terraform code &rarr; Review &rarr; and then applying the change</p></li><li><p>For onboarding new user or revoking activity, the flow will be like this:</p><ol><li><p>A Jira needs to be created by the buddy with details. Follow this link for onboarding - <ac:link><ri:page ri:space-key=\"EW\" ri:content-title=\"Tech Onboarding Access\" ri:version-at-save=\"14\" /><ac:link-body>Tech Onboarding</ac:link-body></ac:link></p></li><li><p>Devops/Appsupport team will make appropriate change in the terraform code in a branch and raise a PR with appropriate label.</p></li><li><p>The PR will be reviewed and approved (by devops) before merging to master (<em>terraform plan</em>).</p></li><li><p>The changes will be applied (<em>terraform apply</em>). On successful execution PR to be merged.</p></li></ol></li><li><p>Installing tools like awsudo and gsts helps in assuming roles by generating temporary credentials for CLI utility. <ac:link><ri:page ri:content-title=\"How to assume role using saml for CLI access\" ri:version-at-save=\"10\" /><ac:link-body>Link</ac:link-body></ac:link></p></li></ul><p><strong>Note</strong>: This also means that no console action should happen for user roles managed by terraform. Manual changes will cause a drift and entirely defeats the purpose of automation.&nbsp;</p><p>&nbsp;</p><h2><em><strong>Future plan</strong></em></h2><hr /><ul><li><p>User onboarding can be managed a proper change management process using terraform which can be automated using lambda and jenkins jobs. </p></li></ul><p /><h2><strong>Implementation details</strong></h2><p><br /><strong>How it works:</strong></p><p>AWS SSO authenticates your G Suite users by using Security Assertion Markup Language (SAML) 2.0 authentication. You can use this service to provide one-click SSO to your AWS resources by using your existing Google Apps credentials. For users to whom you grant SSO access, they will see an additional SAML app in your Google Apps account. The AWS administrator delegates responsibility for authentication to a trusted IDP in this case Google Apps and uses SAML 2.0. This allows an IAM role to grant the federated user permissions to sign in to the AWS Management Console and access your AWS resources.</p><p><br /><strong>The login process is as follows:</strong></p><ul><li><p>The federated user clicks the Google Apps SSO link to AWS in their browser. If the user has not already logged in, he will go to the Google Apps account login portal.</p></li><li><p>The portal authenticates the user&rsquo;s Google Apps credentials and then generates a SAML authentication response that includes <a href=\"http://saml.xml.org/assertions\"><u>assertions </u></a>that identify the user and include attributes about the user. The portal sends this response to the user&rsquo;s browser.</p></li><li><p>The user&rsquo;s browser redirects to the AWS Sign-In endpoint and posts the SAML assertion.</p></li><li><p>AWS sign-in calls the <a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html\"><u>AssumeRoleWithSAML</u></a> API action (behind the scenes) to request temporary security credentials, and then create an AWS Management Console sign-in URL using those credentials.</p></li><li><p>AWS returns the sign-in URL to the user&rsquo;s browser as a redirect.</p></li><li><p>The user&rsquo;s browser redirects to the AWS Management Console. Note that if the SAML authentication response includes attributes that map to multiple IAM roles, the user is prompted to select the IAM role to use for access to the console.</p></li></ul><p /><p><strong>This authentication flow is shown in the following diagram.</strong></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"288\" ac:original-width=\"621\"><ri:attachment ri:filename=\"hn5El6uxbYjQucIENbJJuoCRJ0kZXU0gH7yI1ptIfbsP5EzwFHKdt9SjKu3L2qJ38D-_spfPt2MZcgWZs4C4bP1rzg2-BYlWRK1SHGBe_5k6iOSkJiEXwcSrkkfaGI-mYupkP7ew\" ri:version-at-save=\"1\" /></ac:image><h2><strong>Configuration setup</strong></h2><p>Follow these top-level steps to set up federated SSO to your AWS resources by using Google Apps:</p><p><strong>Step 1:</strong> Get the SAML metadata from your Google Apps account</p><ul><li><p>Log in to your Google Admin console with your super administrator credentials.</p></li><li><p>Security&gt; SSO with Google as SAML IDP.</p></li><li><p>GoTo the IdP metadata section and <strong>download metadata</strong>.</p></li></ul><p>&nbsp;</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"498\" ac:original-width=\"708\"><ri:attachment ri:filename=\"cUDIcplMCwNvI_BtHlaa-G5PgMKQqz7ScG69uYNz5v03uHvquOCme4oYf7GF77R9vQ40SQTvjc7RrDQ9ggCD0PQVMJYAeiZqY0__opnTxFhlcrnCm6ZYrWLkZY2X9YIqoPvFBPQj\" ri:version-at-save=\"1\" /></ac:image><p>&nbsp;&nbsp;</p><p><strong>Step 2:</strong> Create an IdP in your AWS account.</p><ul><li><p>Go to the IAM console and click Identity Providers. Click Create Provider and then select SAML from the Provider Type drop-down list (see the following screenshot). Type a name for the provider (such as GoogleSaml), and then browse to the metadata file you downloaded in Step 1.</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"521\" ac:original-width=\"1000\"><ri:attachment ri:filename=\"Screenshot 2022-04-19 at 10.26.15 PM.png\" ri:version-at-save=\"1\" /></ac:image><p /><ul><li><p>Click Next Step and then click Create. As the following screenshot shows, your newly created IdP will appear in the table of IdPs on the Identity Providers page.</p></li><li><p>Click the new IdP and note its Provider ARN, which you will need in Step3.</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"521\" ac:original-width=\"1577\"><ri:attachment ri:filename=\"Screenshot 2022-04-19 at 10.27.25 PM.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Step 3:</strong> Create IAM roles in AWS accounts.&nbsp;&nbsp;</p><ul><li><p>For users accessing the AWS Management Console, the IAM role that the user assumes governs access to AWS resources within your AWS account. The role is where you define what you allow a federated user to do after they sign in.</p></li><li><p><ac:inline-comment-marker ac:ref=\"8a8982bd-dca3-41c2-9bd7-679338289e55\">On the Select trusted entity Type page, choose Role for Identity Provider Access and then click Select next to Grant Web Single Sign-On (WebSSO) access to SAML providers.</ac:inline-comment-marker></p></li><li><p>Select from the drop-down list the SAML IdP you created in the previous step, and then click Next. Step</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"746\" ac:original-width=\"1577\"><ri:attachment ri:filename=\"Screenshot 2022-04-19 at 10.34.12 PM.png\" ri:version-at-save=\"1\" /></ac:image><p /><ul><li><p>Create assume role policy and attach to this role as permission.</p></li><li><p>You will be presented with the Trust Policy, which grants the SAML IdP you created with permission to assume an IAM role from AWS STS. </p></li><li><p>Type a name in the Role Name box, and then click to create role</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"746\" ac:original-width=\"1577\"><ri:attachment ri:filename=\"Screenshot 2022-04-19 at 10.39.48 PM.png\" ri:version-at-save=\"1\" /></ac:image><p /><p><strong>Step 4:</strong> Go to google suite settings, Add the <strong>AWS SAML attributes to your Google Apps </strong>user profile.</p><ul><li><p>create a custom 2 user attribute for Amazon Web Services in google workspace.</p><ol><li><p>Role</p></li><li><p>SessionDuration</p></li><li><p>Goto &gt; Google admin console.</p></li><li><p>Apps &gt; Web and mobile apps&gt;Add custom SAML app.</p></li><li><p>Updated the ACS URL and Entity ID values for Amazon Web Services (get the details from AWS console)</p></li><li><p>On the Attribute Mapping page, click the Select field menu and map the following Google directory attributes to their corresponding Amazon Web Services attributes.</p></li></ol></li></ul><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"f51e6c3a-3b13-4018-a83a-279fbcf81c65\"><ac:plain-text-body><![CDATA[Google Directory attributes     > App attributes\n\nRole                            > https://aws.amazon.com/SAML/Attributes/Role\nBasic information>Primaryemail  > https://aws.amazon.com/SAML/Attributes/RoleSessionName \nSessionDuraion                  > https://aws.amazon.com/SAML/Attributes/SessionDuration]]></ac:plain-text-body></ac:structured-macro><p><strong>Step 6:</strong> Enable the Amazon Web Service application.</p><ul><li><p>Apps &gt; Web and mobile apps&gt; Amazon Wb Service.</p></li><li><p>Go to User access.</p></li><li><p>Turn ON the service by group.</p></li><li><p>Ensure that your Amazon Web Services user account email IDs match with those in your Google domain.</p></li><li><p><strong>For each user signing in to AWS via SSO</strong>, configure the custom user attribute you created before.</p><ol><li><p>On the user's account page, click User information.</p></li><li><p>Click the Amazon custom attribute.</p></li><li><p>In the Role field, add the AWS Role ARN and the Provider ARN, separated by a comma, <strong>as follow: </strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"6a008075-4150-4fe5-a6ac-32a89e3e6aa3\"><ac:plain-text-body><![CDATA[arn:aws:iam::ACCOUNT_NUMBER:role/googleusers,arn:aws:iam::ACCOUNT_NUMBER:provider/GoogleWorkspace]]></ac:plain-text-body></ac:structured-macro></li><li><p>Click Save.</p></li></ol></li><li><p>Once this setup is done. Users are added in saml app and ready to use. </p></li></ul><p>For access AWS console you can follow this - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"AWS SAML - How to switch roles\" ri:version-at-save=\"17\" /><ac:link-body>AWS SAML - How to switch roles</ac:link-body></ac:link> <br />For accessing AWS cli via saml you can follow this - <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"How to assume role using saml for CLI access\" ri:version-at-save=\"10\" /><ac:link-body>How to assume role using saml for CLI access</ac:link-body></ac:link> </p><p><strong>Conclusion</strong></p><p>We have set up G Suite as an external IdP for AWS SSO (SAML), granted access to an AWS account for meesho users, and enforced fine-grained permission controls for roles. This enables to have easy access to the AWS Cloud.</p>",
      "approach_used": "endpoint_2",
      "word_count": 1324
    },
    {
      "id": "2362376204",
      "title": "Linkerd POC",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2362376204",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2362376204/Linkerd+POC",
      "created": "2022-02-07T14:00:18.274Z",
      "content": "<h2>Introduction - </h2><p>Linkerd is a <em>service mesh</em> for Kubernetes. It makes running services easier and safer by giving you runtime debugging, observability, reliability, and security&mdash;all without requiring any changes to your code.</p><p>How it works - </p><p>Linkerd works by installing a set of ultralight, transparent proxies next to each service instance. These proxies automatically handle all traffic to and from the service. Because they&rsquo;re transparent, these proxies act as highly instrumented out-of-process network stacks, sending telemetry to, and receiving control signals from, the control plane. This design allows Linkerd to measure and manipulate traffic to and from your service without introducing excessive latency.</p><p>In order to be as small, lightweight, and safe as possible, Linkerd&rsquo;s proxies are written in <a href=\"https://www.rust-lang.org/\">Rust</a> and specialized for Linkerd.</p><h2>Architecture - </h2><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"1184\" ac:original-width=\"1200\" ac:width=\"448\"><ri:attachment ri:filename=\"linkerd_control_plane.png\" ri:version-at-save=\"1\" /></ac:image><p>At a high level, Linkerd consists of a <strong>control plane</strong> and a <strong>data plane</strong>.</p><p>The <strong>control plane</strong> is a set of services that and provide control over Linkerd as a whole.</p><p>The <strong>data plane</strong> consists of transparent <em>micro-proxies</em> that run &ldquo;next&rdquo; to each service instance, as sidecar containers in the pods. These proxies automatically handle all TCP traffic to and from the service, and communicate with the control plane for configuration.</p><p>Features  - </p><h2>Multi-cluster communication</h2><p>Linkerd&rsquo;s multi-cluster support works by &ldquo;mirroring&rdquo; service information between clusters.</p><ul><li><p><strong>A unified trust domain.</strong> The identity of source and destination workloads are validated at every step, both in and across cluster boundaries.</p></li><li><p><strong>Separate failure domains.</strong> Failure of a cluster allows the remaining clusters to function.</p></li><li><p><strong>Support for heterogeneous networks.</strong> Since clusters can span clouds, VPCs, on-premises data centers, and combinations thereof, Linkerd does not introduce any L3/L4 requirements other than gateway connectivity.</p></li><li><p><strong>A unified model alongside in-cluster communication.</strong> The same observability, reliability, and security features that Linkerd provides for in-cluster communication extend to cross-cluster communication.</p></li></ul><p>Linkerd&rsquo;s multi-cluster functionality is implemented by two components: a <em>service mirror</em> and a <em>gateway</em>. The <em>service mirror</em> component watches a target cluster for updates to services and mirrors those service updates locally on a source cluster. This provides visibility into the service names of the target cluster so that applications can address them directly. The <em>multi-cluster gateway</em> component provides target clusters a way to receive requests from source clusters. (This allows Linkerd to support <a href=\"https://linkerd.io/2020/02/17/architecting-for-multicluster-kubernetes/#requirement-i-support-hierarchical-networks\">hierarchical networks</a>.)</p><h2>Security  -</h2><p><a href=\"https://linkerd.io/2.11/features/server-policy/\">Authorization Policy</a>. Linkerd can restrict which types of traffic are allowed to .</p><p><a href=\"https://linkerd.io/2.11/features/automatic-mtls/\">Automatic mTLS</a>. Linkerd automatically enables mutual Transport Layer Security (TLS) for all communication between meshed applications.</p><p><a href=\"https://linkerd.io/2.11/features/proxy-injection/\">Automatic Proxy Injection</a>. Linkerd will automatically inject the data plane proxy into your pods based annotations.</p><h2>Traffic Management - </h2><p><a href=\"https://linkerd.io/2.11/features/fault-injection/\">Fault Injection</a>. Linkerd provides mechanisms to programmatically inject failures into services.</p><p><a href=\"https://linkerd.io/2.11/features/ha/\">High Availability</a>. The Linkerd control plane can run in high availability (HA) mode.</p><p><a href=\"https://linkerd.io/2.11/features/http-grpc/\">HTTP, HTTP/2, and gRPC Proxying</a>. Linkerd will automatically enable advanced features (including metrics, load balancing, retries, and more) for HTTP, HTTP/2, and gRPC connections.</p><p><a href=\"https://linkerd.io/2.11/features/retries-and-timeouts/\">Retries and Timeouts</a>. Linkerd can perform service-specific retries and timeouts.</p><p><a href=\"https://linkerd.io/2.11/features/service-profiles/\">Service Profiles</a>. Linkerd's service profiles enable per-route metrics as well as retries and timeouts.</p><p><a href=\"https://linkerd.io/2.11/features/ingress/\">Ingress</a>. Linkerd can work alongside your ingress controller of choice.</p><p><a href=\"https://linkerd.io/2.11/features/load-balancing/\">Load Balancing</a>. Linkerd automatically load balances requests across all destination endpoints on HTTP, HTTP/2, and gRPC connections.</p><p><a href=\"https://linkerd.io/2.11/features/traffic-split/\">Traffic Split (canaries, blue/green deploys)</a>. Linkerd can dynamically send a portion of traffic to different services.</p><p><a href=\"https://linkerd.io/2.11/features/protocol-detection/\">TCP Proxying and Protocol Detection</a>. Linkerd is capable of proxying all TCP traffic, including TLS'd connections, WebSockets, and HTTP tunneling.</p><h2>Observability</h2><p><a href=\"https://linkerd.io/2.11/features/distributed-tracing/\">Distributed Tracing</a>. You can enable distributed tracing support in Linkerd.</p><p><a href=\"https://linkerd.io/2.11/features/telemetry/\">Telemetry and Monitoring</a>. Linkerd automatically collects metrics from all services that send traffic through it.</p><p>it supports cluster dashboard - <a href=\"https://linkerd.io/2.10/features/dashboard/\" data-card-appearance=\"inline\">https://linkerd.io/2.10/features/dashboard/</a>  </p><h2>Web:</h2><p>The Web component is a web application that provides the Linkerd dashboard:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"437\" ac:original-width=\"700\"><ri:attachment ri:filename=\"1*y9YzAJ32EqScT9ujN9wnVw.png\" ri:version-at-save=\"1\" /></ac:image><p>The dashboards provides observability and telemetry on the &ldquo;meshed&rdquo; services. It also allows you to tap into the live requests of services to see in real time what requests are being done.</p><h2>Prometheus:</h2><p>The Prometheus component is responsible to scrape the metrics of the proxy component and store them temporarily. The Prometheus instance is configured for performance. It is not meant to store the metrics in the long term, it stores 6 hours of metrics by default. If you already have a Prometheus in your cluster to store application metrics or if you use another solution, like Data Dog, you should configure them to fetch metrics from the Linkerd&rsquo;s Prometheus.</p><h2>Grafana:</h2><p>The Grafana component is used to display many dashboards about the metrics of your &ldquo;meshed&rdquo; services:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"437\" ac:original-width=\"700\"><ri:attachment ri:filename=\"1*eLA79d00AEZTXJppMe6v8w.png\" ri:version-at-save=\"1\" /></ac:image><p>These dashboards are reachable via links from the Web component.</p><p /><p>Ref - <a href=\"http://linkerd.io\" data-card-appearance=\"inline\">http://linkerd.io</a>  </p><p />",
      "approach_used": "endpoint_2",
      "word_count": 750
    },
    {
      "id": "2363588611",
      "title": "2022-02-08 CI/CD v2 re-architecture",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2363588611",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2363588611/2022-02-08+CI+CD+v2+re-architecture",
      "created": "2022-02-11T05:37:58.475Z",
      "content": "<h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":calendar_spiral:\" ac:emoji-id=\"1f5d3\" ac:emoji-fallback=\"\\uD83D\\uDDD3\" />&nbsp;Date</h2><p><time datetime=\"2022-02-08\" /></p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":busts_in_silhouette:\" ac:emoji-id=\"1f465\" ac:emoji-fallback=\"\\uD83D\\uDC65\" />&nbsp;Participants</h2><p><ac:placeholder>List meeting participants using their @ mention names</ac:placeholder></p><ul><li><p><code>abhinav.gupta@meesho.com, Adarsha Das &lt;adarsha.das@meesho.com&gt;, ajay.mishra@meesho.com, alok.sharma@meesho.com, Aman Srivastava &lt;aman.srivastava@meesho.com&gt;, anand.jain@meesho.com, ankush.tanwar@meesho.com, Arijit Bhattacharyya &lt;arijit.bhattacharyya@meesho.com&gt;, Asha Ashok &lt;asha.ashok@meesho.com&gt;, ashish.asawa@meesho.com, ashish.rawat@meesho.com, Ayush Garg &lt;ayush.garg@meesho.com&gt;, binny.tewani@meesho.com, chetan.kalyan@meesho.com, debashis.mukherjee@meesho.com, Indroneel Das &lt;indroneel.das@meesho.com&gt;, ismail.mohideen@meesho.com, Kandhavelu Muthuvelan &lt;kandhavelu.muthuvelan@meesho.com&gt;, katreddi.kiran@meesho.com, kumar.pratik@meesho.com, Malkit Singh &lt;malkit.singh@meesho.com&gt;, Manirama Kothapall &lt;manirama.kothapall@meesho.com&gt;, naveen.av@meesho.com, navneet@meesho.com, nelaturi.alexander@meesho.com, nithya.shree@meesho.com, pavan@meesho.com, prateek.sharma@meesho.com, praveen.nagarajan@meesho.com, priyanka.verma@meesho.com, Raju Gupta &lt;raju.gupta@meesho.com&gt;, ravindra@meesho.com, samirranjan@meesho.com, sanjeev@meesho.com, sheetal.sharma@meesho.com, shobhit.agarwal@meesho.com, Shubham Sharma &lt;shubhamsharma@meesho.com&gt;, Siddharth Gupta &lt;siddharth.g@meesho.com&gt;, Sidhartha Kumar &lt;sidhartha.kumar@meesho.com&gt;, Sreejith Vijayendran &lt;sreejith.vijayendran@meesho.com&gt;, srikanth.ramachandran@meesho.com, sundaresan.a@meesho.com, vinit.rongata@meesho.com</code></p></li></ul><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":goal:\" ac:emoji-id=\"1f945\" ac:emoji-fallback=\"\\uD83E\\uDD45\" />&nbsp;Goals</h2><p>Overview of CI/CD v2 re-architecture</p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":speaking_head:\" ac:emoji-id=\"1f5e3\" ac:emoji-fallback=\"\\uD83D\\uDDE3\" />&nbsp;Discussion topics</h2><table data-layout=\"wide\" ac:local-id=\"c34b7a50-a83e-4471-b6cb-6b6c71ea59e6\"><colgroup><col style=\"width: 120.0px;\" /><col style=\"width: 107.0px;\" /><col style=\"width: 360.0px;\" /><col style=\"width: 165.0px;\" /><col style=\"width: 73.0px;\" /><col style=\"width: 135.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"#deebff\"><p><strong>Item</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Presenter</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Notes</strong></p></th><th><p><strong>Topic</strong></p></th><th><p><strong>Yes/No</strong></p></th><th><p><strong>Reason</strong></p></th></tr><tr><td><p>Meeting Agenda</p></td><td><p><ac:link><ri:user ri:userkey=\"8a7f808a7a5ca3fb017a600088bb05d0\" /></ac:link> </p></td><td><ul><li><p>Explained the planned enhancement for CICD v2 </p></li><li><p>As we are moving towards containerising the services and moving them to Kubernetes, the CICD v2 efforts are focused on integrating and deploying the containerised applications and services to Kubernetes cluster</p></li><li><p>There won&rsquo;t be major enhancement for the non-containerised application and services; the existing deployment model will be supported for non-containerised deployments</p></li></ul></td><td><p>Enhancing the existing EC2 based deployment</p></td><td><p>No</p></td><td><p>We are moving towards containerising the services and moving them to Kubernetes, hence focusing our efforts on container based deployments</p></td></tr><tr><td><p>CI process<br /></p></td><td><p><ac:link><ri:user ri:userkey=\"8a7f808a7de359a0017de5bf87ab0055\" /></ac:link> </p></td><td><ul><li><p>Proposed CI workflow for the organisation</p></li><li><p>There are different unit testing framework available and the pipeline should be configurable to accommodate these changes</p></li><li><p>There were concerns regarding code coverage, the details are as follows:</p><ul><li><p>Pipeline should be able support code coverage for both the existing and newly added code</p></li><li><p>Pipeline should be configurable to block the merge on the bases of code coverage</p></li><li><p>Provision for configuration of tool in CI pipeline</p></li></ul></li><li><p>Overriding certain stages for hot fixes</p></li><li><p>The master branch is proposed as an example, for the workflow automatic triggering will be supported for trusted target branches</p></li><li><p>Pushing artifacts to s3/artifactory might not be needed for every commit in the master/target branch, the pipeline should be able to distinguish  these changes, for example based on version, when the version changes in master/target branch then push the artifacts to s3/artifactory</p></li><li><p>Defining and meeting SLA&rsquo;s for different stages in the proposed pipeline and ensuring that the CI process should not take much time</p></li><li><p>Running stages and steps in parallel wherever needed to optimise the run time of CI pipeline</p></li><li><p>Propose workflow for QA teams, define integration testing <br /></p></li></ul></td><td><p>Support for different unit testing framework<br /><br />Code coverage based merge blocking<br /><br />Code coverage based on both existing and newly added code<br /><br />Overriding certain stages for hot fixes<br /><br />Running certain stages in parallel to optimise the pipeline time<br /><br />Manually pushing build artifacts<br /><br /><br /><br /><br /><br />Defining SLA&rsquo;s for pipelines<br /><br /></p></td><td><p>Yes<br /><br /><br /><br />Yes<br /><br /><br /><br />Yes<br /><br /><br /><br /><br />Yes<br /><br /><br />Yes<br /><br /><br /><br /><br />No<br /><br /><br /><br /><br /><br /><br />Yes</p></td><td><p><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />Manual steps may block the continuous integration and deployment workflows</p></td></tr></tbody></table><p>Meeting Recording: <a href=\"https://drive.google.com/file/d/1yOcogVvIQlXoSPqOpbD8QTfPlpy5IXfK/view\" data-card-appearance=\"inline\">https://drive.google.com/file/d/1yOcogVvIQlXoSPqOpbD8QTfPlpy5IXfK/view</a> </p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":white_check_mark:\" ac:emoji-id=\"2705\" ac:emoji-fallback=\"✅\" />&nbsp;Action items</h2><p><ac:placeholder>Add action items to close the loop on open questions or discussion topics:</ac:placeholder></p><ac:task-list>\n<ac:task>\n<ac:task-id>73</ac:task-id>\n<ac:task-status>incomplete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Look into QA workflow with respect to the CICD process</span></ac:task-body>\n</ac:task>\n</ac:task-list><p /><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":arrow_heading_up:\" ac:emoji-id=\"2934\" ac:emoji-fallback=\"⤴\" />&nbsp;Decisions</h2><p><ac:placeholder>Type /decision to record the decisions you make in this meeting:</ac:placeholder></p><ac:adf-extension><ac:adf-node type=\"decision-list\"><ac:adf-attribute key=\"local-id\">7b7f2852-ecad-4b64-84ae-f9217e7e08fe</ac:adf-attribute><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-attribute key=\"local-id\">befae5ab-f0fd-4138-9165-e04b1d852ca1</ac:adf-attribute><ac:adf-content>Decide on infra and tools and show a Proof Of Concept</ac:adf-content></ac:adf-node></ac:adf-node><ac:adf-fallback><ul class=\"decision-list\"><li>Decide on infra and tools and show a Proof Of Concept</li></ul></ac:adf-fallback></ac:adf-extension>",
      "approach_used": "endpoint_2",
      "word_count": 588
    },
    {
      "id": "2365850351",
      "title": "2022-02-11 CI/CD v2 re-architecture (CD)2/2",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2365850351",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2365850351/2022-02-11+CI+CD+v2+re-architecture+CD+2+2",
      "created": "2022-02-11T08:38:31.053Z",
      "content": "<h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":calendar_spiral:\" ac:emoji-id=\"1f5d3\" ac:emoji-fallback=\"\\uD83D\\uDDD3\" />&nbsp;Date</h2><p><time datetime=\"2022-02-11\" /></p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":busts_in_silhouette:\" ac:emoji-id=\"1f465\" ac:emoji-fallback=\"\\uD83D\\uDC65\" />&nbsp;Participants</h2><p><ac:placeholder>List meeting participants using their @ mention names</ac:placeholder></p><ul><li><p><code>abhinav.gupta@meesho.com, Adarsha Das &lt;adarsha.das@meesho.com&gt;, ajay.mishra@meesho.com, alok.sharma@meesho.com, Aman Srivastava &lt;aman.srivastava@meesho.com&gt;, anand.jain@meesho.com, ankush.tanwar@meesho.com, Arijit Bhattacharyya &lt;arijit.bhattacharyya@meesho.com&gt;, Asha Ashok &lt;asha.ashok@meesho.com&gt;, ashish.asawa@meesho.com, ashish.rawat@meesho.com, Ayush Garg &lt;ayush.garg@meesho.com&gt;, binny.tewani@meesho.com, chetan.kalyan@meesho.com, debashis.mukherjee@meesho.com, Indroneel Das &lt;indroneel.das@meesho.com&gt;, ismail.mohideen@meesho.com, Kandhavelu Muthuvelan &lt;kandhavelu.muthuvelan@meesho.com&gt;, katreddi.kiran@meesho.com, kumar.pratik@meesho.com, Malkit Singh &lt;malkit.singh@meesho.com&gt;, Manirama Kothapall &lt;manirama.kothapall@meesho.com&gt;, naveen.av@meesho.com, navneet@meesho.com, nelaturi.alexander@meesho.com, nithya.shree@meesho.com, pavan@meesho.com, prateek.sharma@meesho.com, praveen.nagarajan@meesho.com, priyanka.verma@meesho.com, Raju Gupta &lt;raju.gupta@meesho.com&gt;, ravindra@meesho.com, samirranjan@meesho.com, sanjeev@meesho.com, sheetal.sharma@meesho.com, shobhit.agarwal@meesho.com, Shubham Sharma &lt;shubhamsharma@meesho.com&gt;, Siddharth Gupta &lt;siddharth.g@meesho.com&gt;, Sidhartha Kumar &lt;sidhartha.kumar@meesho.com&gt;, Sreejith Vijayendran &lt;sreejith.vijayendran@meesho.com&gt;, srikanth.ramachandran@meesho.com, sundaresan.a@meesho.com, vinit.rongata@meesho.com</code> </p></li></ul><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":goal:\" ac:emoji-id=\"1f945\" ac:emoji-fallback=\"\\uD83E\\uDD45\" />&nbsp;Goals</h2><p>Overview of CI/CD v2 re-architecture</p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":speaking_head:\" ac:emoji-id=\"1f5e3\" ac:emoji-fallback=\"\\uD83D\\uDDE3\" />&nbsp;Discussion topics</h2><table data-layout=\"wide\" ac:local-id=\"c34b7a50-a83e-4471-b6cb-6b6c71ea59e6\"><colgroup><col style=\"width: 120.0px;\" /><col style=\"width: 107.0px;\" /><col style=\"width: 360.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"#deebff\"><p><strong>Item</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Presenter</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Notes</strong></p></th></tr><tr><td><p>CD Process</p></td><td><p><ac:link><ri:user ri:userkey=\"8a7f808a7812f8a5017817483600022a\" /></ac:link> </p></td><td><ul><li><p>Presented different Deployment strategies</p><ul><li><p>Canary Deployment</p><ul><li><p>strategy</p></li><li><p>templating for canary tests</p></li><li><p>integration with different metrics providers</p></li></ul></li><li><p>Rolling Deployment</p></li><li><p>Blue Green Deployment</p></li></ul></li><li><p>Clear details of canary deployment regarding stickiness, basically if a A/B testing kind of setup is possible, and if not then devs need to know that canary is basically only for stateless apps. Right now A/B testing is done from app side, and not from CICD side.</p></li><li><p>A guideline doc on which deployment strategy to use for which purpose.</p></li></ul></td></tr></tbody></table><p>Meeting Recording: <a href=\"https://drive.google.com/file/d/1OWrzuDhf3OhlADT-S54avAISWr1KYmcY/view?usp=drive_web\" data-card-appearance=\"inline\">https://drive.google.com/file/d/1OWrzuDhf3OhlADT-S54avAISWr1KYmcY/view?usp=drive_web</a> </p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":white_check_mark:\" ac:emoji-id=\"2705\" ac:emoji-fallback=\"✅\" />&nbsp;Action items</h2><p><ac:placeholder>Add action items to close the loop on open questions or discussion topics:</ac:placeholder></p><ul><li><p>A guideline doc on which deployment strategy to use for which purpose.</p></li></ul>",
      "approach_used": "endpoint_2",
      "word_count": 229
    },
    {
      "id": "2366046290",
      "title": "WS/BSS Meeting 09/02/2022",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2366046290",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2366046290/WS+BSS+Meeting+09+02+2022",
      "created": "2022-02-10T04:44:45.938Z",
      "content": "<ol><li><p>The discussion happened on the Cluster structuring approach. We need to come up with more structured manners. based on</p><ol><li><p>Benchmark with B2C companies</p></li><li><p>Limitation on clusters or number of applications per cluster.</p></li><li><p>Design frameworks that can then be compared and decided on Other points.</p></li></ol></li><li><p>We discussed service mesh, like how our existing application resilience can be there if we use service mesh. We need to discuss service mesh more to decide if we want a service mesh. (By <ac:link><ri:user ri:userkey=\"8a7f808a7c148ed5017c174adb2c0f19\" /></ac:link> )</p></li><li><p><ac:link><ri:user ri:userkey=\"8a7f808a7c148ed5017c174adb2c0f19\" /></ac:link>  wanted to debate on Config management for app and infra and security.</p></li><li><p>The discussion happened on AMI-based deployments, like how it will be a decision point to go to Kubernetes.</p></li><li><p>Need comparison on current latency and after moving to Kubernetes. (Perf needs to do that).</p></li></ol><p>Open questions:</p><ol><li><p>Are we going to use service mesh or not?</p></li><li><p>Can we have a backup cluster for P0 services?</p></li></ol><p />",
      "approach_used": "endpoint_2",
      "word_count": 141
    },
    {
      "id": "2366505047",
      "title": "WS/BSS Meeting Notes",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2366505047",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2366505047/WS+BSS+Meeting+Notes",
      "created": "2022-02-10T04:45:38.542Z",
      "content": "<ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"9db8e7c3-2d8e-4e41-b049-633cc1b0bf07\" ac:macro-id=\"02fb2117-4a86-4006-a12a-f02c42e00f46\" />",
      "approach_used": "endpoint_2",
      "word_count": 7
    },
    {
      "id": "2369323083",
      "title": "2022-02-08 Kubernetes@Meesho",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2369323083",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2369323083/2022-02-08+Kubernetes+Meesho",
      "created": "2022-02-13T10:27:09.924Z",
      "content": "<h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":calendar_spiral:\" ac:emoji-id=\"1f5d3\" ac:emoji-fallback=\"\\uD83D\\uDDD3\" />&nbsp;Date</h2><p><time datetime=\"2022-02-08\" /></p><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":busts_in_silhouette:\" ac:emoji-id=\"1f465\" ac:emoji-fallback=\"\\uD83D\\uDC65\" />&nbsp;Participants</h2><p><ac:placeholder>List meeting participants using their @ mention names</ac:placeholder></p><ul><li><p><ac:link><ri:user ri:userkey=\"8a7f808a7c4a60df017c4bd26baa0439\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f80896ff7087701700aa42d080874\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7c148ed5017c174adb2c0f19\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7de359a0017de5bf87ab0055\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7b2fb923017b348ca3920831\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a79b553160179c6d39440059e\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808565cb41b70165cdabac180039\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808668e839f30168f0218118019d\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808565cb41b70165cdabac17002c\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a77a6d4000177ab8e71a102f8\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f8086690590d701690a8caf5801ac\" /></ac:link> <ac:link><ri:user ri:userkey=\"8a7f808a7e78936f017e8af226f2046d\" /></ac:link> </p></li></ul><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":goal:\" ac:emoji-id=\"1f945\" ac:emoji-fallback=\"\\uD83E\\uDD45\" />&nbsp;Goals</h2><p><ac:placeholder>List goals for this meeting (e.g., Set design priorities for FY19)</ac:placeholder></p><ul><li><p>Overview of Kubernetes adoption</p></li></ul><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":speaking_head:\" ac:emoji-id=\"1f5e3\" ac:emoji-fallback=\"\\uD83D\\uDDE3\" />&nbsp;Discussion topics</h2><table data-layout=\"wide\" ac:local-id=\"8660b18c-67ec-4690-83ac-94688d5b5f1c\"><colgroup><col style=\"width: 109.0px;\" /><col style=\"width: 107.0px;\" /><col style=\"width: 139.0px;\" /><col style=\"width: 299.0px;\" /><col style=\"width: 306.0px;\" /></colgroup><tbody><tr><th data-highlight-colour=\"#deebff\"><p><strong>Item</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Presenter</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Topic</strong></p></th><th data-highlight-colour=\"#deebff\"><p><strong>Notes</strong></p></th><th><p><strong>Owner and AI</strong></p></th></tr><tr><td rowspan=\"6\"><p>K8s overview</p><p><br /></p></td><td rowspan=\"6\"><p><ac:link><ri:user ri:userkey=\"8a7f808a7c4a60df017c4bd26baa0439\" /></ac:link> </p><p><br /></p></td><td><p>Config Management</p></td><td><p>We discussed briefly on need and implementing config management for app and infra</p></td><td><p>Owner: DevOps</p><p>AI: </p><ul><li><p>Set focussed discussion with relevant stakeholders</p></li><li><p>Go deeper on the need of config management</p></li><li><p>How to implement the same for both app and infra management</p></li><li><p>Once decided on above points, do PoC and come up with comparison on different tools</p></li></ul></td></tr><tr><td><p><br />Service Mesh</p></td><td><p>We discussed service mesh in deep. How our existing application resilience can still be used/implemented if we use service mesh. If introducing service mesh is going to introduce more latency across the system, make it further complex etc.<br /></p></td><td><p>Owner: DevOps</p><p>AI: </p><ul><li><p>Set focussed discussion with relevant stakeholders on this</p></li><li><p>To decide if we want a service mesh</p></li><li><p>How to implement it together with the current resilience system</p></li><li><p>Devops to do PoC with different leading tools and come up with points to be discussed and learnings</p></li></ul></td></tr><tr><td><p>Security</p></td><td><p>Briefly discussed on security implementation on the application and infrastructure on kubernetes</p></td><td><p>Owner: DevOps &amp; Security</p></td></tr><tr><td><p>Multi-Cluster</p></td><td><p>Discussed on having multiple clusters to accommodate and run the entire application infrastructure. Discussed the need for having multiple clusters.<br />Should we have clusters segregated on application priority, number of applications in a cluster, BU or any other reasons.</p></td><td><p>Owner: Devops</p><p>AI: </p><ul><li><p>Set focussed discussion with relevant stakeholders on this once we have the data on below</p></li><li><p>Benchmark with peer B2C companies</p></li><li><p>Document learnings, reasons and come up with a model</p></li><li><p>Design frameworks that can then be compared and decided on other points</p></li><li><p>Can/should we have backup clusters for P0 clusters</p></li></ul></td></tr><tr><td><p>Latency</p></td><td><p>Perf needs to be done to check, compare latency of the applications on the current infra vis-a-vis on the K8s infra</p><ul><li><p>Latency of public calls</p></li><li><p>Latency of S2S internal API calls </p></li></ul></td><td><p>Owner: DevOps &amp; Perf Team<br />AI:</p><ul><li><p>Set focussed discussion on this and come up with below</p></li><li><p>How to check, design and perform performance test on both infra</p></li></ul></td></tr><tr><td><p>Engineering team</p></td><td><p>We discussed about the learning curve, complexity of the new system and load on them to adopt to this.</p></td><td><p>Owner: DevOps</p><p>AI:</p><ul><li><p>Have further discussion with leaders to understand the pain points and other concerns</p></li><li><p>Better structure the expectations on each team with ownerships</p></li></ul></td></tr></tbody></table><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":white_check_mark:\" ac:emoji-id=\"2705\" ac:emoji-fallback=\"✅\" />&nbsp;Action items</h2><p><ac:placeholder>Add action items to close the loop on open questions or discussion topics:</ac:placeholder></p><ac:task-list>\n<ac:task>\n<ac:task-id>80</ac:task-id>\n<ac:task-status>incomplete</ac:task-status>\n\n</ac:task>\n</ac:task-list><h2><ac:emoticon ac:name=\"blue-star\" ac:emoji-shortname=\":arrow_heading_up:\" ac:emoji-id=\"2934\" ac:emoji-fallback=\"⤴\" />&nbsp;Decisions</h2><p><ac:placeholder>Type /decision to record the decisions you make in this meeting:</ac:placeholder></p><ac:adf-extension><ac:adf-node type=\"decision-list\"><ac:adf-attribute key=\"local-id\">60ae6eac-c97f-42c9-b53d-8693e3f3511a</ac:adf-attribute><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-attribute key=\"local-id\">2780a514-d545-42de-af37-17027df1eaf9</ac:adf-attribute><ac:adf-content>Set focussed discussions and brainstorming sessions on each of the above topics with relevant stakeholders and come up with a plan.</ac:adf-content></ac:adf-node><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"local-id\">2ec939ed-93b0-438d-8f64-9bf5ec828e92</ac:adf-attribute><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-content>Set further discussion points with Engineering leaders on look at concern points such as learning curve of the team, complexity of the new system and load on them to adopt this</ac:adf-content></ac:adf-node><ac:adf-node type=\"decision-item\"><ac:adf-attribute key=\"local-id\">e704ef99-3fea-40d0-a8e5-cc2256dcd03f</ac:adf-attribute><ac:adf-attribute key=\"state\">DECIDED</ac:adf-attribute><ac:adf-content>Evangelise containerisation</ac:adf-content></ac:adf-node></ac:adf-node><ac:adf-fallback><ul class=\"decision-list\"><li>Set focussed discussions and brainstorming sessions on each of the above topics with relevant stakeholders and come up with a plan.</li><li>Set further discussion points with Engineering leaders on look at concern points such as learning curve of the team, complexity of the new system and load on them to adopt this</li><li>Evangelise containerisation</li></ul></ac:adf-fallback></ac:adf-extension>",
      "approach_used": "endpoint_2",
      "word_count": 587
    },
    {
      "id": "2377777162",
      "title": "Rancher POC",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2377777162",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2377777162/Rancher+POC",
      "created": "2022-02-18T10:29:56.045Z",
      "content": "<p><a href=\"https://rancher.meeshotest.in/dashboard/c/c-42vsf/explorer#cluster-metrics\" data-card-appearance=\"inline\">https://rancher.meeshotest.in/dashboard/c/c-42vsf/explorer#cluster-metrics</a> </p><p>Rancher is a container management platform built for organisations that deploy containers in production. Rancher makes it easy to run Kubernetes everywhere, meet IT requirements, and empower DevOps teams.</p><p>We have the choice of creating Kubernetes clusters with Rancher Kubernetes Engine (RKE) or cloud Kubernetes services, such as GKE, AKS, and EKS. We can also import and manage our existing Kubernetes clusters created using any Kubernetes distribution or installer.</p><p>Rancher supports centralised authentication, access control, and monitoring for all Kubernetes clusters under its control. For example, we can:</p><ul><li><p>Use our google Directory credentials to access Kubernetes clusters hosted by cloud vendors, such as GKE,EKS,AKS or on-prem Kubernetes.</p></li><li><p>Setup and enforce access control and security policies across all users, groups, projects, clusters, and clouds.</p></li><li><p>View the health and capacity of our Kubernetes clusters from a single-pane-of-glass.<br /><br /></p></li></ul><p><strong>Rancher for Kubernetes:</strong></p><p><strong>Provisioning Kubernetes clusters:</strong> The Rancher API server can provision Kubernetes on existing nodes, or perform Kubernetes upgrades.</p><p><br /><strong>Managing projects:</strong> A project is a group of multiple namespaces and access control policies within a cluster. A project is a Rancher concept, not a Kubernetes concept, which allows us to manage multiple namespaces as a group and perform Kubernetes operations in them. The Rancher UI provides features for project administration and for managing applications within projects.</p><p><br /><strong>Pipelines:</strong> Setting up a pipeline can help developers deliver new software as quickly and efficiently as possible. Within Rancher, we can configure pipelines for each of our Rancher projects.</p><p><br /><strong>Istio:</strong> Our integration with Istio is designed so that a Rancher operator, such as an administrator or cluster owner, can deliver Istio to developers. Then developers can use Istio to enforce security policies, troubleshoot problems, or manage traffic for green/blue deployments, canary deployments, or A/B testing.</p><p><strong>Cluster Visibility</strong></p><p><strong>Logging:</strong> Rancher can integrate with a variety of popular logging services and tools that exist outside of our Kubernetes clusters like fluentd etc.<br /><strong>Monitoring:</strong> Using Rancher, we can monitor the state and processes of our cluster nodes, Kubernetes components, and software deployments through integration with Prometheus.<br /><strong>Alerting:</strong> To keep our clusters and applications healthy and driving your organisational productivity forward, you need to stay informed of events occurring in your clusters and projects, both planned and unplanned.</p><p /><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"245\" ac:original-width=\"791\"><ri:attachment ri:filename=\"image-20220218-095115.png\" ri:version-at-save=\"1\" /></ac:image><p>Actions which we can perform using Rancher:</p><ul><li><p>Using kubectl and a kubeconfig file to Access a Cluster</p></li><li><p>Managing Cluster Members</p></li><li><p>Editing and Upgrading Clusters</p></li><li><p>Managing Nodes</p></li><li><p>Managing Persistent Volumes and Storage Classes</p></li><li><p>Managing Projects, Namespaces and Workloads</p></li><li><p>Configuring Tools (Alerts, Notifiers, Monitoring, Logging, Istio)</p></li><li><p>Running Security Scans</p></li><li><p>Use existing configuration to create additional clusters</p></li><li><p>Ability to rotate certificates</p></li><li><p>Ability to backup and restore Rancher-launched clusters</p></li><li><p>Configuring Pod Security Policies<br /><br /><br /><strong>Rancher cluster:</strong></p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"875\" ac:original-width=\"1749\"><ri:attachment ri:filename=\"image-20220218-100558.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>POD monitoring:</strong></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"875\" ac:original-width=\"1749\"><ri:attachment ri:filename=\"image-20220218-100623.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>POD logs:</strong></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"875\" ac:original-width=\"1749\"><ri:attachment ri:filename=\"image-20220218-100739.png\" ri:version-at-save=\"1\" /></ac:image><p>We can directly login pods using Rancher for debugging:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"667\" ac:original-width=\"1735\"><ri:attachment ri:filename=\"image-20220218-101938.png\" ri:version-at-save=\"1\" /></ac:image><p />",
      "approach_used": "endpoint_2",
      "word_count": 470
    },
    {
      "id": "2382725146",
      "title": "Ingress Controller - k8's",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2382725146",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2382725146/Ingress+Controller+-+k8+s",
      "created": "2022-02-24T04:47:34.028Z",
      "content": "<h2>Introduction</h2><p>An <strong>Ingress controller</strong> is a specialized load balancer for Kubernetes (and other containerized) environments. <a href=\"https://www.nginx.com/resources/glossary/kubernetes\">Kubernetes</a> is the de facto standard for managing containerized applications. For many enterprises, moving production workloads into Kubernetes brings additional challenges and complexities around application traffic management. An Ingress controller abstracts away the complexity of Kubernetes application traffic routing and provides a bridge between Kubernetes services and external ones.</p><p>Kubernetes Ingress controllers:</p><ul><li><p>Accept traffic from outside the Kubernetes platform, and load balance it to pods (containers) running inside the platform</p></li><li><p>Can manage egress traffic within a cluster for services which need to communicate with other services outside of a cluster</p></li><li><p>Are configured using the Kubernetes API to deploy objects called &ldquo;Ingress Resources&rdquo;</p></li><li><p>Monitor the pods running in Kubernetes and automatically update the load‑balancing rules when pods are added or removed from a service</p></li></ul><p>With NGINX Ingress Controller you harness Kubernetes networking on Layers 4 through 7, to enable tighter security and traffic control among Kubernetes services.</p><h2>Why do I need a load balancer in front of an ingress?</h2><p>Ingress is tightly integrated into Kubernetes, meaning that your existing workflows around kubectl will likely extend nicely to managing ingress. An Ingress controller does not typically eliminate the need for an external load balancer , it simply adds an additional layer of routing and control behind the load balancer.</p><p>Pods and nodes are not guaranteed to live for the whole lifetime that the user intends: pods are ephemeral and vulnerable to kill signals from Kubernetes during occasions such as:</p><ul><li><p>Scaling.</p></li><li><p>Memory or CPU saturation.</p></li><li><p>Rescheduling for more efficient resource use.</p></li><li><p>Downtime due to outside factors.</p></li></ul><p /><h2 style=\"text-align: center;\"><strong>Ingress With load balancer </strong></h2><p><br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"484\" ac:original-width=\"482\"><ri:attachment ri:filename=\"aA14Km9IE7k7fVKnWgas-OfLeuv2Lis9h0pqMr2JIWOCZ_Sa4BncZW2VgaYAMI-bSfOkTaXpc1NVk_VBvVi9LXlVcZl-rmymLXX0nMc5akgt0dFlaz9_YipospmH1gyErFmNNpoP\" ri:version-at-save=\"1\" /></ac:image><p><br /></p><h2>Nginx ingress controller</h2><p><strong>Features</strong></p><ul><li><p>Traffic routing can be controlled by annotations</p></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"7321ae3f-315b-480a-890b-536ec3869f2c\"><ac:parameter ac:name=\"title\">Snippet</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"a90ce2b0-c03c-4514-a773-6eaebbf00b59\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/rewrite-target: /]]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><ul><li><p>It supports canary</p><ul><li><p>Sending a small number of requests to a different service than the production service).The canary annotation enables the Ingress spec to act as an alternative service for requests to route to depending on the rules applied.</p></li></ul></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"32b51791-2edc-4772-85f4-7b680b92c588\"><ac:parameter ac:name=\"title\">Snippet</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"6b5b08d1-b978-48a0-b097-c5723cbd3d70\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"30\"]]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><ul><li><p>It supports basic authentication</p><ul><li><p>It is possible to add authentication by adding additional annotations in the Ingress rule. The source of the authentication is a secret that contains usernames and passwords.</p></li></ul></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"e09e3e85-7d09-49d4-9822-19bb95fffe71\"><ac:parameter ac:name=\"title\">Snippet</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"0814c2c8-f85e-460e-a57d-08b1d18d2067\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[  annotations:\n    # type of authentication\n    nginx.ingress.kubernetes.io/auth-type: basic\n    # name of the secret that contains the user/password definitions\n    nginx.ingress.kubernetes.io/auth-secret: basic-auth\n    # message to display with an appropriate context why the authentication is required\n    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - foo']]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><ul><li><p>It supports local and global rate limiting</p><ul><li><p>These annotations define limits on connections and transmission rates. These can be used to mitigate <a href=\"https://www.nginx.com/blog/mitigating-ddos-attacks-with-nginx-and-nginx-plus\">DDoS Attacks</a>.</p></li></ul></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"b3285d49-3a67-468a-aa56-50c699de7702\"><ac:parameter ac:name=\"title\">Snippet</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"3533fcc8-2a41-4bcf-9b02-f1d409dc9e9d\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[  annotations:\n    nginx.ingress.kubernetes.io/limit-rps: \"5\"\n    nginx.ingress.kubernetes.io/limit-rpm: \"300\"]]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><ul><li><p>It supports mirroring&nbsp;</p><ul><li><p>Enables a request to be mirrored to a mirror backend. Responses by mirror backends are ignored. This feature is useful, to see how requests will react in &quot;test&quot; backends.</p></li></ul></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"383ac17c-5ddc-4de3-ae02-f694459a5f86\"><ac:parameter ac:name=\"title\">Snippet</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"b24f9c43-3db5-4b91-af48-7bf0c1225eea\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[  annotations:\n     nginx.ingress.kubernetes.io/mirror-target: https://test.env.com/$request_uri]]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><p /><p><strong>Observability</strong></p><ul><li><p>It supports metrics (prometheus) with simple pod annotations. <a href=\"https://kubernetes.github.io/ingress-nginx/user-guide/monitoring/\">Link</a></p></li><li><p>It supports open tracing globally or ingress object level. <a href=\"https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/\">Link</a></p></li></ul><h2>Aws LoadBalancer Controller</h2><p><strong>Features</strong></p><ul><li><p>Traffic routing can be controlled by annotations</p></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"fe94cf1c-e16e-4705-94d8-6fa1824e1647\"><ac:parameter ac:name=\"title\">Snippet</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"b8e317da-b4f9-4ec0-8a99-9904cceb0e2f\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: \"<name>\"\n  namespace: \"<namespace>\"\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internal | internet facing\n    alb.ingress.kubernetes.io/subnets: <subnet_ids>\n    alb.ingress.kubernetes.io/security-groups: <sg_ids>\n    alb.ingress.kubernetes.io/certificate-arn: <acm_cert>\n    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-1-2017-01\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/actions.ssl-redirect: '{\"Type\": \"redirect\", \"RedirectConfig\": { \"Protocol\": \"HTTPS\", \"Port\": \"443\", \"StatusCode\": \"HTTP_301\"}}'\n    \n  labels:\n    app: <if_any>\n\nspec:\n  rules:\n    - host: <host_name>\n      http:\n        paths:\n          - path: /*\n            backend:\n              serviceName: ssl-redirect\n              servicePort: use-annotation]]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><ul><li><p>It supports weighted targeting</p></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"28ed7854-9a6e-4315-a75b-625c4d6cbd2a\"><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"e9f90ad8-095c-423f-bdef-fb8f2978653d\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/actions.response-503: >\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"503\",\"messageBody\":\"503 error text\"}}\n    alb.ingress.kubernetes.io/actions.redirect-to-eks: >\n      {\"type\":\"redirect\",\"redirectConfig\":{\"host\":\"aws.amazon.com\",\"path\":\"/eks/\",\"port\":\"443\",\"protocol\":\"HTTPS\",\"query\":\"k=v\",\"statusCode\":\"HTTP_302\"}}\n    alb.ingress.kubernetes.io/actions.forward-single-tg: >\n      {\"type\":\"forward\",\"targetGroupARN\": \"arn-of-your-target-group\"}\n    alb.ingress.kubernetes.io/actions.forward-multiple-tg: >\n      {\"type\":\"forward\",\"forwardConfig\":{\"targetGroups\":[{\"serviceName\":\"service-1\",\"servicePort\":\"http\",\"weight\":20},{\"serviceName\":\"service-2\",\"servicePort\":80,\"weight\":20},{\"targetGroupARN\":\"arn-of-your-non-k8s-target-group\",\"weight\":60}],\"targetGroupStickinessConfig\":{\"enabled\":true,\"durationSeconds\":200}}}\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /503\n            backend:\n              serviceName: response-503\n              servicePort: use-annotation\n          - path: /eks\n            backend:\n              serviceName: redirect-to-eks\n              servicePort: use-annotation\n          - path: /path1\n            backend:\n              serviceName: forward-single-tg\n              servicePort: use-annotation\n          - path: /path2\n            backend:\n              serviceName: forward-multiple-tg\n              servicePort: use-annotation]]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><ul><li><p>It supports incognito for authentication</p></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"b28e5b25-ba0e-4cab-ad55-93d8702fdd32\"><ac:parameter ac:name=\"title\">Snippet</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"875b4acb-2a79-4f16-9e04-fa1162a67869\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[  annotations:\n    kubernetes.io/ingress.class: alb\n    #specifies the authentication type on targets\n    alb.ingress.kubernetes.io/auth-type: cognito\n    #specifies the cognito idp configuration\n    alb.ingress.kubernetes.io/auth-idp-cognito: '{\"userPoolARN\":\"arn:aws:cognito-idp:us-west-2:xxx:userpool/xxx\",\"userPoolClientID\":\"my-clientID\",\"userPoolDomain\":\"my-domain\"}'\n    #specifies the oidc idp configuration. You need to create an secret within the same namespace as Ingress to hold your OIDC clientID and clientSecret.\n    alb.ingress.kubernetes.io/auth-idp-oidc: '{\"issuer\":\"https://example.com\",\"authorizationEndpoint\":\"https://authorization.example.com\",\"tokenEndpoint\":\"https://token.example.com\",\"userInfoEndpoint\":\"https://userinfo.example.com\",\"secretName\":\"my-k8s-secret\"}']]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><ul><li><p>It supports WAF and Aws Shield&nbsp;&nbsp;</p></li></ul><ac:structured-macro ac:name=\"expand\" ac:schema-version=\"1\" ac:macro-id=\"9b7c82e3-b39f-4401-9067-52351ac62b38\"><ac:parameter ac:name=\"title\">Snippet</ac:parameter><ac:rich-text-body><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"9f931725-793e-41d4-8dfa-c67e341546ed\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/waf-acl-id: <acl_id>\n    #If shield is to be enabled\n    alb.ingress.kubernetes.io/shield-advanced-protection: 'true']]></ac:plain-text-body></ac:structured-macro><p /></ac:rich-text-body></ac:structured-macro><p><strong>Observability </strong></p><ul><li><p>It supports metrics (prometheus) with simple pod annotations.</p></li><li><p>Tracing not supported.</p></li></ul><h2>Installation guide</h2><p><strong>Aws</strong></p><ul><li><p>Nginx with NLB. <a href=\"https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx\">Link</a></p></li><li><p>Nginx with ALB. <a href=\"https://www.padok.fr/en/blog/application-load-balancer-aws\">Link</a></p></li></ul><p><strong>Gcp&nbsp;</strong></p><ul><li><p>Nginx with NLB. <a href=\"https://cloud.google.com/community/tutorials/nginx-ingress-gke\">Link</a></p></li><li><p>Nginx with http(s) LB. <a href=\"https://docs.cloudbees.com/docs/cloudbees-ci/latest/cloud-setup-guide/gke-https-setup\">Link</a></p></li></ul><p>Performance with nginx. <a href=\"https://www.nginx.com/blog/testing-performance-nginx-ingress-controller-kubernetes/\">Link</a></p><p>Nginx also have production grade support. <a href=\"https://www.nginx.com/resources/datasheets/nginx-ingress-controller-kubernetes/\">Link</a> </p><h2>Conclusion</h2><p>The key advantage of using a cloud provider-specific Ingress Controller is native integration with other cloud services. As for ALB Ingress Controller, it creates an Application Load Balancer by default and integrates well with Route 53, Cognito, and AWS WAF.</p><p>On the other hand, if you are going for a hybrid or multi-cloud strategy, using an open-source option will be easier than maintaining multiple solutions per cloud provider.</p>",
      "approach_used": "endpoint_2",
      "word_count": 854
    },
    {
      "id": "2383282261",
      "title": "Cilium Working Guide",
      "type": "page",
      "status": "current",
      "is_runbook": true,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2383282261",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2383282261/Cilium+Working+Guide",
      "created": "2022-02-22T17:32:01.648Z",
      "content": "<h2>To install Cilium with enabled Hubble and prom in single cluster:</h2><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"a4e25675-be34-46ce-bdaf-5d108f0593dc\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[helm install cilium cilium/cilium --version 1.11.1 \\\n  --namespace kube-system \\\n  --set prometheus.enabled=true \\\n  --set operator.prometheus.enabled=true \\\n  --set cluster.name=<EKS Cluster Name> \\\n  --set cluster.id=<Cluster ID e.g 1> \\\n  --set eni.enabled=true \\\n  --set ipam.mode=eni \\\n  --set tunnel=disabled \\\n  --set kubeProxyReplacement=strict \\\n  --set k8sServiceHost=<EKS apiserver endpoint> \\\n  --set k8sServicePort=443 \\\n  --set localRedirectPolicy=true \\\n  --set installNoConntrackIptablesRules=true \\\n  --set bpf.masquerade=true \\\n  --set bandwidthManager=true \\\n  --set hubble.metrics.enabled=\"{dns:query;ignoreAAAA,drop,tcp,flow,icmp,http}\" \\\n  --set hubble.ui.enabled=true \\\n  --set hubble.relay.enabled=true]]></ac:plain-text-body></ac:structured-macro><h2>To install Cilium with enabled Hubble and prom in another cluster for cluster mesh:</h2><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"708b0fc3-47b5-45b3-8cfd-a6fd6f7a1a04\"><ac:parameter ac:name=\"language\">yaml</ac:parameter><ac:plain-text-body><![CDATA[helm install cilium cilium/cilium --version 1.11.1 \\\n  --namespace kube-system \\\n  --set prometheus.enabled=true \\\n  --set operator.prometheus.enabled=true \\\n  --set cluster.name=<EKS cluster name> \\\n  --set cluster.id=<Cluster id e.g. 2> \\\n  --set eni.enabled=true \\\n  --set ipam.mode=eni \\\n  --set tunnel=disabled \\\n  --set kubeProxyReplacement=strict \\\n  --set k8sServiceHost=<EKS api server endpoint> \\\n  --set k8sServicePort=443 \\\n  --set localRedirectPolicy=true \\\n  --set installNoConntrackIptablesRules=true \\\n  --set bpf.masquerade=true \\\n  --set bandwidthManager=true \\\n  --set hubble.metrics.enabled=\"{dns:query;ignoreAAAA,drop,tcp,flow,icmp,http}\" \\\n  --set hubble.ui.enabled=true \\\n  --set hubble.relay.enabled=true \\\n  --set egressMasqueradeInterfaces=eth+ \\\n  --set hubble.tls.ca.cert=\"<Hubble CA Certificate from cluster 1 secret>\" \\\n  --set hubble.tls.ca.key=\"<Hubble CA key from cluster 1 secret>\"]]></ac:plain-text-body></ac:structured-macro><h2>To Enable Cluster Mesh:</h2><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"189bcddf-8c5d-45e6-bfde-297d42c30265\"><ac:parameter ac:name=\"language\">bash</ac:parameter><ac:plain-text-body><![CDATA[cilium clustermesh enable --context $CLUSTER1\ncilium clustermesh enable --context $CLUSTER2]]></ac:plain-text-body></ac:structured-macro><h2>To connect Both Cluster:</h2><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"2bd99cf1-ce7a-4888-9daa-5da778c3dcb0\"><ac:parameter ac:name=\"language\">bash</ac:parameter><ac:plain-text-body><![CDATA[cilium clustermesh connect --context $CLUSTER1 --destination-context $CLUSTER2]]></ac:plain-text-body></ac:structured-macro><h2>To Enable l7 Visibility:</h2><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"5e4947df-d2f6-4b4f-a974-a7cba1368542\"><ac:parameter ac:name=\"language\">bash</ac:parameter><ac:plain-text-body><![CDATA[kubectl annotate pod <podname> -n <namespace> io.cilium.proxy-visibility=\"<Egress/8080/TCP/HTTP>\"]]></ac:plain-text-body></ac:structured-macro><h2>To Enable cilium tracing:</h2><p><a href=\"https://github.com/cilium/hubble-otel/blob/main/USER_GUIDE_KIND.md\" data-card-appearance=\"inline\">https://github.com/cilium/hubble-otel/blob/main/USER_GUIDE_KIND.md</a> </p><h2>Extra cilium_config</h2><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"8afc4e07-c6f4-4c3d-8638-85ce860e107d\"><ac:plain-text-body><![CDATA[  egress-masquerade-interfaces: eth+\n  aws-instance-limit-mapping: \"m6i.2xlarge=4,15,15\"]]></ac:plain-text-body></ac:structured-macro><h2>Demo Application</h2><p><a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/main/release/kubernetes-manifests.yaml\">https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/main/release/kubernetes-manifests.yaml</a></p><h2>Perf testing Report</h2><p><ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"PE\" ri:content-title=\"Cilium Latency POC\" ri:version-at-save=\"4\" /><ac:link-body>Cilium Latency POC</ac:link-body></ac:link> </p>",
      "approach_used": "endpoint_2",
      "word_count": 256
    },
    {
      "id": "2383675522",
      "title": "Kubernetes multi-cluster Framework",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2383675522",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2383675522/Kubernetes+multi-cluster+Framework",
      "created": "2022-02-23T11:32:33.870Z",
      "content": "<ac:structured-macro ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\" ac:local-id=\"0ed9fb3c-2b8d-408b-a0da-4aa6f4eecf61\" ac:macro-id=\"6340ce547781e0785febc5c2035c8d6f\"><ac:parameter ac:name=\"minLevel\">1</ac:parameter><ac:parameter ac:name=\"maxLevel\">7</ac:parameter></ac:structured-macro><h1>Why we can not use Single Shared Cluster</h1><p>We will be listing a few reasons here:</p><ol><li><p>Single point of failure.</p></li><li><p>Security Isolation.</p></li><li><p>Based on Kubernetes <a href=\"https://kubernetes.io/docs/setup/best-practices/cluster-large/\"><u>documentation</u></a>, The Cluster can't grow infinitely large: More specifically, Kubernetes designed to accommodate configurations that meet all of the following criteria:</p><ol><li><p>No more than 110 pods per node</p></li><li><p>No more than 5000 nodes</p></li><li><p>No more than 150000 total pods</p></li><li><p>No more than 300000 total containers</p></li></ol></li></ol><p>Based on the above point, we have to go for a multi-cluster. So now we have to decide what basis we have to separate our Kubernetes cluster. So we will have the following points to determine our multi-cluster approach.&nbsp;&nbsp;</p><ol><li><p>Efficient resource utilisation.</p></li><li><p><strong>Cost and visibility</strong>: Some tools in the market give us cost visibility based on application/namespace in Kubernetes. <a href=\"https://github.com/kubecost/cost-model\">kubecost</a> is one of them.&nbsp;</p></li><li><p><strong>Maintainability</strong>: This will be the same for all our multi-cluster approaches. It's good that the multi-cluster system will give us isolation based on whatever direction we will be going. Still, the multi-cluster process will increase complexity, increasing our maintenance cost.&nbsp;</p></li><li><p><strong>Security and Regulatory Compliance</strong>: It depends on segregating our Clusters based on compliance requirements like GDPR, ISO, PCI-DSS; we currently do not have PCI-DSS compliance requirements(based on a conversation with the Security Team). But we will be going for ISO soon.</p></li></ol><p>First, we need to separate env based multi-cluster like separate prod and non-prod env clusters. Now we need to have a multi-cluster in prod. Here are some approaches that we have and we will be evaluating.&nbsp;</p><ol><li><p><strong>We can have resource utilisation based clusters</strong>: This approach will help us utilise resources more efficiently; for example, We can have Compute Optimised workload in a group of Clusters and an Optimised memory workload in a group of Clusters. It will also help us in Cost Optimisation.&nbsp;</p></li><li><p><strong>We can have application priority-based clusters</strong>: It will help us isolate priority services and can have a backup plan for them. Resource and cost optimisation will not be that good because our priority workload can be in Compute and Memory Optimised.&nbsp;</p></li><li><p><strong>We can have Cluster based on BU/pod/team</strong>: This approach does not resource and cost-effective, but it will give isolation based on BU/pod/team. Also, it will provide BU-wide cost visibility directly from cloud.&nbsp;</p></li><li><p><strong>We can have a fixed service count per Cluster</strong>: It is also not resourceful and cost-effective. It will also not give you cost visibility based on BU. There are some tools that we use for cost visibility in Kubernetes.&nbsp;</p></li></ol><p>Also, we need to keep in mind that we have to fix our number of nodes per Cluster. </p><p><strong>The discussion happened with other companies</strong>: I have reached out to a few folks working on Kubernetes for a long time. The suggestion is first based on regularity and compliance, and then we can have a combination of resources, priority, and severity.</p><h1>How we will do multi cluster communication</h1><p>Now that we are talking about multi cluster, we need to also think about communications b/w clusters. Before jumping to how multi cluster communication happens, let's look at Kubernetes architecture.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"585\" ac:original-width=\"1252\"><ri:attachment ri:filename=\"4oDXlXlsXc6-3918jhNjTW7dgF8W0Q8pgG-N4fCn8QPjoF_Tl7Moy55qD7uRIzWcx-kFDOqvPvWHcE0fR7_snnCxrqQlVk1Vu4BDUSe2Lp1hvIRo55Ro5xJOPFQqybcnMh882ec0\" ri:version-at-save=\"1\" /></ac:image><p>As you can see, the Kubernetes Cluster creates an isolated environment. From the outside world to services sitting inside the Kubernetes cluster, we will not be able to access reason because Kubernetes will create logical services to communicate inside Cluster which will not be exposed to outside the Cluster. However, if we want to communicate any service from outside, we usually need reverse-proxy, mainly called ingress/gateway.&nbsp;</p><p>Now, if we have multiple Kubernetes clusters and want to communicate with each other, usually, two options will be there:</p><ol><li><p>Ingress/Gateway: By default, Kubernetes doesn't support multi cluster communication. Communicate to any service; we can use an ingress controller. Traffic flow will be:</p><ol><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"61\" ac:original-width=\"687\"><ri:attachment ri:filename=\"1eBfTbiEdn8I7uocwn7ojl8NJ_n7irdEiu883TmsdY4ZYgYzH8kDoOSD8fH1ICDXg-9NiJEcjW6Y6XIJn6pj_-dj_cIjnj2HKPr1ZGegCeYQZlU_2f1bcNzktKcM7kBa9Mq7WvHc\" ri:version-at-save=\"3\" /></ac:image></li></ol></li><li><p>Multi-Cluster communication using tools like service mesh or cluster mesh.&nbsp;</p></li></ol><h1><strong>Tooling for Multi cluster communication</strong></h1><p>Many tools are there in the market. I am listing out two approaches that are currently leading in the CNCF community, and we did a perf test on both of them with multiple combinations.&nbsp;</p><ol><li><p>Istio Service Mesh</p></li><li><p>Cilium CNI with Cluster mesh.</p></li></ol><h1>Tools complexity</h1><p>&ldquo;The feature comes with its complexity.&ldquo;</p><ol><li><p><strong>Istio Service Mesh</strong>: Istio has many components and features; we summarised in a <ac:link><ri:page ri:content-title=\"Istio POC\" ri:version-at-save=\"6\" /><ac:link-body><u>doc</u></ac:link-body></ac:link> around it. Istio has good documentation, but the wrong configuration can still bring down the complete Kube cluster. Complexity will increase more when we talk about multi cluster service mesh. Multiple options Istio provides to solve multi cluster service mesh.</p><ul><li><p>It supports shared Istio control-plane b/w multiple clusters.</p></li></ul><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"978\" ac:original-width=\"1076\" ac:width=\"646\"><ri:attachment ri:filename=\"-gamf7688F_f06Rai5wS1ht8PbJcNSiJPEDA5sLwIhYPDfVUFmbvvb06awTpiyjcsn1YPCCXbjSqEes8Pid0aGkWzjRfaYuidLUlCmQMU1otDhyWpMrFpo6fDQjnWKrz4jMk5gtC\" ri:version-at-save=\"1\" /></ac:image><ul><li><p>We can have multiple control-planes in multiple clusters with remote setup.</p></li><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"946\" ac:original-width=\"1172\"><ri:attachment ri:filename=\"TYlfg73Z6JdNVIm2v1cXuZHMy5K2XpyaPonyidpjxA3tIxk1lt9pA0X6Rk8OCdK0Xw_gjY8LT94vMYftGRLw1ityTYLgIszXjrfT5JIYM5tBnPQGhh-IYZhpNIZup59o8UUFyfZs\" ri:version-at-save=\"1\" /></ac:image></li><li><p>We can use multiple Istio control-planes and then have <a href=\"https://github.com/istio-ecosystem/admiral+\"><u>admiral</u></a> help us in a single service mesh in various clusters.</p></li></ul></li><li><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"824\" ac:original-width=\"1400\"><ri:attachment ri:filename=\"u2GMFbvmINTEDaDluI2d0ckaI16K38jxTiv_ShKK1sOpv5nY2zCNtZignRF4xAGwo09Vbz46w5OybVxUQe6QZlYOrB58nKEWGQ_QrfhfJu1_bWGF-T5UPwAQfzTJgono3pejN7DF\" ri:version-at-save=\"1\" /></ac:image><p>Cilium: Cilium is CNI(Container Network Interface), but it provides Network, application, and cluster-wide Observability with Hubble. It supports cluster mesh(multi cluster communication). It has service mesh also, but it is in beta currently.&nbsp;</p></li></ol><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"284\" ac:original-width=\"850\"><ri:attachment ri:filename=\"LamHMhFOzEo1_oYKFUFWRS9Qa58j1ZuTKifsCoK44zJJwzpc7b95Xucw6v89LwuTWqXm9r1NulTIHeJmOd_I02nKvI1tslM6POwVTWpTHVWo2i937r4M-rqL9CvaZn0GA3g0BT6f\" ri:version-at-save=\"1\" /></ac:image><h1>Perf Test</h1><h2>Combination</h2><ol><li><p>AMI: Amazon Linux Kernel Version 5.4 and EKS Version 1.21</p><ol><li><p>EKS with default CNI(VPC CNI).</p></li><li><p>EKS with Istio Service Mesh and default CNI(VPC CNI).</p></li><li><p>EKS with Cilium CNI</p></li><li><p>EKS With Cilium CNI enabled Hubble.</p></li></ol></li><li><p>AMI: Ubuntu 20.04, Linux Kernel Version 5.11.</p><ol><li><p>EKS with default CNI (VPC CNI)</p></li><li><p>EKS with Cilium CNI</p></li><li><p>EKS with Cilium CNI and Hubble enabled</p></li><li><p>EKS with Cilium CNI with Hubble enabled and L7 Visibility.</p></li><li><p>EKS With Cilium CNI with Cluster mesh enabled</p></li><li><p>EKS With Cilium CNI with Hubble and Cluster mesh enabled.</p></li></ol></li></ol><h2>Test Result</h2><p><ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"PE\" ri:content-title=\"Cilium Latency POC\" ri:version-at-save=\"5\" /><ac:link-body>Cilium Latency POC</ac:link-body></ac:link> </p>",
      "approach_used": "endpoint_2",
      "word_count": 914
    },
    {
      "id": "2385084508",
      "title": "Config Map",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2385084508",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2385084508/Config+Map",
      "created": "2022-02-24T06:46:59.946Z",
      "content": "<p>Objective:</p><ul><li><p>Current config map setup in non-k8</p></li><li><p>Config map with k8</p></li><li><p>Current config map tool compatibility with K8</p></li></ul><h1>Current Config Map Setup in non-k8:</h1><p>We have an in-house tool developed for having all the configuration files.</p><p>Steps to create config file for a service:</p><ol><li><p>Create a env file with a service name in the path <code>/etc/sysconfig</code></p></li><li><p>Creation of configuration file using below command which will encrypt the env file with the kms key and push it to the s3 by creating a folder with name as &lsquo;meesho-prod-(your-service-name)&rsquo; in &lsquo;<a href=\"https://s3.console.aws.amazon.com/s3/buckets/meesho-prod-config?region=ap-southeast-1\">meesho-prod-config</a>&rsquo; bucket</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"fa172c6d-ba7f-43df-90fa-111325260a26\"><ac:plain-text-body><![CDATA[sudo kms -p]]></ac:plain-text-body></ac:structured-macro></li><li><p>Services are added to the database by logging into the instance <code>172.31.25.220</code> which hosts the config tool &amp; below commands are run</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"e679fc03-1ee2-4b2c-897b-e225863c5cd1\"><ac:plain-text-body><![CDATA[mysql\nuse mgmt;\ninsert into service_prod (service_name) value ('your-service-name}');\ninsert into users (name, service_ids_prod) values ('user_id', ‘service_id’);\nexit]]></ac:plain-text-body></ac:structured-macro></li><li><p>The web tool <a href=\"https://configs-prod.meesho.com\">link</a> will be used by developers to check the configs and edit in the UI</p></li><li><p>Any changes made by developers in the UI, will be deployed into the instances using the respective CD job which has an option of just updating the config values</p></li></ol><p /><h1>Config map with k8:</h1><p>A ConfigMap is a dictionary of configuration settings. This dictionary consists of key-value pairs of strings. Kubernetes provides these values to your containers. Like with other dictionaries (maps, hashes, ...) the key lets you get and set the configuration value.</p><h2>How does a ConfigMap work?:</h2><p>First, you have multiple ConfigMaps, one for each environment.<br />Second, a ConfigMap is created and added to the Kubernetes cluster.<br />Third, containers in the Pod reference the ConfigMap and use its values.</p><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"866\" ac:original-width=\"846\" ac:width=\"340\"><ri:attachment ri:filename=\"bzkku3dGNW5zO0OZvTRwULnJeEXrlnI1VpZ5EWK4kzUhsrqK3YW-73FP-u3f7744am-gJ5dugyVDpsFlY2cDicAI9Oe9pT2rPD_TZhgd6diRCqpB43xkN8NuHSiqnUDZ7Ma-x13D\" ri:version-at-save=\"1\" /></ac:image><h1>Different ways of creating config map:</h1><h2>Config map in YAML:</h2><ol><li><p>Define the ConfigMap in a YAML file.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"8ff469fb-e86e-401f-8cf6-02b11b90dfa2\"><ac:plain-text-body><![CDATA[kind: ConfigMap \napiVersion: v1 \nmetadata:\n  name: example-configmap \ndata:\n  # Configuration values can be set as key-value properties\n  database: mongodb\n  database_uri: mongodb://localhost:27017\n  \n  # Or set as complete file contents (even JSON!)\n  keys: | \n    image.public.key=771 \n    rsa.public.key=42]]></ac:plain-text-body></ac:structured-macro></li></ol><p>2. Create the ConfigMap in your Kubernetes cluster</p><p>Create the ConfigMap using the command <code>kubectl apply -f config-map.yaml</code></p><p>3. Mount the ConfigMap through a Volume</p><p>Each property name in this ConfigMap becomes a new file in the mounted directory (`/etc/config`) after you mount it.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"080fad28-81ff-48b5-926b-dc45e3cf2edc\"><ac:plain-text-body><![CDATA[kind: Pod \napiVersion: v1 \nmetadata:\n  name: pod-using-configmap \n\nspec:\n  # Add the ConfigMap as a volume to the Pod\n  volumes:\n    # `name` here must match the name\n    # specified in the volume mount\n    - name: example-configmap-volume\n      # Populate the volume with config map data\n      configMap:\n        # `name` here must match the name \n        # specified in the ConfigMap's YAML \n        name: example-configmap\n\n  containers:\n    - name: container-configmap\n      image: nginx:1.7.9\n      # Mount the volume that contains the configuration data \n      # into your container filesystem\n      volumeMounts:\n        # `name` here must match the name\n        # from the volumes section of this pod\n        - name: example-configmap-volume\n            mountPath: /etc/config]]></ac:plain-text-body></ac:structured-macro><p>Attach to the created Pod using <code>kubectl exec -it pod-using-configmap sh</code>. Then run <code>ls /etc/config</code> and you can see each key from the ConfigMap added as a file in the directory. Use `cat` to look at the contents of each file and you'll see the values from the ConfigMap.</p><h2>ConfigMap with Environment Variables and <code>envFrom</code>:</h2><ol><li><p>Create the ConfigMap using the example from the previous section.</p></li><li><p>Add the <code>envFrom</code> property to your Pod's YAML</p></li></ol><p>Set the <code>envFrom</code> key in each container to an object containing the list of ConfigMaps you want to include.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"d6b94311-8d9c-49a9-af3c-b3dd7f5e2899\"><ac:plain-text-body><![CDATA[kind: Pod \napiVersion: v1 \nmetadata:\n  name: pod-env-var \nspec:\n  containers:\n    - name: env-var-configmap\n      image: nginx:1.7.9 \n      envFrom:\n        - configMapRef:\n            name: example-configmap]]></ac:plain-text-body></ac:structured-macro><p>Attach to the created Pod using <code>kubectl exec -it pod-env-var sh</code>. Then run <code>env</code> and see that each key from the ConfigMap is now available as an environment variable.</p><h2>Other ways to create and use ConfigMaps</h2><p>There are three other ways to create ConfigMaps using the <code>kubectl create configmap</code> command.</p><ol><li><p>Use the contents of an entire directory with <code>kubectl create configmap my-config --from-file=./my/dir/path/</code></p></li><li><p>Use the contents of a file or specific set of files with <code>kubectl create configmap my-config --from-file=./my/file.txt</code></p></li><li><p>Use literal key-value pairs defined on the command line with <code>kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2</code></p></li></ol><p /><h1>Current config map tool compatibility with K8:</h1><p>It seems we will not be able to use the current tool with K8. It would be better to go with native configMap in K8.</p>",
      "approach_used": "endpoint_2",
      "word_count": 682
    },
    {
      "id": "2390654987",
      "title": "Installing node exporter on EMR Cluster using bootstrap script",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2390654987",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2390654987/Installing+node+exporter+on+EMR+Cluster+using+bootstrap+script",
      "created": "2022-02-28T13:04:15.513Z",
      "content": "<p>We are using node exporter to monitor the EC2 infra metrics like CPU , Memory and Disk Usage. For normal EC2 infra we are having node exporter installed in the Golden AMI / in other Application AMIs, which is not possible for the EMR instances.<br /><br />To overcome this issue, the only option is to install node exporter via a bootstrap script during the cluster creation.</p><p /><p>s3://meesho-hbase-backup/node-exporter/bootstap.sh<br /><br />We are having the bootstrap script updated in <strong>meesho-hbase-backup </strong>bucket.</p><p><strong>Path:   </strong>s3://meesho-hbase-backup/node-exporter/bootstrap.sh</p><p /><p>Please provide above s3 path in the bootstrap action to install the node exporter for any new instance spawned by the EMR cluster.<br /></p><p>Please refer below screenshot</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"786\" ac:original-width=\"1236\"><ri:attachment ri:filename=\"Screenshot 2022-02-28 at 6.26.38 PM.png\" ri:version-at-save=\"1\" /></ac:image><p><br /><br /></p><p /><p><strong>Reference links</strong></p><p><ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"f76912a9-5fca-4740-bf4f-92e5a466d2c7\"><ac:parameter ac:name=\"key\">DEVOPS-8260</ac:parameter><ac:parameter ac:name=\"serverId\">008259a9-4030-39d8-8c3d-2b3e9dcbfcea</ac:parameter><ac:parameter ac:name=\"server\">System JIRA</ac:parameter></ac:structured-macro> </p><p />",
      "approach_used": "endpoint_2",
      "word_count": 130
    },
    {
      "id": "2393931777",
      "title": "Database Security:- Best Practices For Maintaining Data Integrity",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2393931777",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2393931777/Database+Security+-+Best+Practices+For+Maintaining+Data+Integrity",
      "created": "2022-03-02T07:30:01.886Z",
      "content": "<p />",
      "approach_used": "endpoint_2",
      "word_count": 2
    },
    {
      "id": "2397700114",
      "title": "Grafana New Version Changes",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2397700114",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2397700114/Grafana+New+Version+Changes",
      "created": "2022-03-04T12:53:10.518Z",
      "content": "<p>With the newer version of Grafana, we noticed  few changes required regarding visualization options.</p><p><strong>Problem :</strong> </p><p>The graph is continuous between the data points  and leads to confusion with the data.</p><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"348\" ac:original-width=\"304\"><ri:attachment ri:filename=\"Screenshot 2022-03-04 at 6.08.38 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p><em><strong>problematic  dashboard</strong></em></p></ac:caption></ac:image><p style=\"margin-left: 180.0px;\">                                  <em><strong> </strong></em></p><p><strong>Solution:</strong></p><p>The reason behind this is, In the  <strong>Graph ( old )</strong> the null points are considered by default, which leads to continuous graph ( linear ) on the value of the last non zero  decimal till it find another non zero value.<br /><br />To over come this issue, we need to select the <strong>Time series</strong> visualization , as well as select <strong>never</strong> for <strong>connect null values </strong>option. </p><p /><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"145\" ac:original-width=\"489\"><ri:attachment ri:filename=\"Screenshot 2022-03-04 at 5.28.54 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p><em><strong> Graph ( old ) Visualization</strong></em></p></ac:caption></ac:image><p style=\"margin-left: 180.0px;\">                        </p><p /><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"699\" ac:original-width=\"379\"><ri:attachment ri:filename=\"Screenshot 2022-03-04 at 6.09.14 PM.png\" ri:version-at-save=\"2\" /><ac:caption><p><em><strong>Time series </strong>visualization and selected<strong> Never </strong>for<strong> Connect null values</strong></em></p></ac:caption></ac:image><p style=\"margin-left: 150.0px;\" /><p /><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"292\" ac:original-width=\"363\"><ri:attachment ri:filename=\"Screenshot 2022-03-04 at 6.09.20 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p><em><strong>Final</strong></em> <em><strong>Graph after the fix</strong></em></p></ac:caption></ac:image><p style=\"margin-left: 180.0px;\">            </p><p style=\"margin-left: 90.0px;\"> </p><p>Can select <strong>Auto</strong> for <strong>Show points</strong> option for even better visualization , here is the graph for that</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"320\" ac:original-width=\"260\"><ri:attachment ri:filename=\"Screenshot 2022-03-04 at 6.21.24 PM.png\" ri:version-at-save=\"1\" /></ac:image><p />",
      "approach_used": "endpoint_2",
      "word_count": 212
    },
    {
      "id": "2399896627",
      "title": "CD Enhancements",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2399896627",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2399896627/CD+Enhancements",
      "created": "2022-03-07T06:38:11.610Z",
      "content": "<ol><li><p>Typically, distribution management is always a parent pom. Hence there is no point in having &quot;sub_service_name&quot; in CI</p><ol><li><p>sub_service_name are used to define which artifacts need to be pushed into Jfrog among multiple generated artifacts, it gives the granular control to the developer that among multiple sub modules which one need to be pushed into Jfrog and it justifies the reason to still remain as an option in CI job.</p></li></ol></li><li><p>Why &quot;module&quot; is an option. CD is always strictly coupled to a module, the module should be implicit and prepopulated</p><ol><li><p>We had a job of deploying multiple sub-modules of the respective repository, so we tend to get the input from the developer during the time of deployment to choose which one has to be deployed. Module option cannot be eliminated but we can mark it as an immutable parameter by having separate jobs for all the sub-modules of the repository. So the developer doesn&rsquo;t need to give any inputs during deployment.</p></li></ol></li><li><p>In CD &quot;async_worker_deployment&quot; is only for workers/crons. Why does the option open in web APIs?</p><ol><li><p>async_worker_deployment used to deploy the artifact in servers without deregistration from the Target group which helps us to deploy the servers doesn&rsquo;t expose via LB. Since we are using the job of the same template across web services and Worker/crons it gives confusion to the developer during the deployment. So we can remove the option from web services and converts it as an immutables parameter in Worker/crons jobs.</p></li></ol></li><li><p>In consumer CD, AMI not getting updated with the deployment.</p><ol><li><p>Consumers' jobs are as well considered as a worker in some cases which eventually doesn&rsquo;t bake AMI during the deployments. Despite some of them having ASG associated with but no LB&rsquo;s so after the deployment the AMI are not getting baked and updated in LT. So have to add  AMI bake functionality for consumer applications.</p></li></ol></li><li><p>CD gets ignored if an already deployed tag is used (same tag deployment can be used <br />if some of the machines run on old AMI)</p><ol><li><p>The intended behaviour of the option is to avoid redundant deployment with the same revision. But in some cases when Launch templates got updated manually with some others AMI id (which might be baked manually) which affects the entire flow, It should be avoided. But we can add some flags in CD jobs to provide an exemption, which will do the deployment even the same revision has already been deployed.</p></li></ol></li><li><p>Delete the old jars.</p><ol><li><p>Since we aren&rsquo;t terminated instances for long while, which implies the old jars are got accumulated in many of the machines and started to consume the storage of the root volume. We have to add logic to keep only a certain number of old jars or need to wipe out all the old jars during the deployment.</p></li></ol></li><li><p>Parallel deployment</p><ol><li><p>Most of the time parallel deployment helps us to reduce the time taken off but isn&rsquo;t considered an option in CD, due to a lack of insight about the option. We can give the default surge percentage over number as build with parameter  (1 was commonly given among all the jobs).</p></li></ol></li></ol><p />",
      "approach_used": "endpoint_2",
      "word_count": 507
    },
    {
      "id": "2405302324",
      "title": "AWS SAML - How to switch roles",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2405302324",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2405302324/AWS+SAML+-+How+to+switch+roles",
      "created": "2023-12-14T12:11:38.993Z",
      "content": "<h2><strong>Introduction - </strong></h2><p>This page demonstrates how to switch roles from user account to production, sandbox and member accounts w.r.t access provided to users. </p><p /><p>please find your user name is belongs to which group (i:e) gdevelopers-01, gdevelopers-02<br /><strong><span style=\"color: rgb(191,38,0);\">Account id is common for all the roles</span></strong><br /><a href=\"https://docs.google.com/spreadsheets/d/1VJ5muSSFirnaxBx7qph02jKnFFcQda3DfdLfljd0a40/edit#gid=751312632\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1VJ5muSSFirnaxBx7qph02jKnFFcQda3DfdLfljd0a40/edit#gid=751312632</a> </p><p>Active account name to numbers mapping:</p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"2ad37ff4-a213-4229-a332-10a1f747cdfe\"><colgroup><col style=\"width: 340.0px;\" /><col style=\"width: 340.0px;\" /></colgroup><tbody><tr><th><p><strong>AWS Account name</strong></p></th><th><p><strong>Account Id</strong></p></th></tr><tr><td><p><del>Sandbox</del></p></td><td><p><del>898247906677</del></p></td></tr><tr><td><p>Production</p></td><td><p>847438129436</p></td></tr><tr><td><p>Thirdpartytech</p></td><td><p>124510532221</p></td></tr><tr><td><p>Meesho_perf</p></td><td><p>389989942015</p></td></tr><tr><td><p>Indonesia/meesho-indonesia</p></td><td><p>517083796157</p></td></tr><tr><td><p>Indonesia/sandbox-indonesia</p></td><td><p>826470428947</p></td></tr><tr><td><p>Meeshoone</p></td><td><p>241485335291</p></td></tr><tr><td><p>ControlTower</p></td><td><p>208971692001</p></td></tr><tr><td><p>Meesho-stockone</p></td><td><p>189653073019</p></td></tr><tr><td><p>Data-bricks</p></td><td><p>746623708947</p></td></tr><tr><td><p>Infra-Ops</p></td><td><p>587651380458</p></td></tr><tr><td><p>staging(new sandbox)</p></td><td><p>766380763301</p></td></tr><tr><td><p>DR</p></td><td><p>166882238445</p></td></tr></tbody></table><ul><li><p>To access these accounts, get access of  <strong>mppl-googleusers  </strong>google<strong> </strong>group<strong> .</strong></p></li></ul><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"4fdef74a-d34b-4f29-8645-f6efa9c7a853\"><tbody><tr><th><p><strong>Account ID</strong></p></th><th><p><strong>Account</strong></p></th><th><p><strong>Role to access</strong></p></th></tr><tr><td><p>240134366782</p></td><td><p>Management account</p></td><td><p>gdevops</p></td></tr><tr><td><p>738387322815</p></td><td><p>MPPL Security</p></td><td><p>gdevops</p></td></tr><tr><td><p>360519864065</p></td><td><p>MPPL Sandbox</p></td><td><p>gdevops</p></td></tr><tr><td><p>444473042359</p></td><td><p>MPPL Production</p></td><td><p>gdevops</p></td></tr></tbody></table><p><strong>Note: For now the mentioned teams can configure the corresponding account ids using the corresponding role names through this process. </strong></p><p /><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"142ae1a2-dc59-4d72-bfc5-9bc55ecb439d\"><colgroup><col style=\"width: 310.0px;\" /><col style=\"width: 292.0px;\" /><col style=\"width: 158.0px;\" /></colgroup><tbody><tr><th><p><strong>Team</strong></p></th><th><p><strong>Role Name</strong></p></th><th><p><strong>Account Ids</strong></p></th></tr><tr><td><p>New Joiners</p></td><td><p>greadonly</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li></ul></td></tr><tr><td><p>Devops</p></td><td><p>gdevops</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li><li><p>389989942015(Perf account Only for Arijit, Abhinav, namit, shri for now)</p></li><li><p>241485335291(Meesho_One)</p></li><li><p>data-bricks</p></li></ul></td></tr><tr><td><p>Performance</p></td><td><p>gperformance</p></td><td><ul><li><p>389989942015(Meesho_perf)</p></li><li><p>847438129436(Production)</p></li></ul></td></tr><tr><td><p>App Support</p></td><td><p>gappsupport</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li></ul></td></tr><tr><td><p>DB Team</p></td><td><p>gdatabase</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li></ul></td></tr><tr><td><p>Security Team</p></td><td><p>gsecurity</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li><li><p>389989942015(Performance account)</p></li><li><p>241485335291(Meesho_one)</p></li></ul></td></tr><tr><td><p>Backend Team</p></td><td><p><ac:inline-comment-marker ac:ref=\"ae085c62-ce5c-4897-a610-59cfaa201e36\">gdevelopers-0x</ac:inline-comment-marker> ( <a href=\"https://docs.google.com/spreadsheets/d/1VJ5muSSFirnaxBx7qph02jKnFFcQda3DfdLfljd0a40\">Role to Assume</a> )</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li></ul></td></tr><tr><td><p>Leads</p></td><td><p>gleads</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li></ul></td></tr><tr><td><p>Data Team</p></td><td><p>gdata-0x ( <a href=\"https://docs.google.com/spreadsheets/d/1VJ5muSSFirnaxBx7qph02jKnFFcQda3DfdLfljd0a40\">Role to Assume</a> )</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li></ul></td></tr><tr><td><p>Architects (Directors and engineering managers)</p></td><td><p>garchitects</p></td><td><ul><li><p>847438129436(Production)</p></li><li><p>766380763301(staging)</p></li></ul></td></tr></tbody></table><p><strong>Please visit this link to find your email and the role to assume -</strong><br /><a href=\"https://docs.google.com/spreadsheets/d/1VJ5muSSFirnaxBx7qph02jKnFFcQda3DfdLfljd0a40\">Role to Assume</a><br /></p><p><strong>Steps - </strong></p><p><strong>Step 1  &ndash; &gt; </strong></p><p>Login to you gmail account and click on apps in the top right corner in gmail page as shown below -</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"560\" ac:original-width=\"762\"><ri:attachment ri:filename=\"Screenshot 2022-03-09 at 4.42.44 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>You will see an icon with name <code>Amazon Web Service</code>. Click on the icon and you will be redirected to default user account where you don't have any access in this account. You will see a page something like below if you try to access any page with permission errors - </p><p> </p><p><strong>                                   select                Account: 208971692001</strong></p><p><strong>                                                                        googleusers</strong></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"885\" ac:original-width=\"1607\"><ri:attachment ri:filename=\"image-20230217-051641.png\" ri:version-at-save=\"1\" /></ac:image><p /><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"851\" ac:original-width=\"1912\"><ri:attachment ri:filename=\"Screenshot 2022-03-09 at 4.46.53 PM.png\" ri:version-at-save=\"1\" /></ac:image><p /><p /><p><strong>Step 2 -</strong></p><p style=\"margin-left: 30.0px;\">1. Click on the dropdown at the top right corner of the aws console page as shown below - </p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"851\" ac:original-width=\"1912\"><ri:attachment ri:filename=\"Screenshot 2022-03-09 at 4.50.43 PM.png\" ri:version-at-save=\"1\" /></ac:image><p style=\"margin-left: 30.0px;\">2. Click on switch role. You will see a page as shown below. </p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"851\" ac:original-width=\"1912\"><ri:attachment ri:filename=\"Screenshot 2022-03-09 at 4.53.53 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>       3. Click on switch role again, you will be redirect to a page a shown below -</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"851\" ac:original-width=\"1912\"><ri:attachment ri:filename=\"Screenshot 2022-03-09 at 4.57.28 PM.png\" ri:version-at-save=\"1\" /></ac:image><p style=\"margin-left: 30.0px;\">4. Add account number, role name and any display name you wish to add and then click on switch as shown below - </p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"851\" ac:original-width=\"1912\"><ri:attachment ri:filename=\"Screenshot 2022-03-09 at 4.56.35 PM.png\" ri:version-at-save=\"2\" /></ac:image><p /><p style=\"margin-left: 30.0px;\">5. Here i added production account number <code>847438129436</code>, <code>gdevops</code>(google devops) role and display name as <code>Production</code>.</p><p style=\"margin-left: 30.0px;\">6. For account number and roles names you can reach out to app-support team and devops team.</p><p style=\"margin-left: 30.0px;\">7. Once you click on switch role you will be redirected to respective account you choosed. And the page looks as below - </p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"851\" ac:original-width=\"1912\"><ri:attachment ri:filename=\"Screenshot 2022-03-09 at 5.03.28 PM.png\" ri:version-at-save=\"1\" /></ac:image><p style=\"margin-left: 30.0px;\">8. You can switch back to previous account or to any other accounts following the same steps. </p><p style=\"margin-left: 30.0px;\">9. After one time switching roles to different accounts, all accounts will be added in console so that no need to follow all steps as mentioned above and you can easily switch to other accounts with display names as shown below.</p><p>      10. please select </p><ul><li><p><a href=\"https://ap-southeast-1.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-1\">Asia Pacific (Singapore)ap-southeast-1</a> - Production region</p></li><li><p><a href=\"https://ap-south-1.console.aws.amazon.com/console/home?region=ap-south-1\">Asia Pacific (Mumbai)ap-south-1</a> - Staging region</p></li></ul><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"652\" ac:original-width=\"329\" ac:width=\"204\"><ri:attachment ri:filename=\"Screenshot 2022-03-11 at 5.22.38 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"659\" ac:original-width=\"924\"><ri:attachment ri:filename=\"Screenshot 2022-03-09 at 5.09.44 PM.png\" ri:version-at-save=\"1\" /></ac:image><p><strong> Hope this helps everyone using saml authentication rather than using user name and password for multi accounts. </strong></p><h2><strong>Thank you. </strong></h2>",
      "approach_used": "endpoint_2",
      "word_count": 605
    },
    {
      "id": "2409136204",
      "title": "Resource Tagging",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2409136204",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2409136204/Resource+Tagging",
      "created": "2022-03-11T11:49:17.617Z",
      "content": "<p>We can assign metadata to our AWS resources in the form of tags. Each tag is a label consisting of a user-defined key and value. Tags can help us in  managing, identifying, organising, searching for, and filter resources. We can create tags to categorize resources by purpose, owner, environment, or other criteria.</p><p>Each tag has two parts:</p><p>A tag key (for example, pod, env, or project). Tag keys are case sensitive.<br />A tag value (for example, backend or prod). Like tag keys, tag values are case sensitive.</p><p><strong>Tagging strategies:</strong></p><ul><li><p><strong>Tags for resource organisation:</strong> Tags are a good way to organize AWS resources in the AWS Management Console. We can configure tags to be displayed with resources, and can search and filter by tag.</p></li><li><p><strong>Tags for Cost allocation:</strong> AWS Cost Explorer and detailed billing reports let us break down AWS costs by tag. We can associate tags like team, project for cost-allocation according to teams or project.</p></li><li><p><strong>Tags for automation:</strong> Resource or service-specific tags are often used to filter resources during automation activities. Automation tags are used to opt in or opt out of automated tasks or to identify specific versions of resources to archive, update, or delete. We are using these tags for backup, snapshots and monitoring.</p></li></ul><p>We are using tags for automation and cost-allocation to create billing reports team wise. Below are the tags which we need to add while creating infra ex. EC2 instances, EMR cluster, RDS.</p><p><strong>pod=backend/frontend/data-science/data-platform/frontend</strong></p><p><strong>project=supply/demand</strong><br /><strong>env=prod/stage</strong></p><p><strong>priority= p0/p1/p2</strong></p><p>** Tags are case-sensitive so we can use small case for creating tags key and value to avoid having multiple entries.</p><p>   </p>",
      "approach_used": "endpoint_2",
      "word_count": 255
    },
    {
      "id": "2417786929",
      "title": "How to assume role using saml for CLI access",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2417786929",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2417786929/How+to+assume+role+using+saml+for+CLI+access",
      "created": "2023-12-13T06:48:21.474Z",
      "content": "<p>This guide will help you to assume an IAM Role on AWS using Google SSO (SAML User). You will need to follow this guide to allow you to execute any command on CLI using these credentials.</p><p><br /><strong>Prerequisites</strong></p><p>Before able to continue following instructions on this page, you need to have these software installed:</p><ol start=\"1\"><li><p>Python &gt;= 2.7</p></li><li><p>virtualenv. </p></li></ol><p>Step-by-step guide</p><p>These are steps needed to be followed:</p><ol start=\"1\"><li><p>Make sure that your Google Account is allowed to use SSO to AWS.</p></li><li><p>Install all tools required.</p></li><li><p>Add several configurations to a file on your local machine.</p></li><li><p>Run the tools and verify that you successfully assumed the role.</p></li></ol><p><strong>Make sure that your Google Account is allowed to use SSO to AWS</strong></p><ol start=\"1\"><li><p>Open this <a href=\"https://accounts.google.com/o/saml2/initsso?idpid=C02ylfc3o&amp;spid=665882465155&amp;forceauthn=false\"><u>LINK</u></a> and login using your Meesho Google Account.</p></li><li><p>After you successfully logged in into your account, you will be redirected to a page. If the page contains: <strong>Your request included an invalid SAML response</strong>, you need to ask IT to allow your Google Account to use SSO to AWS. If the page shown up is AWS Web Console, you are ready to go to the next step.</p></li></ol><p><strong>Install the Tools</strong></p><p>It is recommended to use <strong>virtualenv </strong>to minimize dependency conflict between Python libraries, because it will create isolated Python environment for you. Read more:<a href=\"https://virtualenv.pypa.io/en/stable/\"> </a><a href=\"https://virtualenv.pypa.io/en/stable/\" data-card-appearance=\"inline\">https://virtualenv.pypa.io/en/stable/</a> </p><p>You need to install 2 tools:<a href=\"https://github.com/cevoaustralia/aws-google-auth\"> </a><a href=\"https://github.com/ruimarinho/gsts\"><strong><u>gsts</u></strong></a><strong><u> </u></strong>and<a href=\"https://github.com/makethunder/awsudo\"> <strong><u>awsudo</u></strong></a><strong><u>(on Virtualenv)</u></strong></p><p /><p><strong>INSTALLATION</strong></p><p># make sure you have virtualenv installed. If this command fails, refer to prerequisites above no.2</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"07078ecc-d30a-42dc-a859-25e3a595e0c8\"><ac:plain-text-body><![CDATA[brew install virtualenv                 # install virtualenv in mac\nvirtualenv --version                    # check verion of virtualenv]]></ac:plain-text-body></ac:structured-macro><p># create one separate directory for awsudo.&nbsp;The path of each directory is actually up to you, but on this example it will be:</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"ff8f2e30-44ab-4326-93c4-25fe2749a33f\"><ac:plain-text-body><![CDATA[mkdir ~/.awsudo                         # create directory for awsudo]]></ac:plain-text-body></ac:structured-macro><h3><strong>Install the tools</strong></h3><p><strong>## awsudo</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"822e5922-43c5-49a5-a94f-657474bb1d8f\"><ac:plain-text-body><![CDATA[cd ~/.awsudo                           # open awsudodirectory\nvirtualenv .                           # create virtualenv on this directory\nsource bin/activate                    # activate virtualenv\npip install git+https://github.com/makethunder/awsudo.git # install awsudo awsudo # verify the installation\ndeactivate                             # deactivate virtualenv]]></ac:plain-text-body></ac:structured-macro><p><strong>## gsts</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c88db44c-08c7-42e0-a95e-f77530c26987\"><ac:plain-text-body><![CDATA[brew tap ruimarinho/tap\nbrew install gsts]]></ac:plain-text-body></ac:structured-macro><p><br />Since the updated gsts &gt;= 5.0 have issues with multiple aws profile switching, use npm to install the 4.0 version <br /></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"60309a0c-1107-4f4d-b08c-1b0c59f5b6fc\"><ac:plain-text-body><![CDATA[npm install --global gsts@4.1.0]]></ac:plain-text-body></ac:structured-macro><p><strong>## export path of the directories</strong></p><p><strong>Add the following lines into the bottom of ~/.bashrc file&nbsp;or ~/.zshrc</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"f33e4fdf-4e2b-42ef-92ab-a3df17e7bd81\"><ac:plain-text-body><![CDATA[export PATH=$PATH:~/.awsudo/bin]]></ac:plain-text-body></ac:structured-macro><p><strong>## Reload ~/.bashrc or ~/.zshrc file&nbsp;</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"bbaa25b7-b0f3-48a0-82c2-be2c59b85912\"><ac:plain-text-body><![CDATA[source ~/.bashrc                  #(For Mac, use ~/.bash_profile, For myzsh, use ~/.zshrc)]]></ac:plain-text-body></ac:structured-macro><p><strong>## make sure the path is reloaded</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"5e92a598-241f-4e40-bf6f-f4253e188934\"><ac:plain-text-body><![CDATA[which awsudo                      # the output should be: /home/<your_laptop_username>/. awsudo/bin/awsudo]]></ac:plain-text-body></ac:structured-macro><p><strong>Add Credential Configurations</strong></p><p>Append these configs into <strong>~/.aws/config </strong>file. Do not forget to replace the values inside angle bracket <strong>&lt; &gt; </strong>with the correct value.</p><p><strong>Add a profile for different accounts in the same file i.e ~/.aws/config</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"b7320123-6002-4d73-9fe3-afe097df6f55\"><ac:plain-text-body><![CDATA[[profile <NAME_OF_THE_PROFILE>]\nrole_arn = <AWS_IAM_ROLE_ARN>\nsource_profile = sts\nrole_session_name = <YOUR_USER_NAME>\nregion = ap-southeast-1]]></ac:plain-text-body></ac:structured-macro><p><strong>Example -&nbsp;</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"c8b0710e-58f4-4756-bdc1-079051cbc1c6\"><ac:plain-text-body><![CDATA[[profile sandbox]\nrole_arn = arn:aws:iam::898247906677:role/gdevops\nsource_profile = sts\nrole_session_name = <user_name>@meesho.com\nregion = ap-southeast-1\n\n[profile prod]\nrole_arn = arn:aws:iam::847438129436:role/gdevops\nsource_profile = sts\nrole_session_name = example@meesho.com\nregion = ap-southeast-1]]></ac:plain-text-body></ac:structured-macro><p><strong>Please visit this sheet to find your email and the role to assume -</strong><br /><a href=\"https://docs.google.com/spreadsheets/d/1VJ5muSSFirnaxBx7qph02jKnFFcQda3DfdLfljd0a40/edit#gid=1653450066\" data-card-appearance=\"inline\">https://docs.google.com/spreadsheets/d/1VJ5muSSFirnaxBx7qph02jKnFFcQda3DfdLfljd0a40/edit#gid=1653450066</a> <br /><br /><strong>Run the Tools and Verify</strong></p><p><strong># Genertate/Inject your temporary credentials into ~/.aws /credentials file via saml login. </strong></p><p><br />If you are getting below error check the <strong>gsts</strong> version. <code>gsts --version</code>  </p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"656\" ac:original-width=\"1293\" ac:width=\"442\"><ri:attachment ri:filename=\"Screenshot 2023-03-25 at 2.25.51 PM.png\" ri:version-at-save=\"1\" /></ac:image><p>If the <strong>gsts</strong> version is above <code>5.x.x</code>  update your <code>cat ~/.aws/credentials</code> file ,If its less than <code>5.x.x</code> please follow the old process.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"4f14e92d-aded-492a-a395-8e75d68a88f6\"><ac:plain-text-body><![CDATA[[sts]\ncredential_process = gsts --aws-role-arn arn:aws:iam::208971692001:role/googleusers --sp-id 665882465155 --idp-id C02ylfc3o --username <user>@meesho.com --aws-region ap-southeast-1]]></ac:plain-text-body></ac:structured-macro><p>You will be asked to provide your information to login using Google.</p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"b6f8a948-5f8b-49d7-b5a7-81f56216b85d\"><ac:plain-text-body><![CDATA[gsts --aws-role-arn arn:aws:iam::208971692001:role/googleusers --sp-id 665882465155 --idp-id C02ylfc3o --username <user>@meesho.com]]></ac:plain-text-body></ac:structured-macro><p><strong>Note:</strong> If you get below issue,<br /><code>fatal: not a git repository (or any of the parent directories): .git</code> </p><p>then use the below command<br /><code>unalias gsts</code></p><p><strong>Note:</strong> If browser doesn&rsquo;t open for Google SSO login, try using above command with param <code>--headful=true</code></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"2f80ed7a-c3b7-4edf-884e-8c7fffc48ed4\"><ac:plain-text-body><![CDATA[gsts --aws-role-arn arn:aws:iam::208971692001:role/googleusers --sp-id 665882465155 --idp-id C02ylfc3o --headful=true --username <user>@meesho.com]]></ac:plain-text-body></ac:structured-macro><p>Notice on the picture below after executing <code>gsts</code> it produces output: </p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"994\" ac:original-width=\"1651\"><ri:attachment ri:filename=\"Screenshot 2022-04-14 at 12.04.26 PM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"985\" ac:original-width=\"1642\"><ri:attachment ri:filename=\"Screenshot 2022-04-14 at 12.04.48 PM.png\" ri:version-at-save=\"1\" /></ac:image><p /><p>Once you try to login with your google account and 2 way auth confirmation integrated with your google account. After successful login temporary credentials will be created in the session duration is set to 43200sec i.e 12hrs. It means once saml credential is expired, you have to re-run the command again.</p><p>To check the expiration time you can <code>cat ~/.aws/credentials</code> file. It looks something like below -</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"288\" ac:original-width=\"1909\"><ri:attachment ri:filename=\"Screenshot 2022-04-14 at 5.39.25 PM.png\" ri:version-at-save=\"1\" /></ac:image><p><strong>Use the Tools!</strong></p><p>Next, to actually assume a Role and execute the command, you will use <strong>awsudo</strong></p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"a3cad1be-a332-43c5-be2d-73759045fa90\"><ac:plain-text-body><![CDATA[awsudo -u <NAME_OF_THE_PROFILE> -- <ANY_COMMAND_THAT_YOU_WANT_TO_EXECUTE>]]></ac:plain-text-body></ac:structured-macro><p><strong># Example&nbsp; -&nbsp;</strong></p><p>Using sandbox as profile name from above example config set - </p><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"dab91ea2-7b38-44d8-a5f6-b6cf4f149f04\"><ac:plain-text-body><![CDATA[awsudo -u sandbox -- aws s3 ls\nawsudo -u sandbox -- terraform init, plan, apply, destroy]]></ac:plain-text-body></ac:structured-macro><p>Now we will be able to install tools required to securely communicate aws api via CLI using SAML authentication. </p><p>Please reach out to devops team if any queries. </p><p><strong>Thank you</strong></p><h3>Troubleshooting</h3><p>If facing below issue while using gsts, run the suggested commands for the error as shown below:</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"636\" ac:original-width=\"3328\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2023-12-13 at 11.52.19 AM.png\" ri:version-at-save=\"1\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"1436\" ac:original-width=\"3456\" ac:custom-width=\"true\" ac:width=\"760\"><ri:attachment ri:filename=\"Screenshot 2023-12-13 at 11.52.42 AM.png\" ri:version-at-save=\"1\" /></ac:image><p />",
      "approach_used": "endpoint_2",
      "word_count": 899
    },
    {
      "id": "2445246569",
      "title": "Database Architecture Diagram",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2445246569",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2445246569/Database+Architecture+Diagram",
      "created": "2022-04-07T08:43:28.090Z",
      "content": "<p>Please find the DB Architecture diagram below.</p><p /><ac:image ac:align=\"center\" ac:layout=\"wide\" ac:original-height=\"742\" ac:original-width=\"795\" ac:width=\"677\"><ri:attachment ri:filename=\"Screenshot 2022-04-07 at 2.12.28 PM.png\" ri:version-at-save=\"1\" /><ac:caption><p /></ac:caption></ac:image><p />",
      "approach_used": "endpoint_2",
      "word_count": 22
    },
    {
      "id": "2449015768",
      "title": "Incident Management",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2449015768",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2449015768/Incident+Management",
      "created": "2023-03-09T04:15:40.823Z",
      "content": "<h2><strong>What is incident management</strong></h2><p>Incident management refers to a set of practices, processes, and automations that enable teams to detect, investigate and respond to incidents(unplanned event or service interruption) and restore the service to its operational state.</p><p>Effective Incident Management helps reduce the overall impact of incidents, mitigate damages, and ensure that systems and services continue to operate as planned.</p><h2><strong>Steps in an Incident Management</strong></h2><ul><li><p>Incident identification</p></li><li><p>Incident logging</p></li><li><p>Incident categorization(<em>is not a blocking step</em>)</p><ul><li><p>Incident prioritization and tag SLA</p></li></ul></li><li><p>Incident response</p></li><li><p>Initial diagnosis</p><ul><li><p>Incident escalation and communication</p></li><li><p>Investigation and diagnosis</p></li><li><p>Resolution and recovery and communication</p></li></ul></li><li><p>Incident closure</p></li><li><p>RCA documentation</p><ul><li><p>RCA Action Item implementation</p></li></ul></li></ul><h3><strong>Incident Identification:</strong></h3><p>The first and most obvious step is identifying the problem. An incident can come from monitoring systems (only tech issues). Once an incident is <ac:inline-comment-marker ac:ref=\"41588971-db58-41be-91ba-3c832bd19720\">identified</ac:inline-comment-marker>, it needs to be logged and subsequent steps are initiated. Ownership of this step relies on first responder i.e application owner/app-oncall to define the severity of the incident ( s0/s1/s2) based on the incident reported by app-support / pagerduty.</p><p>All the incidents S0, S1 should get reported in <strong>#the_firefighters</strong> ( <a href=\"https://meesho.slack.com/archives/C03G9MTDP09\" data-card-appearance=\"inline\">https://meesho.slack.com/archives/C03G9MTDP09</a> ) channel.</p><p>Incident manager would be from each team on a rotation shift. Someone with EM or higher. They would be responsible for managing the incident during their oncall schedule.</p><h3><strong>Incident Logging:</strong></h3><p>Every S0 incidents should have</p><ul><li><p><ac:inline-comment-marker ac:ref=\"5cebe0ae-4044-443c-9bf6-364b2c896987\">Jira</ac:inline-comment-marker> logged in(<em>is not a blocking step, can be taken up post resolution</em>)</p></li><li><p>Slack channel created</p></li><li><p>War room created (google hangout)</p></li><li><p>RCA doc prepared</p></li></ul><p>Every S1 incidents should have</p><ul><li><p>Jira logged in</p></li><li><p>Slack channel created</p></li><li><p>RCA doc prepared(till the sale season '22 completes, on a pro-active measure)</p></li></ul><p><ac:inline-comment-marker ac:ref=\"30ae3358-c848-4d14-9103-03cd470d704d\">Every S2 incidents should have</ac:inline-comment-marker></p><ul><li><p>Jira logged in</p></li><li><p>RCA document prepared and shared across relevant stakeholders for the future reference.<br /></p></li></ul><h3><strong>Incident Categorization:</strong></h3><h4><strong>Types of Incidents:</strong></h4><p>Incidents are classified into 3 levels</p><p><ac:inline-comment-marker ac:ref=\"ca80cc7d-7137-42af-8bb5-bd6f180b52fb\">S0, S1, S2.</ac:inline-comment-marker> the lower the severity number, the more impactful the incident. Once we prioritize each of the incidents, we can then tag them with the corresponding SLA and adhere to the incident response for each. We will define Severity of incidents based on business(monetary $), reputation(website, visual impact etc) and Security.</p><table data-layout=\"default\" ac:local-id=\"e89255fd-7baf-4270-b08b-5801987be383\"><colgroup><col style=\"width: 212.0px;\" /><col style=\"width: 546.0px;\" /></colgroup><tbody><tr><th><p>Column 1</p></th><th><p>Column 2</p></th></tr><tr><td><p>Severity</p></td><td><p>Definition</p></td></tr><tr><td><p>S0</p></td><td><p>Entire app or core app flow impacted or is down <br /><ac:inline-comment-marker ac:ref=\"8f861dc0-a65a-4b9f-a106-a2703d64e36d\">Large number</ac:inline-comment-marker> of users impacted and business flow impacted<br />High impact on business/app flow in terms of $<br /></p></td></tr><tr><td><p>S1</p></td><td><p>High reputation impact / but <ac:inline-comment-marker ac:ref=\"f8d0cda4-b715-4d2d-aa97-8c60226b1350\">no business impact</ac:inline-comment-marker>($), majorly on the website styling/layout/typos/wrong-info etc</p><p>Some users affected with little to no interruption for core app flow<br />Some impact on business in terms of $<br />Incidents that can become S0 in future, if not controlled</p></td></tr><tr><td><p>S2</p></td><td><p>Incidents that do not have direct/present impact on the app-flow/services<br />Incidents that can become S1/S0 in future, if not controlled</p></td></tr></tbody></table><p><em><strong>WIP</strong>: &nbsp;{{What is the $$ numbers that qualify an incident between S0, S1 and S2</em><br />          <em>What is the definition of Security violation}}</em></p><p><em> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;What is the definition of reputation impact</em><br /><br /><br /><strong><ac:inline-comment-marker ac:ref=\"155e10c6-6e1b-4129-97d4-374d9a8c9535\">Incident Prioritisation:</ac:inline-comment-marker></strong></p><p>Prioritising incidents based on their severity will clearly point to major incidents that need to be solved right away, and minor incidents whose necessary resolution time is much more flexible. An incident&rsquo;s priority and urgency will be based on the level of impact to users and their ability to use the service. Incident severity levels are a measurement of the impact an incident has on the business.</p><h3><strong>Incident Response:</strong></h3><p>Response steps for each of the incidents varies with respect to the severity of the incident. </p><table data-layout=\"wide\" ac:local-id=\"38838246-a521-43c2-9ed1-12f7b25a1f42\"><colgroup><col style=\"width: 178.0px;\" /><col style=\"width: 167.0px;\" /><col style=\"width: 275.0px;\" /><col style=\"width: 223.0px;\" /><col style=\"width: 117.0px;\" /></colgroup><tbody><tr><th><p><strong>Steps</strong></p></th><th><p><strong>Ownership</strong></p></th><th><p><strong>S0</strong></p></th><th><p><strong>S1</strong></p></th><th><p><strong>S2</strong></p></th></tr><tr><td><p>Incident Identification &amp; classification?(during incident)</p></td><td><p><ac:inline-comment-marker ac:ref=\"fc878f7b-0b49-43fc-8ba5-a5bd4d87d274\"><ac:inline-comment-marker ac:ref=\"449b4f9d-5517-4a88-9c26-5f4afb38d7c5\">app-owner</ac:inline-comment-marker></ac:inline-comment-marker> (eng mgr/oncall)</p></td><td><ul><li><p>Incident identification and categorisation to S0</p></li><li><p>Pagerduty S0 incident creation and playbook execution</p></li><li><p>Slack channel &amp; warroom(hangout) creation</p></li><li><p>Escalate and get relevant oncalls to warroom</p></li></ul></td><td><ul><li><p>Incident identification and categorisation to S1</p></li><li><p>Slack channel creation</p></li><li><p>Escalate and get relevant oncalls to warroom</p></li></ul></td><td><ul><li><p>Jira ticket created.</p></li></ul></td></tr><tr><td><p>Incident Response Steps<br />(during incident)</p></td><td><p>app-owner/SRE</p></td><td><ul><li><p>Investigation and diagnosis</p></li><li><p>Resolution and recovery and <ac:inline-comment-marker ac:ref=\"90e5c0fe-f4f9-45b4-8141-95b086d484ae\">communication</ac:inline-comment-marker></p></li></ul></td><td><ul><li><p>Investigation and diagnosis</p></li><li><p>Resolution and recovery and communication</p></li></ul></td><td><p /></td></tr><tr><td><p>Logging<br />(post incident)</p></td><td><p>app-owner</p></td><td><ul><li><p>Jira</p></li><li><p>Slack channel</p></li><li><p>RCA document</p></li></ul></td><td><ul><li><p>Jira</p></li><li><p>Slack Channel</p></li></ul></td><td><ul><li><p>Jira</p></li></ul></td></tr><tr><td><p>SLA<br />(post incident)</p></td><td><p>app-owner/SRE</p></td><td><ul><li><p>Incident detection(<ac:inline-comment-marker ac:ref=\"d4070c67-0347-417d-8824-15723cfd1459\">MTTD</ac:inline-comment-marker>): 10 mins</p></li><li><p>Provisional RCA: 24hrs within incident resolution(24/7)</p></li><li><p>Complete RCA: 72hrs within incident resolution(24/7)</p></li></ul></td><td><ul><li><p>Incident detection(MTTD): 10 mins</p></li><li><p>Complete RCA: 72hrs within incident resolution(24/7)</p></li></ul></td><td><ul><li><p>Nil</p></li></ul></td></tr><tr><td><p>RCA<br />(post incident)</p></td><td><p>app-owner/SRE</p></td><td><ul><li><p><ac:inline-comment-marker ac:ref=\"5ea59f2c-b257-4720-bb7e-52b93c45239a\">To be documented and presented in the RCA meet</ac:inline-comment-marker></p></li><li><p>Actions Items tracked through jira to closure</p></li></ul></td><td><ul><li><p>To be documented</p></li><li><p>Actions Items tracked through jira to closure</p></li></ul></td><td><p>Nil</p></td></tr></tbody></table><h3><strong><ac:inline-comment-marker ac:ref=\"c64a7f45-b82f-4620-89da-e8ff733bf1c4\">Logging</ac:inline-comment-marker>:</strong></h3><ul><li><p>Slack: channel to be opened up for each of the S0 Issue with below naming conventions and relevant oncalls to be added:</p></li><li><p>s0-yyyy-mm-dd-&lt;servicename&gt; &nbsp; <em>or should it be <ac:inline-comment-marker ac:ref=\"fea5c9a2-ab8a-49e1-b9f7-5c14c92ddb2f\">feature</ac:inline-comment-marker> instead of service</em>?</p></li><li><p>Jira: jira to be created for each of the S0 Issue with below naming conventions:</p></li><li><p>s0-yyyy-mm-dd-&lt;servicename&gt;</p></li><li><p><ac:inline-comment-marker ac:ref=\"4a2bf259-0d65-4e0d-bcaa-116626107ae3\">Hangout</ac:inline-comment-marker>: to be opened for each of the S0 Issue with below naming conventions and relevant oncalls added</p></li><li><p>s0-yyyy-mm-dd-&lt;servicename&gt;</p></li></ul><p><strong>Pagerduty</strong>:</p><p>Pagerduty will play a central role in complete incident management and will be at the core of reducing MTTD.</p><p><strong>Current status/planned updates &nbsp;</strong></p><p><ac:link><ri:page ri:space-key=\"EW\" ri:content-title=\"Pager Duty Integration\" ri:version-at-save=\"9\" /><ac:link-body>Pager Duty Integration</ac:link-body></ac:link></p><ul><li><p>Pagerduty onboarding has <ac:inline-comment-marker ac:ref=\"9bea67ff-bbbb-4ac7-bc25-0457536c139b\">started</ac:inline-comment-marker>. Cloudwatch integration is completed</p></li><li><p>Every team will have independent Service and Incident management escalation policy in pagerduty</p></li><li><p>Pagerduty will be integrated with each and every monitoring/metric tool and/or manual scripts</p></li><li><p>Pagerduty supports out of the box integrations with all the tools we use, also has options to integrate manually using mail/api/web-hooks</p></li><li><p>Pagerduty can be used for S0 incident management initiation like notification and timely update automation</p></li></ul><p>Future(suggestion):</p><ul><li><p>PD has acquired rundeck, which is an automation tool</p></li><li><p>With PD and rundeck, all alerts and their subsequent actions can be automated as playbook</p></li><li><p>PoC has to be done on the same, depending on the outcome and subsequent pricing discussions, integration can be looked at(if needed)</p></li></ul><p><strong>Postmortem</strong> and <strong>RCA</strong></p><p>details in: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Postmortem(RCA)\" ri:version-at-save=\"11\" /><ac:link-body>Postmortem(RCA)</ac:link-body></ac:link> </p><ol start=\"1\"><li><p>We have created and saved a template in confluence, which can be used directly while creating an RCA doc(only fill the incident specific details): <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Postmortem(RCA)\" ri:version-at-save=\"11\" /><ac:link-body>Postmortem(RCA)</ac:link-body></ac:link> </p></li><li><p>SRE to own the RCA lifecycle and follow-ups with the corresponding app-owners to make sure it is published and the AI's(action item) are tracked</p></li><li><p>RCA review meetings to be scheduled and run by SRE team on a fortnightly basis(every alternate Monday at 5.30pm IST), where each month&rsquo;s S0 issues can be presented</p></li></ol><p><strong>Notification:</strong></p><p>For every S0 incident a<ac:inline-comment-marker ac:ref=\"f5ab8d28-b0c8-4c20-bbea-f5f535741583\"> mail notification</ac:inline-comment-marker> to the leadership(and other relevant) audience has to be sent out.</p><p>This will be followed every 30 mins during the course of the incident</p><p>Mail template during/post incident: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"S0 Outage Communication template\" ri:version-at-save=\"3\" /><ac:link-body>S0 Outage Communication template</ac:link-body></ac:link> </p><p><br /><em><strong>Execution Plan:</strong></em></p><p><em>What has to be done right away:</em></p><ac:task-list>\n<ac:task>\n<ac:task-id>1</ac:task-id>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\"><em>RCA template standardisation and onboarding.</em></span></ac:task-body>\n</ac:task>\n<ac:task>\n<ac:task-id>2</ac:task-id>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\"><em><ac:inline-comment-marker ac:ref=\"4d23b353-16eb-488f-bdd1-73533c25228c\">RCA review meeting</ac:inline-comment-marker></em></span></ac:task-body>\n</ac:task>\n<ac:task>\n<ac:task-id>3</ac:task-id>\n<ac:task-status>incomplete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">RCA adherence(culture)</span></ac:task-body>\n</ac:task>\n<ac:task>\n<ac:task-id>4</ac:task-id>\n<ac:task-status>incomplete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Notification part to be closed and implemented</span></ac:task-body>\n</ac:task>\n</ac:task-list><p><em><strong>WIP</strong>: What needs deliberation further</em></p><ol start=\"1\"><li><p><em>Incident classification conditions to S0/S1/S2 - set a brainstorming session with the leaders(tech)</em></p></li><li><p><em>Application/service criticality classification &nbsp;- Can we take below as the source of truth: &nbsp;</em><a href=\"https://coda.io/d/Scalability_dlKFzEJPskm/Master-List-of-Services-and-Tiers_suVmc#_luoOa\"><em>https://coda.io/d/Scalability_dlKFzEJPskm/Master-List-of-Services-and-Tiers_suVmc#_luoOa</em></a></p></li><li><p><em>Impact in terms of $ and reputations if this application deteriorates- this can be a way to classify the applications</em></p></li></ol>",
      "approach_used": "endpoint_2",
      "word_count": 1110
    },
    {
      "id": "2450259983",
      "title": "SRE",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2450259983",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2450259983/SRE",
      "created": "2022-04-12T09:06:21.088Z",
      "content": "<p />",
      "approach_used": "endpoint_2",
      "word_count": 2
    },
    {
      "id": "2451046401",
      "title": "Postmortem(RCA)",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2451046401",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2451046401/Postmortem+RCA",
      "created": "2022-10-18T10:41:59.622Z",
      "content": "<h3><br /><strong>What is postmortem</strong>:</h3><p>Postmortem is a written record of an incident, its impact, the actions taken to mitigate or resolve it, the root cause(s), and the follow up actions(action items AI) to prevent the incident from recurring.</p><p>The outcome of a success postmortem is to ensure that the incident is documented, that all contributing root cause(s) are well understood and especially that, effective preventive actions(action items AI) are put in place to reduce the likelihood and/or impact of a recurrence.</p><h3><strong>When is postmortem needed</strong>:</h3><p>To start off with, we will start with postmortem exercise for all the <strong>S0</strong> incidents that happen in in tech. We will have a <strong>bi-monthly</strong> postmortem review meeting, which will be attended by the tech leads across team (invitation to follow further)</p><h3><strong>Preparing the RCA doc:</strong></h3><p>RCA template: Have templatised the RCA doc and you can choose the same while creating new page and only fill in the data, details present at: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"RCA Template\" ri:version-at-save=\"5\" /><ac:link-body>RCA Template</ac:link-body></ac:link> </p><p>All RCAs are to be placed at the below location, following below naming convention<br />Location: <ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"EW\" ri:content-title=\"RCA-Directory\" ri:version-at-save=\"5\" /><ac:link-body>RCA-Directory</ac:link-body></ac:link> <br />Naming convention: <code>s0-yyyy-mm-dd-&lt;servicename&gt;</code></p><h3><strong>What to expect out of postmortem review</strong>:</h3><p>We will be dividing this to two sections, Internal and external</p><h4><strong>Internal:</strong> </h4><p>Where the app-owners have to prepare the Provisional RCA within 24 hrs &nbsp;and a complete RCA within 72hrs of the incident closure which will have success criteria defined as below:</p><ul><li><p>was the key incident data collected sufficiently</p></li><li><p>Are the impact assessments complete</p></li><li><p>Is the actions plans appropriate and right priority maintained</p></li><li><p>EM sign-off on the RCA doc to be presented to the external RCA meeting.</p></li></ul><h4><strong>External: </strong></h4><p>RCA review meeting, where the app-owner(EM or a delegate) will present the RCA for the two weeks and will have the success criteria defined as below:</p><ul><li><p>Was the root cause sufficiently deep</p></li><li><p>Suggestions and/or learnings from others</p></li><li><p>Sharing of the learning to cross team</p></li><li><p>Better collaboration going forward</p></li></ul><h3><strong>How to run the review meeting:</strong></h3><ul><li><p>The app owners can send out the pre-reads(RCA doc) as part of the meeting invite(edit the incident as it will be recurring)</p></li><li><p>Suggestions, comments and questions can be added to the document(async) by the recipients</p></li><li><p>Blameless and constructive: major point to note is that the meeting has to be run in a completely <strong>blameless</strong> manner and feedbacks/suggestions have to me passed on in a <strong>constructive</strong> way</p></li></ul><p><strong>Role of RCA Owner</strong></p><p>Before presenting the RCA, RCA owner to confirm the template used to chart out RCA and take required stakeholders concurrence on the RCA document prepared against the occurred incident.</p><p /><h3><strong>Where does Postmortem fit into the larger Incident management</strong></h3><p>Please read through the larger Incident Management process and details at: <ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"Incident Management\" ri:version-at-save=\"14\" /><ac:link-body>Incident Management (Draft)</ac:link-body></ac:link> </p><p /><ac:structured-macro ac:name=\"code\" ac:schema-version=\"1\" ac:macro-id=\"498bdf35-daba-4e1a-8e16-09283d6a8ab8\"><ac:plain-text-body><![CDATA[NOTE:This is v1 of the process and in draft mode, which should mature as and when we go forward. Please keep comments/recommendations flowing in here]]></ac:plain-text-body></ac:structured-macro><p />",
      "approach_used": "endpoint_2",
      "word_count": 460
    },
    {
      "id": "2451046428",
      "title": "RCA Template",
      "type": "page",
      "status": "current",
      "is_runbook": false,
      "space": "DEVOPS",
      "url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2451046428",
      "direct_url": "https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2451046428/RCA+Template",
      "created": "2023-02-17T11:17:47.541Z",
      "content": "<p>NoA template has been created, please follow the below steps to use it while creating a new doc<br />1. click on Create link on the confluence(inside RCA Folder: <ac:link><ri:page ri:space-key=\"EW\" ri:content-title=\"RCA-Directory\" ri:version-at-save=\"5\" /><ac:link-body>RCA-Directory</ac:link-body></ac:link> )<br />2. on the tops right click on Templates-&gt;Promoted-&gt;RCA Template</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"414\" ac:original-width=\"1536\"><ri:attachment ri:filename=\"image-20220418-091800.png\" ri:version-at-save=\"1\" /></ac:image><p /><p>Or copy-paste contents of this page below the line.</p><p><strong><u>Abbreviations:</u></strong></p><p><strong>Incident Start time</strong> : When did the incident start , in date + time format.</p><p><strong>Incident End time</strong> : Closure of the said incident/recovery of the system which includes the systems state to serve the users before the incident has occurred.</p><p><strong>Severity</strong> : One of S0, S1 or S2. S0/S1 need mandatory RCA publishing and presentation. S2 is for documentation purpose. Refer <a href=\"https://meesho.atlassian.net/wiki/spaces/DEVOPS/pages/2449015768/Incident+Management+Draft\">this</a> for severity clarification.</p><p><strong>MTTD</strong> - How long it took to find out about this incident ( via monitoring systems, communication channel, users complaints etc)</p><p><strong>MTTM</strong> - Time taken to reduce the impact by applying temporary/permanent measures.</p><p><strong>MTTR</strong> - Time taken to restore system/functionality completely after failure/issue occurrence.</p><p><strong>RCA Published Date - </strong>When was the RCA document published. </p><p><strong>No. of days taken to publish RCA</strong> - All the RCA&rsquo;s (S0/S1) to be published within 7 days of the incident.</p><p><strong>RCA Reviewed by</strong> : RCA reviewer Name and email. All the RCA&rsquo;s to be reviewed by either D/EM/Arch/SD4&rsquo;s.</p><p><strong>Team</strong> : Team prepared the RCA.</p><p><strong>RCA Stakeholders</strong> : People who are all part of incident mitigation/resolution, could be multiple teams ( mention the participants who all are involved and their team details).</p><hr /><h3>                                           <strong><u> &lt;Incident Title - S0/S1_YYYY-MM-DD_RCA:Name&gt;</u></strong></h3><table data-layout=\"default\" ac:local-id=\"06a6e481-5192-41b1-a116-326ec0eaa59e\"><colgroup><col style=\"width: 340.0px;\" /><col style=\"width: 340.0px;\" /></colgroup><tbody><tr><td><p><strong>Incident Start Time</strong>: &lt;dd-mm-yyyy hh:mm&gt; </p></td><td><p><strong>Incident End Time</strong>: &lt;dd-mm-yyyy hh:mm&gt;</p></td></tr><tr><td><p><strong>Severity</strong>: &lt;S0/S1/S2&gt;  </p></td><td><p><strong>Time To Detect (MTTD): </strong>&lt;hh:mm&gt;</p></td></tr><tr><td><p><strong>Time to Mitigate (MTTM): </strong>&lt;hh:mm&gt; </p></td><td><p><strong>Time to Resolve (MTTR)</strong>: &lt;hh:mm&gt;</p></td></tr><tr><td><p><strong>RCA Published Date</strong> : &lt;dd-mm-yyyy hh:mm&gt; </p></td><td><p><strong>No. of days taken to publish RCA :</strong> &lt;Number&gt;</p></td></tr><tr><td><p><strong>RCA Reviewed By :  </strong>&lt;reviewer name, email&gt;</p></td><td><p><strong>Team</strong> : &lt;team name&gt;</p></td></tr></tbody></table><p /><h3><strong><u>RCA Stakeholders :</u> </strong></h3><p>[<em> List of members &amp; team who are part of RCA preparation/incident mitigation. All the stakeholders should give concurrence on the RCA for the occurred incident. </em></p><p><em>For eg: Devops + backend etc</em>]</p><p>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</p><h3><strong><u>Incident Summary:</u></strong></h3><p><em>[This is a brief summary from a customer point of view Put this in layman&rsquo;s terms to ensure non-tech folks can understand this, avoid technical detailing here. The summary should be crisp and callout the impact. Ex: 1) A bad deployment resulted in X% of customers seeing an error screen in Home Page. 2) A data issue caused wrong push notifications to be sent to YYY users. 3) A deterioration in a service caused customers not to be able to see recommendations.]</em></p><p><em>[Suggested to attach screenshots, graphs, logs related to incident for better clarity]</em></p><p /><p><strong><u>Systems Impacted:</u>\t</strong>\t\t\t\t</p><p><em>[List down all the systems, sub-systems, services that are impacted. Use the %age and absolute number of a metric in the impact column. Ex: X% (YY) of customers couldn&rsquo;t make payments] \t\t \t \t \t\t</em></p><table data-layout=\"default\" ac:local-id=\"fcc6c519-9251-47cb-8a1a-9e0aef03ca67\"><colgroup><col style=\"width: 226.67px;\" /><col style=\"width: 226.67px;\" /><col style=\"width: 226.67px;\" /></colgroup><tbody><tr><td data-highlight-colour=\"#auto\"><p><strong>System/Component</strong></p></td><td data-highlight-colour=\"#auto\"><p><strong>Functionality</strong></p></td><td data-highlight-colour=\"#auto\"><p><strong>Impact</strong></p></td></tr><tr><td data-highlight-colour=\"#auto\"><p><em>[images.meesho.com]</em></p></td><td data-highlight-colour=\"#auto\"><p><em>[Image serving to the app]</em></p></td><td data-highlight-colour=\"#auto\"><p><em>[Few images in homepage were not loading for customers]</em></p></td></tr></tbody></table><p><em>\t</em></p><h3><em>\t\t\t\t\t\t\t\t\t\t\t</em><strong><u>Timeline:</u></strong></h3><p><em>[Call out the complete events list in timeline order. Eg: when/how did we come to know about the issue, pd alerts received, mitigation actions taken like restarts, deployments, notifying the stakeholders, war room creation, resolution taken etc.] \t</em></p><table data-layout=\"default\" ac:local-id=\"68716223-71d2-4e96-981d-b36afaf33dfb\"><colgroup><col style=\"width: 340.0px;\" /><col style=\"width: 340.0px;\" /></colgroup><tbody><tr><th><p><strong><u>Date &amp; Time </u></strong></p></th><th><p><strong><u>Action occurred</u></strong></p></th></tr><tr><td><p><em>DD_MM_YYYY  HH:MM:SS</em></p></td><td><p>&lt;<em>incident detected&gt;</em></p></td></tr><tr><td><p><em>DD_MM_YYYY  HH:MM:SS</em></p></td><td><p>&lt;<em>notified to relevant teams &gt;</em></p></td></tr><tr><td><p><em>DD_MM_YYYY  HH:MM:SS</em></p></td><td><p><em>&lt;started evaluation&gt;&hellip;etc</em></p></td></tr><tr><td><p>&hellip;</p></td><td><p>&hellip;</p></td></tr></tbody></table><p /><h3><strong><u>Incident Description:\t</u></strong></h3><p><em>[1. Explain in detail about this incident(<strong>technical</strong>). What caused/led to this.</em><br /><em> 2. Attach evidence of the impact: </em><br /><em>eg: Screenshots from monitoring dashboards(cloudwatch, prometheus etc). </em><br /><em>      Github link, code snippets, PR links, deployment tickets </em><br /><em>      Screenshot of the impact on business(mixpanel etc).</em><br /><em>to analyze the issue]</em></p><p /><p><strong><u>Fixes Done</u></strong></p><p><em>[List down the changes, fixes and/or steps(PRs, breakfix(CMR) tickets etc) performed to mitigate and resolve the issue] \t\t\t\t</em></p><p><em>\t\t\t\t\t\t</em></p><p><strong><u>5 Why Analysis:</u></strong></p><p><em>[Keep asking whys sequentially till you get to the <strong>root cause</strong>. Each should be a followup to the previous question. If there are multiple followup questions, use a sub-list. Ideally we should reach to a conclusion in 5 questions]\t\t\t\t\t</em></p><p><em><strong>Why 1: &lt;Question&gt; [ex: why did the user see an error screen?] </strong></em><br /><em>[Briefly explain the answer here. Ex: The API call from frontend to backend was failing.]\t\t\t\t\t</em></p><p><em><strong>Why 2: &lt;Question&gt; [ex: why did the backend api call fail?]</strong></em><br /><em>[Briefly explain the answer here. Ex: The backend server failed to acquire a database connection]\t\t\t\t\t</em></p><p><em><strong>Why 3: &lt;Question&gt; [ex: why did the service fail to get the db connection?]</strong> </em><br /><em>[Briefly explain the answer here. Ex: There were no available connections in the connection pool]\t\t\t\t\t</em></p><p><em><strong>Why 4: &lt;Question&gt; [ex: why did we exhaust all the connections in the pool?]</strong></em><br /><em>[Briefly explain the answer here. Ex: A slow query was executing on the DB which consumed all DB resources, hence slowing down other DB queries as well]</em></p><p><em><u>[Optional subquestions to get to root cause of the incident]</u></em></p><p><em><strong>Why 5.1: &lt;Question&gt; [ex: why was the query running for long?]</strong></em><br /><em>[Briefly explain the answer here. Ex: A developer mistakenly deployed an untested query <strong>(the root cause)</strong>]\t\t\t\t\t</em></p><p><em><strong>Why 5.2: &lt;Question&gt; [ex: why did we let the query run for long and consume all the resources?]</strong></em><br /><em>[Briefly explain the answer here. Ex: There was no monitoring of DB resources and slow queries. <strong>(the root cause)</strong>]</em></p><p><em>\t\t\t\t\t\t\t\t</em></p><p><strong><u>Reducing Incident Duration:</u></strong></p><p><strong>How can we reduce the Time to Mitigate by half?</strong></p><p><em>[Briefly explain what we could have done to reduce the mitigation time by half. This would also feed into Action Items subsequently </em><br /><em>Ex: 1) Setup a dashboard for all database connection pool metrics, 2) Write scripts to kill a database query etc.]\t\t\t\t\t\t</em></p><p><strong>How can we reduce the Time to Resolve by Half?</strong></p><p><em>[Briefly explain what we can do to reduce the resolve time by half. </em><br /><em>Ex: 1) Auto kill queries running for longer than 1min. 2) Add a capability in the CD pipeline for hot-fixes etc.]\t</em></p><p><em>\t\t\t\t</em></p><p><strong><u>Action Items:</u></strong></p><p><em>[Identify and list down all the action items to prevent this or similar issues from recurring again. The action items can be a bug fix, process improvement, adding instrumentation/monitoring/alerting etc. The action items should be prioritized as P0/P1/P2 based on the criticality and tracked via jira tickets and linked to the main RCA jira] \t</em></p><p /><p><strong><u>Short Term Action Items:</u></strong></p><p><em>[To be implemented within few hours or days. All the AI&rsquo;s to have jira&rsquo;s attached to them for tracking.]</em></p><table data-layout=\"default\" ac:local-id=\"c0ca80c2-3975-48dd-a7e5-1b7f5625cb74\"><colgroup><col style=\"width: 226.67px;\" /><col style=\"width: 226.67px;\" /><col style=\"width: 226.67px;\" /></colgroup><tbody><tr><th><p><strong>SL NO</strong></p></th><th><p><strong>Short term AI Name</strong></p></th><th><p><strong>Link to Jira</strong></p></th></tr><tr><td><p>1</p></td><td><p><em>[action item 1]</em></p></td><td><p>[<em>Link to Jira tickets created for short term action items</em>]</p></td></tr><tr><td><p>2</p></td><td><p><em>[action item 2]</em></p></td><td><p>[<em>Link to Jira tickets created for short term action items</em>]</p></td></tr></tbody></table><p /><p><strong><u>Long Term Action Items:</u></strong></p><p><em>[Anything that needs architectural or design change or needs releases to be fixed, where the implementation time be around/more than a week]\t\t\t\t\t\t</em></p><p>[<em>Link to Jira tickets created for long term action items</em>]</p><table data-layout=\"default\" ac:local-id=\"7d7853c0-c105-4eef-ad16-baba68d9d8fa\"><colgroup><col style=\"width: 226.67px;\" /><col style=\"width: 226.67px;\" /><col style=\"width: 226.67px;\" /></colgroup><tbody><tr><th><p><strong>SL NO</strong></p></th><th><p><strong>Long term AI Name</strong></p></th><th><p><strong>Link to Jira</strong></p></th></tr><tr><td><p>1</p></td><td><p><em>[action item 1]</em></p></td><td><p>[<em>Link to Jira tickets created for short term action items</em>]</p></td></tr><tr><td><p>2</p></td><td><p><em>[action item 2]</em></p></td><td><p>[<em>Link to Jira tickets created for short term action items</em>]</p></td></tr></tbody></table><p><em>\t\t\t\t</em></p>",
      "approach_used": "endpoint_2",
      "word_count": 1155
    }
  ]
}